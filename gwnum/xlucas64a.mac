; Copyright 2009 - Mersenne Research, Inc.  All rights reserved
; Author:  George Woltman
; Email: woltman@alum.mit.edu
;
; These macros implement AMD 64-bit SSE2 optimized versions of macros found
; in xlucas.mac.  We make use of the 8 extra registers and AMD-specific optimizations.


;;  The AMD64 (K8) and Phenom (K10) architecture differ from the Intel architectures
;;  in that it can perform 2 load operations per clock cycle.  Adds and multiplies
;;  both take 4 clocks.
;;
;;  The K8 architecture also pays a big penalty for use of "movapd reg, reg".  Use
;;  of this instruction causes "bubbles" in both the add and multiply pipes.

;; On AMD64 (K8 architecture, not the Phenom K10 architecture), punpckhqdq is grossly slow.
shuffle_store MACRO mem1, mem2, reglo, reghi
	movsd	Q mem1, reglo
	movsd	Q mem1[8], reghi
	unpckhpd reglo, reghi
	movapd	mem2, reglo
	ENDM

shuffle_load MACRO reglo, reghi, mem1, mem2
	movapd	reglo, mem1
	movapd	reghi, mem2
	unpcklpd reglo, reghi
	movlpd	reghi, Q mem1[8]
	ENDM

shuffle_load_with_temp MACRO reglo, reghi, mem1, mem2
	shuffle_load reglo, reghi, mem1, mem2
	ENDM

;; Cheat sheet for scheduling dependency chains (and num registers required)
;;	  12345678901234567890123456789012345678901234567890123456789012345678901234567890
;;A3      MMMMAAAAMMMM					
;;B3       MMMMAAAAMMMM
;;B4        MMMMAAAAMMMM
;;B2	     MMMMAAAAMMMM
;;A4	      MMMMAAAAMMMM
;;A2	       MMMMAAAAMMMM
;;nxt A3        MMMMAAAAMMMM
;;nxt B3         MMMMAAAAMMMM
;;mR3(depA3)	      AAAA
;;mR1(depA3)	       AAAA
;;mI3(depB3)	        AAAA			
;;mI1(depB3)		 AAAA					
;;mI4(depB2B4)            AAAA
;;mR4(depA2A4)	           AAAA
;;nxt mR3(depA3)            AAAA
;;nxt mR1(depA3)             AAAA				11 registers
;;nxt A2                  MMMMAAAAMMMM
;;nxt A4                   MMMMAAAAMMMM
;;nxt B2                    MMMMAAAAMMMM
;;nxt B4                     MMMMAAAAMMMM			15 registers
;;mI2(depB2B4)	              MMMMAAAA
;;mR2(depA2A4)	               MMMMAAAA
;;r3(depmR3mI4)                     AAAA
;;i4(depmI3mR4)                      AAAA
;;i2(depmI1mI2)                       AAAA
;;r2(depmR1mR2)                        AAAA
;;nxt mR4(depA2A4)                      AAAA
;;nxt mI4(depB2B4)                       AAAA
;;r4(depmR3mI4)                       MMMMAAAA			2 storeable
;;i3(depmI3mR4)                        MMMMAAAA			2 storeable
;;i1(depmI1mI2)                         MMMMAAAA		2 storeable
;;nxt mI3(depB3)                             AAAA
;;nxt mI1(depB3)	                      AAAA		-- now no free regs
;;nxt mR2(depA2A4)                         MMMMAAAA
;;nxt mI2(depB2B4)                          MMMMAAAA
;;nxt r3(depmR3mI4)                              AAAA
;;nxt i4(depmI3mR4)                               AAAA
;;nxt r2(depmR1mR2)                                AAAA
;;nxt i2(depmI1mI2)                                 AAAA
;;r1(depmR1mR2)                                  MMMMAAAA	2 storeable
;;nxt r4(depmR3mI4)                               MMMMAAAA
;;nxt i3(depmI3mR4)                                MMMMAAAA
;;nxt r1(depmR1mR2)                                 MMMMAAAA
;;nxt i1(depmI1mI2)                                  MMMMAAAA

x4cl_fft_cmn MACRO srcreg,srcinc,d1,d2,screg,off2,off3,off4,off6,off7,off8
	movapd	xmm0, [srcreg+d2]	;; R3					;K8	;K10
	movapd	xmm1, [screg+off3+16]	;; cosine/sine
	mulpd	xmm0, xmm1		;; A3 = R3 * cosine/sine		;1-4	;1-4
	movapd	xmm3, [srcreg+d2+16]	;; I3
	mulpd	xmm1, xmm3		;; B3 = I3 * cosine/sine		;3-6	;2-5	;2456789ABCDEF
	movapd	xmm9, [srcreg+d2+d1+16]	;; I4
	movapd	xmm11, [screg+off4+16]	;; cosine/sine
	mulpd	xmm9, xmm11		;; B4 = I4 * cosine/sine		;5-8	;3-6	;245678ACDEF
	movapd	xmm5, [srcreg+d1+16]	;; I2
	movapd	xmm7, [screg+off2+16]	;; cosine/sine
	mulpd	xmm5, xmm7		;; B2 = I2 * cosine/sine		;7-10	;4-7	;2468ACDEF
	subpd	xmm0, xmm3		;; A3 = A3 - I3				;6-9	;5-8	;23468ACDEF
	movapd	xmm8, [srcreg+d2+d1]	;; R4							;2346ACDEF
	mulpd	xmm11, xmm8		;; A4 = R4 * cosine/sine		;9-12	;5-8
	addpd	xmm1, [srcreg+d2]	;; B3 = B3 + R3				;8-11	;6-9
	movapd	xmm4, [srcreg+d1]	;; R2							;236ACDEF
	mulpd	xmm7, xmm4		;; A2 = R2 * cosine/sine		;11-14	;6-9
	movapd	xmm12, [srcreg+d2+32]	;; nxt R3
	movapd	xmm13, [screg+off7+16]	;; nxt cosine/sine					;236AEF
	addpd	xmm9, xmm8		;; B4 = B4 + R4				;10-13	;7-10	;2368AEF
	mulpd	xmm12, xmm13		;; nxt A3 = R3 * cosine/sine		;13-16	;7-10
	addpd	xmm5, xmm4		;; B2 = B2 + R2				;12-15	;8-11	;23468AEF
	movapd	xmm14, [srcreg+d2+48]	;; nxt I3						;23468AF
	mulpd	xmm13, xmm14		;; nxt B3 = I3 * cosine/sine		;15-18	;8-11
	subpd	xmm11, [srcreg+d2+d1+16];; A4 = A4 - I4				;14-17	;9-12
	movapd	xmm2, [screg+off3]	;; sine							;3468AF
	mulpd	xmm0, xmm2		;; A3 = A3 * sine (new R3)		;17-20	;9-12
	subpd	xmm7, [srcreg+d1+16]	;; A2 = A2 - I2				;16-19	;10-13
	mulpd	xmm1, xmm2		;; B3 = B3 * sine (new I3)		;19-22	;10-13	;23468AF
	subpd	xmm12, xmm14		;; nxt A3 = A3 - I3			;18-21	;11-14	;23468AEF
	movapd	xmm2, [screg+off4]	;; sine							;3468AEF
	mulpd	xmm9, xmm2		;; B4 = B4 * sine (new I4)		;21-24	;11-14
	addpd	xmm13, [srcreg+d2+32]	;; nxt B3 = B3 + R3			;20-23	;12-15
	movapd	xmm3, [screg+off2]	;; sine							;468AEF
	mulpd	xmm5, xmm3		;; B2 = B2 * sine (new I2)		;23-26	;12-15
	 movapd	xmm10, [srcreg]		;; R1							;468EF
	 subpd	xmm10, xmm0		;; R1 = R1 - R3 (mid R3)		;22-25	;13-16
	mulpd	xmm11, xmm2		;; A4 = A4 * sine (new R4)		;25-28	;13-16	;2468EF
	 addpd	xmm0, [srcreg]		;; R3 = R1 + R3 (mid R1)		;24-27	;14-17
	mulpd	xmm7, xmm3		;; A2 = A2 * sine (new R2)		;27-30	;14-17	;23468EF
	 movapd	xmm14, [srcreg+16]	;; I1							;23468F
	 subpd	xmm14, xmm1		;; I1 = I1 - I3 (mid I3)		;26-29	;15-18
	movapd	xmm2, [screg+off7]	;; nxt sine						;3468F
	mulpd	xmm12, xmm2		;; nxt A3 = A3 * sine (new R3)		;29-32	;15-18
	 addpd	xmm1, [srcreg+16]	;; I3 = I1 + I3 (mid I1)		;28-31	;16-19
	mulpd	xmm13, xmm2		;; nxt B3 = B3 * sine (new I3)		;31-34	;16-19	;23468F
	xprefetchw [srcreg+srcinc]
	 subpd	xmm5, xmm9		;; I2 = I2 - I4 (mid I4)		;30-33	;17-20
	movapd	xmm2, [srcreg+d1+32]	;; nxt R2
	movapd	xmm3, [screg+off6+16]	;; nxt cosine/sine					;468F
	mulpd	xmm2, xmm3		;; nxt A2 = R2 * cosine/sine		;33-36	;17-20
	 subpd	xmm7, xmm11		;; R2 = R2 - R4 (mid R4)		;32-35	;18-21
	movapd	xmm4, [srcreg+d2+d1+32]	;; nxt R4						;68F
	movapd	xmm6, [screg+off8+16]	;; nxt cosine/sine					;8F
	mulpd	xmm4, xmm6		;; nxt A4 = R4 * cosine/sine		;35-38	;18-21
	 movapd	xmm8, [srcreg+32]	;; nxt R1						;F
	 subpd	xmm8, xmm12		;; nxt R1 = R1 - R3 (mid R3)		;34-37	;19-22
	movapd	xmm15, [srcreg+d1+48]	;; nxt I2						;
	mulpd	xmm3, xmm15		;; nxt B2 = I2 * cosine/sine		;37-40	;19-22
	 addpd	xmm12, [srcreg+32]	;; nxt R3 = R1 + R3 (mid R1)		;36-39	;20-23
	mulpd	xmm6, [srcreg+d2+d1+48]	;; nxt B4 = I4 * cosine/sine		;39-42	;20-23
	subpd	xmm2, xmm15		;; nxt A2 = A2 - I2			;38-41	;21-24	;F
	movapd	xmm15, XMM_TWO									;
	 mulpd	xmm9, xmm15		;; new I4 * 2				;41-44	;21-24
	subpd	xmm4, [srcreg+d2+d1+48]	;; nxt A4 = A4 - I4			;40-43	;22-25
	 mulpd	xmm11, xmm15		;; new R4 * 2				;43-46	;22-25
	addpd	xmm3, [srcreg+d1+32]	;; nxt B2 = B2 + R2			;42-45	;23-26
	xprefetchw [srcreg+srcinc+d1]
	addpd	xmm6, [srcreg+d2+d1+32]	;; nxt B4 = B4 + R4			;44-47	;24-27
	 addpd	xmm9, xmm5		;; I4 = I2 + I4 (mid I2)		;46-49	;25-28
	mulpd	xmm2, [screg+off6]	;; nxt A2 = A2 * sine (new R2)		;45-48	;25-28
	 addpd	xmm11, xmm7		;; R4 = R2 + R4 (mid R2)		;48-51	;26-29
	mulpd	xmm4,[screg+off8]	;; nxt A4 = A4 * sine (new R4)		;47-50	;26-29
	subpd	xmm10, xmm5		;; R3 = R3 - I4 (final R3)		;50-53	;27-30
	mulpd	xmm3, [screg+off6]	;; nxt B2 = B2 * sine (new I2)		;49-52	;27-30
	subpd	xmm14, xmm7		;; I3 = I3 - R4 (final I4)		;52-55	;28-31
	mulpd	xmm6, [screg+off8]	;; nxt B4 = B4 * sine (new I4)		;51-54	;28-31
	subpd	xmm1, xmm9		;; I1 = I1 - I2 (final I2)		;54-57	;29-32
	mulpd	xmm5, xmm15		;; mid I4 * 2				;53-56	;29-32
	xprefetchw [srcreg+srcinc+d2]
	subpd	xmm0, xmm11		;; R1 = R1 - R2 (final R2)		;56-59	;30-33
	mulpd	xmm7, xmm15		;; mid R4 * 2				;55-58	;30-33
	 subpd	xmm2, xmm4		;; nxt R2 = R2 - R4 (mid R4)		;58-61	;31-34
	mulpd	xmm9, xmm15		;; mid I2 * 2				;57-60	;31-34
	movapd	[srcreg+d1], xmm10	;; Save R3					;31-34	;A
	 subpd	xmm3, xmm6		;; nxt I2 = I2 - I4 (mid I4)		;60-63	;32-35
	movapd	[srcreg+d1+48], xmm14	;; Save I4					;32-35	;AE
	addpd	xmm5, xmm10		;; I4 = R3 + I4 (final R4)		;62-65	;33-36
	addpd	xmm7, xmm14		;; R4 = I3 + R4 (final I3)		;64-67	;34-37
	 mulpd	xmm4, xmm15		;; nxt new R4 * 2			;65-68*	;34-37*
	xprefetchw [srcreg+srcinc+d2+d1]
	addpd	xmm9, xmm1		;; I2 = I1 + I2 (final I1)		;66-69	;35-38
	 mulpd	xmm6, xmm15		;; nxt new I4 * 2			;67-70*	;35-38*
	 movapd	xmm10, [srcreg+48]	;; nxt I1						;E
	 subpd	xmm10, xmm13		;; nxt I1 = I1 - I3 (mid I3)		;68-71	;36-39
	 addpd	xmm13, [srcreg+48]	;; nxt I3 = I1 + I3 (mid I1)		;70-73	;37-39
	 addpd	xmm4, xmm2		;; nxt R4 = R2 + R4 (mid R2)		;72-75	;38-41
	movapd	[srcreg+48], xmm1	;; Save I2					; 33-36
	 addpd	xmm6, xmm3		;; nxt I4 = I2 + I4 (mid I2)		;74-77	;39-42
	movapd	[srcreg+32], xmm0	;; Save R2					; 34-37
	subpd	xmm8, xmm3		;; nxt R3 = R3 - I4 (final R3)		;76-79	;40-43
	mulpd	xmm11, xmm15		;; mid R2 * 2				;77-80*	;40-43*
	movapd	[srcreg+d1+32], xmm5	;; Save R4					; 37-40
	subpd	xmm10, xmm2		;; nxt I3 = I3 - R4 (final I4)		;78-81	;41-44
	mulpd	xmm3, xmm15		;; nxt mid I4 * 2			;79-82	;41-44
	movapd	[srcreg+d1+16], xmm7	;; Save I3					; 38-41
	subpd	xmm12, xmm4		;; nxt R1 = R1 - R2 (final R2)		;80-83	;42-45
	mulpd	xmm2, xmm15		;; nxt mid R4 * 2			;81-84	;42-45
	movapd	[srcreg+16], xmm9	;; Save I1					; 39-42
	subpd	xmm13, xmm6		;; nxt I1 = I1 - I2 (final I2)		;82-85	;43-46
	mulpd	xmm4, xmm15		;; nxt mid R2 *	2			;83-86	;43-46
	addpd	xmm11, xmm0		;; R2 = R1 + R2 (final R1)		;84-87	;44-47
	mulpd	xmm6, xmm15		;; nxt mid I2 * 2			;85-88	;44-47
	movapd	[srcreg+d2+d1], xmm8	;; nxt Save R3					; 44-47
	addpd	xmm3, xmm8		;; nxt I4 = R3 + I4 (final R4)		;87-90	;45-48
	movapd	[srcreg+d2+d1+48], xmm10;; nxt Save I4					; 45-48
	addpd	xmm2, xmm10		;; nxt R4 = I3 + R4 (final I3)		;89-92	;46-49
	movapd	[srcreg+d2+32], xmm12	;; nxt Save R2					; 46-49
	addpd	xmm4, xmm12		;; nxt R2 = R1 + R2 (final R1)		;91-94	;47-50
	movapd	[srcreg+d2+48], xmm13	;; nxt Save I2					; 47-50
	addpd	xmm6, xmm13		;; nxt I2 = I1 + I2 (final I1)		;93-96	;48-51
	movapd	[srcreg], xmm11		;; Save R1					; 48-51
	movapd	[srcreg+d2+d1+32], xmm3	;; nxt Save R4					; 49-52
	movapd	[srcreg+d2+d1+16], xmm2	;; nxt Save I3					; 50-53
	movapd	[srcreg+d2], xmm4	;; nxt Save R1					; 51-54
	movapd	[srcreg+d2+16], xmm6	;; nxt Save I1					; 52-55
	ENDM

g4cl_fft_cmn MACRO srcreg,srcinc,d1,d2,dstreg,dstinc,e1,e2,screg,off2,off3,off4,off6,off7,off8
	movapd	xmm0, [srcreg+d2]	;; R3					;K8	;K10
	movapd	xmm1, [screg+off3+16]	;; cosine/sine
	mulpd	xmm0, xmm1		;; A3 = R3 * cosine/sine		;1-4	;1-4
	movapd	xmm3, [srcreg+d2+16]	;; I3
	mulpd	xmm1, xmm3		;; B3 = I3 * cosine/sine		;3-6	;2-5	;2456789ABCDEF
	movapd	xmm9, [srcreg+d2+d1+16]	;; I4
	movapd	xmm11, [screg+off4+16]	;; cosine/sine
	mulpd	xmm9, xmm11		;; B4 = I4 * cosine/sine		;5-8	;3-6	;245678ACDEF
	movapd	xmm5, [srcreg+d1+16]	;; I2
	movapd	xmm7, [screg+off2+16]	;; cosine/sine
	mulpd	xmm5, xmm7		;; B2 = I2 * cosine/sine		;7-10	;4-7	;2468ACDEF
	subpd	xmm0, xmm3		;; A3 = A3 - I3				;6-9	;5-8	;23468ACDEF
	movapd	xmm8, [srcreg+d2+d1]	;; R4							;2346ACDEF
	mulpd	xmm11, xmm8		;; A4 = R4 * cosine/sine		;9-12	;5-8
	addpd	xmm1, [srcreg+d2]	;; B3 = B3 + R3				;8-11	;6-9
	movapd	xmm4, [srcreg+d1]	;; R2							;236ACDEF
	mulpd	xmm7, xmm4		;; A2 = R2 * cosine/sine		;11-14	;6-9
	movapd	xmm12, [srcreg+d2+32]	;; nxt R3
	movapd	xmm13, [screg+off7+16]	;; nxt cosine/sine					;236AEF
	addpd	xmm9, xmm8		;; B4 = B4 + R4				;10-13	;7-10	;2368AEF
	mulpd	xmm12, xmm13		;; nxt A3 = R3 * cosine/sine		;13-16	;7-10
	addpd	xmm5, xmm4		;; B2 = B2 + R2				;12-15	;8-11	;23468AEF
	movapd	xmm14, [srcreg+d2+48]	;; nxt I3						;23468AF
	mulpd	xmm13, xmm14		;; nxt B3 = I3 * cosine/sine		;15-18	;8-11
	subpd	xmm11, [srcreg+d2+d1+16];; A4 = A4 - I4				;14-17	;9-12
	movapd	xmm2, [screg+off3]	;; sine							;3468AF
	mulpd	xmm0, xmm2		;; A3 = A3 * sine (new R3)		;17-20	;9-12
	subpd	xmm7, [srcreg+d1+16]	;; A2 = A2 - I2				;16-19	;10-13
	mulpd	xmm1, xmm2		;; B3 = B3 * sine (new I3)		;19-22	;10-13	;23468AF
	subpd	xmm12, xmm14		;; nxt A3 = A3 - I3			;18-21	;11-14	;23468AEF
	movapd	xmm2, [screg+off4]	;; sine							;3468AEF
	mulpd	xmm9, xmm2		;; B4 = B4 * sine (new I4)		;21-24	;11-14
	addpd	xmm13, [srcreg+d2+32]	;; nxt B3 = B3 + R3			;20-23	;12-15
	movapd	xmm3, [screg+off2]	;; sine							;468AEF
	mulpd	xmm5, xmm3		;; B2 = B2 * sine (new I2)		;23-26	;12-15
	 movapd	xmm10, [srcreg]		;; R1							;468EF
	 subpd	xmm10, xmm0		;; R1 = R1 - R3 (mid R3)		;22-25	;13-16
	mulpd	xmm11, xmm2		;; A4 = A4 * sine (new R4)		;25-28	;13-16	;2468EF
	 addpd	xmm0, [srcreg]		;; R3 = R1 + R3 (mid R1)		;24-27	;14-17
	mulpd	xmm7, xmm3		;; A2 = A2 * sine (new R2)		;27-30	;14-17	;23468EF
	 movapd	xmm14, [srcreg+16]	;; I1							;23468F
	 subpd	xmm14, xmm1		;; I1 = I1 - I3 (mid I3)		;26-29	;15-18
	movapd	xmm2, [screg+off7]	;; nxt sine						;3468F
	mulpd	xmm12, xmm2		;; nxt A3 = A3 * sine (new R3)		;29-32	;15-18
	 addpd	xmm1, [srcreg+16]	;; I3 = I1 + I3 (mid I1)		;28-31	;16-19
	mulpd	xmm13, xmm2		;; nxt B3 = B3 * sine (new I3)		;31-34	;16-19	;23468F
	xprefetch [srcreg+srcinc]
	xprefetchw [dstreg+dstinc]
	 subpd	xmm5, xmm9		;; I2 = I2 - I4 (mid I4)		;30-33	;17-20
	movapd	xmm2, [srcreg+d1+32]	;; nxt R2
	movapd	xmm3, [screg+off6+16]	;; nxt cosine/sine					;468F
	mulpd	xmm2, xmm3		;; nxt A2 = R2 * cosine/sine		;33-36	;17-20
	 subpd	xmm7, xmm11		;; R2 = R2 - R4 (mid R4)		;32-35	;18-21
	movapd	xmm4, [srcreg+d2+d1+32]	;; nxt R4						;68F
	movapd	xmm6, [screg+off8+16]	;; nxt cosine/sine					;8F
	mulpd	xmm4, xmm6		;; nxt A4 = R4 * cosine/sine		;35-38	;18-21
	 movapd	xmm8, [srcreg+32]	;; nxt R1						;F
	 subpd	xmm8, xmm12		;; nxt R1 = R1 - R3 (mid R3)		;34-37	;19-22
	movapd	xmm15, [srcreg+d1+48]	;; nxt I2						;
	mulpd	xmm3, xmm15		;; nxt B2 = I2 * cosine/sine		;37-40	;19-22
	 addpd	xmm12, [srcreg+32]	;; nxt R3 = R1 + R3 (mid R1)		;36-39	;20-23
	mulpd	xmm6, [srcreg+d2+d1+48]	;; nxt B4 = I4 * cosine/sine		;39-42	;20-23
	subpd	xmm2, xmm15		;; nxt A2 = A2 - I2			;38-41	;21-24	;F
	movapd	xmm15, XMM_TWO									;
	 mulpd	xmm9, xmm15		;; new I4 * 2				;41-44	;21-24
	subpd	xmm4, [srcreg+d2+d1+48]	;; nxt A4 = A4 - I4			;40-43	;22-25
	 mulpd	xmm11, xmm15		;; new R4 * 2				;43-46	;22-25
	addpd	xmm3, [srcreg+d1+32]	;; nxt B2 = B2 + R2			;42-45	;23-26
	xprefetch [srcreg+srcinc+d1]
	xprefetchw [dstreg+dstinc+e1]
	addpd	xmm6, [srcreg+d2+d1+32]	;; nxt B4 = B4 + R4			;44-47	;24-27
	 addpd	xmm9, xmm5		;; I4 = I2 + I4 (mid I2)		;46-49	;25-28
	mulpd	xmm2, [screg+off6]	;; nxt A2 = A2 * sine (new R2)		;45-48	;25-28
	 addpd	xmm11, xmm7		;; R4 = R2 + R4 (mid R2)		;48-51	;26-29
	mulpd	xmm4,[screg+off8]	;; nxt A4 = A4 * sine (new R4)		;47-50	;26-29
	subpd	xmm10, xmm5		;; R3 = R3 - I4 (final R3)		;50-53	;27-30
	mulpd	xmm3, [screg+off6]	;; nxt B2 = B2 * sine (new I2)		;49-52	;27-30
	subpd	xmm14, xmm7		;; I3 = I3 - R4 (final I4)		;52-55	;28-31
	mulpd	xmm6, [screg+off8]	;; nxt B4 = B4 * sine (new I4)		;51-54	;28-31
	subpd	xmm1, xmm9		;; I1 = I1 - I2 (final I2)		;54-57	;29-32
	mulpd	xmm5, xmm15		;; mid I4 * 2				;53-56	;29-32
	xprefetch [srcreg+srcinc+d2]
	xprefetchw [dstreg+dstinc+e2]
	subpd	xmm0, xmm11		;; R1 = R1 - R2 (final R2)		;56-59	;30-33
	mulpd	xmm7, xmm15		;; mid R4 * 2				;55-58	;30-33
	 subpd	xmm2, xmm4		;; nxt R2 = R2 - R4 (mid R4)		;58-61	;31-34
	mulpd	xmm9, xmm15		;; mid I2 * 2				;57-60	;31-34
	movapd	[dstreg+e1], xmm10	;; Save R3					;31-34	;A
	 subpd	xmm3, xmm6		;; nxt I2 = I2 - I4 (mid I4)		;60-63	;32-35
	movapd	[dstreg+e1+48], xmm14	;; Save I4					;32-35	;AE
	addpd	xmm5, xmm10		;; I4 = R3 + I4 (final R4)		;62-65	;33-36
	addpd	xmm7, xmm14		;; R4 = I3 + R4 (final I3)		;64-67	;34-37
	 mulpd	xmm4, xmm15		;; nxt new R4 * 2			;65-68*	;34-37*
	xprefetch [srcreg+srcinc+d2+d1]
	xprefetchw [dstreg+dstinc+e2+e1]
	addpd	xmm9, xmm1		;; I2 = I1 + I2 (final I1)		;66-69	;35-38
	 mulpd	xmm6, xmm15		;; nxt new I4 * 2			;67-70*	;35-38*
	 movapd	xmm10, [srcreg+48]	;; nxt I1						;E
	 subpd	xmm10, xmm13		;; nxt I1 = I1 - I3 (mid I3)		;68-71	;36-39
	 addpd	xmm13, [srcreg+48]	;; nxt I3 = I1 + I3 (mid I1)		;70-73	;37-39
	 addpd	xmm4, xmm2		;; nxt R4 = R2 + R4 (mid R2)		;72-75	;38-41
	movapd	[dstreg+48], xmm1	;; Save I2					; 33-36
	 addpd	xmm6, xmm3		;; nxt I4 = I2 + I4 (mid I2)		;74-77	;39-42
	movapd	[dstreg+32], xmm0	;; Save R2					; 34-37
	subpd	xmm8, xmm3		;; nxt R3 = R3 - I4 (final R3)		;76-79	;40-43
	mulpd	xmm11, xmm15		;; mid R2 * 2				;77-80*	;40-43*
	movapd	[dstreg+e1+32], xmm5	;; Save R4					; 37-40
	subpd	xmm10, xmm2		;; nxt I3 = I3 - R4 (final I4)		;78-81	;41-44
	mulpd	xmm3, xmm15		;; nxt mid I4 * 2			;79-82	;41-44
	movapd	[dstreg+e1+16], xmm7	;; Save I3					; 38-41
	subpd	xmm12, xmm4		;; nxt R1 = R1 - R2 (final R2)		;80-83	;42-45
	mulpd	xmm2, xmm15		;; nxt mid R4 * 2			;81-84	;42-45
	movapd	[dstreg+16], xmm9	;; Save I1					; 39-42
	subpd	xmm13, xmm6		;; nxt I1 = I1 - I2 (final I2)		;82-85	;43-46
	mulpd	xmm4, xmm15		;; nxt mid R2 *	2			;83-86	;43-46
	addpd	xmm11, xmm0		;; R2 = R1 + R2 (final R1)		;84-87	;44-47
	mulpd	xmm6, xmm15		;; nxt mid I2 * 2			;85-88	;44-47
	movapd	[dstreg+e2+e1], xmm8	;; nxt Save R3					; 44-47
	addpd	xmm3, xmm8		;; nxt I4 = R3 + I4 (final R4)		;87-90	;45-48
	movapd	[dstreg+e2+e1+48], xmm10;; nxt Save I4					; 45-48
	addpd	xmm2, xmm10		;; nxt R4 = I3 + R4 (final I3)		;89-92	;46-49
	movapd	[dstreg+e2+32], xmm12	;; nxt Save R2					; 46-49
	addpd	xmm4, xmm12		;; nxt R2 = R1 + R2 (final R1)		;91-94	;47-50
	movapd	[dstreg+e2+48], xmm13	;; nxt Save I2					; 47-50
	addpd	xmm6, xmm13		;; nxt I2 = I1 + I2 (final I1)		;93-96	;48-51
	movapd	[dstreg], xmm11		;; Save R1					; 48-51
	movapd	[dstreg+e2+e1+32], xmm3	;; nxt Save R4					; 49-52
	movapd	[dstreg+e2+e1+16], xmm2	;; nxt Save I3					; 50-53
	movapd	[dstreg+e2], xmm4	;; nxt Save R1					; 51-54
	movapd	[dstreg+e2+16], xmm6	;; nxt Save I1					; 52-55
	ENDM

;; Cheat sheet for scheduling dependency chains (and num registers required)
;;	      12345678901234567890123456789012345678901234567890123456789012345678901234567890
;;r24(i2)     AAAA
;;r57(i4)      AAAA
;;r68(r4)       AAAA
;;r13(r2)        AAAA
;;r24(i1)         AAAA
;;r68(i3)          AAAA
;;r57(r3)           AAAA
;;r13(r1)            AAAA
;;mI4(depI2I4)	      AAAA
;;mR4(depR2R4)	       AAAA
;;mI3(depI1I3)	        AAAA			
;;mR3(depR1R3)	         AAAA			
;;nxt r24(i2)             AAAA
;;mI2(depI2I4)	       MMMMAAAA
;;mR2(depR2R4)	        MMMMAAAA
;;mI1(depI1I3)		 MMMMAAAA				
;;B4		          MMMMAAAAMMMM
;;A4		           MMMMAAAAMMMM
;;B3                        MMMMAAAAMMMM
;;A3                         MMMMAAAAMMMM
;;B2		              MMMMAAAAMMMM
;;A2		               MMMMAAAAMMMM				
;;nxt r24(i1)                       AAAA
;;nxt r57(i4)                        AAAA
;;nxt r68(r4)		              AAAA
;;nxt r13(r2)		               AAAA
;;nxt r68(i3)		                AAAA
;;nxt r57(r3)	                         AAAA
;;nxt r13(r1)		                  AAAA
;;nxt mI4(depI2I4)                         AAAA
;;nxt mR4(depR2R4)	                    AAAA
;;nxt mI3(depI1I3)			     AAAA
;;nxt mR3(depR1R3)		              AAAA
;;mR1(depR1R3)	                           MMMMAAAA
;;nxt mI2(depI2I4)                          MMMMAAAA
;;nxt mR2(depR2R4)		             MMMMAAAA
;;nxt mI1(depI1I3)	                      MMMMAAAA
;;nxt mR1(depR1R3)                             MMMMAAAA
;;nxt B4			                MMMMAAAAMMMM
;;nxt A4			                 MMMMAAAAMMMM
;;nxt B3				          MMMMAAAAMMMM
;;nxt A3				           MMMMAAAAMMMM
;;nxt B2			                    MMMMAAAAMMMM
;;nxt A2			                     MMMMAAAAMMMM

x4cl_unfft_cmn MACRO srcreg,srcinc,d1,d2,screg,off2,off3,off4,off6,off7,off8
	movapd	xmm1, [srcreg+32]	;; mem2 (I1)				;K8	;K10
	movapd	xmm0, [srcreg+d1+32]	;; mem4 (I2)
	subpd	xmm1, xmm0		;; new I2 = I1 - I2			;1-4	;1-4	;23456789ABCDEF
	movapd	xmm6, [srcreg+d2+d1]	;; mem7 (R4)
	movapd	xmm7, [srcreg+d2]	;; mem5 (R3)
	subpd	xmm6, xmm7		;; new I4 = R4 - R3			;3-6	;2-5	;234589ABCDEF
	movapd	xmm4, [srcreg+d2+32]	;; mem6 (I3)
	movapd	xmm3, [srcreg+d2+d1+32]	;; mem8 (I4)
	subpd	xmm4, xmm3		;; new R4 = I3 - I4			;5-8	;3-6	;2589ABCDEF
	movapd	xmm12, [srcreg]		;; mem1 (R1)
	movapd	xmm11, [srcreg+d1]	;; mem3 (R2)
	subpd	xmm12, xmm11		;; new R2 = R1 - R2			;7-10	;4-7	;2589ADEF
	addpd	xmm0, [srcreg+32]	;; new I1 = I1 + I2			;9-12	;5-8
	addpd	xmm3, [srcreg+d2+32]	;; new I3 = I3 + I4			;11-14	;6-9
	addpd	xmm7, [srcreg+d2+d1]	;; new R3 = R3 + R4			;13-16	;7-10
	addpd	xmm11, [srcreg]		;; new R1 = R1 + R2			;15-18	;8-11
	xprefetchw [srcreg+srcinc]
	 subpd	xmm1, xmm6		;; I2 = I2 - I4 (mid I4)		;17-20	;9-12
	 movapd	xmm15, XMM_TWO									;2589ADE
	 subpd	xmm12, xmm4		;; R4 = R2 - R4 (mid R4)		;19-22	;10-13
	 mulpd	xmm6, xmm15		;; new I4 * 2				;20-23	;10-13
	movapd	xmm9, [srcreg+48]	;; nxt mem2 (I1)					;258ADE
	 subpd	xmm0, xmm3		;; I1 = I1 - I3 (mid I3)		;21-24	;11-14
	 mulpd	xmm4, xmm15		;; new R4 * 2				;22-25	;11-14
	movapd	xmm5, [srcreg+d1+48]	;; nxt mem4 (I2)					;28ADE
	 subpd	xmm11, xmm7		;; R3 = R1 - R3 (mid R3)		;23-26	;12-15
	 mulpd	xmm3, xmm15		;; new I3 * 2				;24-27	;12-15
	movapd	xmm8, [screg+off4+16]	;; B4 = pre_real/pre_imag				;2ADE
	subpd	xmm9, xmm5		;; nxt new I2 = I1 - I2			;25-28	;13-16
	mulpd	xmm8, xmm1		;; B4 = I4 * pre_real/pre_imag		;26-29	;13-16
	movapd	xmm2, [screg+off4+16]	;; A4 = pre_real/pre_imag				;ADE
	 addpd	xmm6, xmm1		;; I4 = I2 + I4 (mid I2)		;27-30	;14-17
	mulpd	xmm2, xmm12		;; A4 = R4 * pre_real/pre_imag		;28-31	;14-17
	movapd	xmm13, [screg+off3+16]	;; B3 = pre_real/pre_imag				;AE
	 addpd	xmm4, xmm12		;; R2 = R2 + R4 (mid R2)		;29-32	;15-18
	mulpd	xmm13, xmm0		;; B3 = I3 * pre_real/pre_imag		;30-33	;15-18
	movapd	xmm10, [screg+off3+16]	;; A3 = pre_real/pre_imag				;E
	 addpd	xmm3, xmm0		;; I3 = I1 + I3 (mid & final I1)	;31-34	;16-19
	mulpd	xmm10, xmm11		;; A3 = R3 * pre_real/pre_imag		;32-35	;16-19
	movapd	xmm14, [screg+off2+16]	;; B2 = pre_real/pre_imag				;
	subpd	xmm8, xmm12		;; B4 = B4 - R4				;33-36	;17-20	;C
	mulpd	xmm14, xmm6		;; B2 = I2 * pre_real/pre_imag		;34-37	;17-20
	movapd	xmm12, [screg+off2+16]	;; A2 = pre_real/pre_imag				;
	addpd	xmm2, xmm1		;; A4 = A4 + I4				;35-38	;18-21	;1
	mulpd	xmm12, xmm4		;; A2 = R2 * pre_real/pre_imag		;36-39	;18-21
	movapd	xmm1, [screg+off4]	;; pre_imag						;
	subpd	xmm13, xmm11		;; B3 = B3 - R3				;37-40	;19-22
	xprefetchw [srcreg+srcinc][d1]
	addpd	xmm10, xmm0		;; A3 = A3 + I3				;39-42	;20-23	;0
	movapd	[srcreg+32], xmm3	;; Save I1					; 20-23	;03
	subpd	xmm14, xmm4		;; B2 = B2 - R2				;41-44	;21-24	;034
	mulpd	xmm8, xmm1		;; B4 = B4 * pre_imag (final I4)	;42-45	;21-24
	movapd	xmm0, [srcreg+d2+d1+16]	;; nxt mem7 (R4)					;34
	movapd	xmm3, [srcreg+d2+16]	;; nxt mem5 (R3)					;4
	addpd	xmm12, xmm6		;; A2 = A2 + I2				;43-46	;22-25	;46
	mulpd	xmm2, xmm1		;; A4 = A4 * pre_imag (final R4)	;44-47	;22-25	;146
	movapd	xmm6, [screg+off3]	;; pre_imag						;14
	addpd	xmm5, [srcreg+48]	;; nxt new I1 = I1 + I2			;45-48	;23-26
	mulpd	xmm13, xmm6		;; B3 = B3 * pre_imag (final I3)	;46-49	;23-26
	movapd	xmm4, [srcreg+d2+48]	;; nxt mem6 (I3)					;1
	subpd	xmm0, xmm3		;; nxt new I4 = R4 - R3			;47-50	;24-27
	mulpd	xmm10, xmm6		;; A3 = A3 * pre_imag (final R3)	;48-51	;24-27	;16
	movapd	xmm1, [srcreg+d2+d1+48]	;; nxt mem8 (I4)					;6
	subpd	xmm4, xmm1		;; nxt new R4 = I3 - I4			;49-52	;25-28
	mulpd	xmm14, [screg+off2]	;; B2 = B2 * pre_imag (final I2)	;50-53	;25-28
	movapd	xmm6, [srcreg+16]	;; nxt mem1 (R1)					;
	movapd	[srcreg+d2+16], xmm2	;; Save R4					; 26-29	;2
	movapd	xmm2, [srcreg+d1+16]	;; nxt mem3 (R2)					;
	subpd	xmm6, xmm2		;; nxt new R2 = R1 - R2			;51-54	;26-29
	mulpd	xmm12, [screg+off2]	;; A2 = A2 * pre_imag (final R2)	;52-55	;26-29
	addpd	xmm1, [srcreg+d2+48]	;; nxt new I3 = I3 + I4			;53-56	;27-30
	xprefetchw [srcreg+srcinc+d2]
	addpd	xmm3, [srcreg+d2+d1+16]	;; nxt new R3 = R3 + R4			;55-58	;28-31
	addpd	xmm2, [srcreg+16]	;; nxt new R1 = R1 + R2			;57-60	;29-32
	xprefetchw [srcreg+srcinc+d2][d1]
	 subpd	xmm9, xmm0		;; nxt I2 = I2 - I4 (mid I4)		;59-62	;30-33
	 mulpd	xmm7, xmm15		;; new R3 * 2				;60-63	;30-33
	movapd	[srcreg+d2+48], xmm8	;; Save I4					; 25-28	;8
	 subpd	xmm6, xmm4		;; nxt R4 = R2 - R4 (mid R4)		;61-64	;31-34
	 mulpd	xmm0, xmm15		;; nxt new I4 * 2			;62-65	;31-34
	movapd	[srcreg+48], xmm13	;; Save I3					; 27-30	;8D
	 subpd	xmm5, xmm1		;; nxt I1 = I1 - I3 (mid I3)		;63-66	;32-35
	 mulpd	xmm4, xmm15		;; nxt new R4 * 2			;64-67	;32-35
	movapd	[srcreg+16], xmm10	;; Save R3					; 28-31	;8AD
	 subpd	xmm2, xmm3		;; nxt R3 = R1 - R3 (mid R3)		;65-68	;33-36
	 mulpd	xmm1, xmm15		;; nxt new I3 * 2			;66-69	;33-36
	movapd	xmm8, [screg+off8+16]	;; nxt B4 = pre_real/pre_imag				;AD
	 addpd	xmm7, xmm11		;; R1 = R1 + R3 (mid and final R1)	;67-70	;34-37	;ABD
	 mulpd	xmm3, xmm15		;; nxt new R3 * 2			;68-71	;34-37
	movapd	[srcreg+d2+32], xmm14	;; Save I2					; 29-32	;ABDE
	movapd	xmm11, [screg+off8+16]	;; nxt A4 = pre_real/pre_imag				;ADE
	 addpd	xmm0, xmm9		;; nxt I4 = I2 + I4 (mid I2)		;69-72	;35-38
	mulpd	xmm8, xmm9		;; nxt B4 = I4 * pre_real/pre_imag	;70-73	;35-38
	movapd	[srcreg+d2], xmm12	;; Save R2					; 30-33	;ACDE
	movapd	xmm13, [screg+off7+16]	;; nxt B3 = pre_real/pre_imag				;ACE
	 addpd	xmm4, xmm6		;; nxt R2 = R2 + R4 (mid R2)		;71-74	;36-39
	mulpd	xmm11, xmm6		;; nxt A4 = R4 * pre_real/pre_imag	;72-75	;36-39
	movapd	xmm10, [screg+off7+16]	;; nxt A3 = pre_real/pre_imag				;CE
	 addpd	xmm1, xmm5		;; nxt I3 = I1 + I3 (mid and final I1)	;73-76	;37-40
	mulpd	xmm13, xmm5		;; nxt B3 = I3 * pre_real/pre_imag	;74-77	;37-40
	movapd	xmm14, [screg+off6+16]	;; nxt B2 = pre_real/pre_imag				;C
	 addpd	xmm3, xmm2		;; nxt R1 = R1 + R3 (mid and final  R1)	;75-78	;38-41
	mulpd	xmm10, xmm2		;; nxt A3 = R3 * pre_real/pre_imag	;76-79	;38-41
	movapd	xmm12, [screg+off6+16]	;; nxt A2 = pre_real/pre_imag				;
	subpd	xmm8, xmm6		;; nxt B4 = B4 - R4			;77-80	;39-42	;6
	mulpd	xmm14, xmm0		;; nxt B2 = I2 * pre_real/pre_imag	;78-81	;39-42
	movapd	[srcreg], xmm7		;; Save R1					; 38-41	;67
	addpd	xmm11, xmm9		;; nxt A4 = A4 + I4			;79-82	;40-43	;679
	mulpd	xmm12, xmm4		;; nxt A2 = R2 * pre_real/pre_imag	;80-83	;40-43
	movapd	xmm6, [screg+off8]	;; pre_imag						;79
	subpd	xmm13, xmm2		;; nxt B3 = B3 - R3			;81-84	;41-44	;279
	movapd	xmm7, [screg+off7]								;29
	addpd	xmm10, xmm5		;; nxt A3 = A3 + I3			;83-86	;42-45	;+5
	movapd	[srcreg+d1+32], xmm1	;; nxt Save I1					; 42-45	;+1
	subpd	xmm14, xmm4		;; nxt B2 = B2 - R2			;85-88	;43-46	;+4
	mulpd	xmm8, xmm6		;; nxt B4 = B4 * pre_imag (final I4)	;86-89	;43-46
	movapd	xmm9, [screg+off6]	;; pre_imag						;-9
	addpd	xmm12, xmm0		;; nxt A2 = A2 + I2			;87-90	;44-47	;+0
	mulpd	xmm11, xmm6		;; nxt A4 = A4 * pre_imag (final R4)	;88-91	;44-47	;+6
	movapd	[srcreg+d1], xmm3	;; nxt Save R1					; 43-46	;+0
	mulpd	xmm13, xmm7		;; nxt B3 = B3 * pre_imag (final I3)	;90-93	;45-48
	mulpd	xmm10, xmm7		;; nxt A3 = A3 * pre_imag (final R3)	;92-95	;46-49	;+7
	mulpd	xmm14, xmm9		;; nxt B2 = B2 * pre_imag (final I2)	;94-97	;47-50
	movapd	[srcreg+d2+d1+48], xmm8	;; nxt Save I4					; 47-50	;+8
	mulpd	xmm12, xmm9		;; nxt A2 = A2 * pre_imag (final R2)	;96-99	;48-51	;+9
	movapd	[srcreg+d2+d1+16], xmm11;; nxt Save R4					; 48-51
	movapd	[srcreg+d1+48], xmm13	;; nxt Save I3					; 49-52
	movapd	[srcreg+d1+16], xmm10	;; nxt Save R3					; 50-53
	movapd	[srcreg+d2+d1+32], xmm14;; nxt Save I2					; 51-54
	movapd	[srcreg+d2+d1], xmm12	;; nxt Save R2					; 52-55
	ENDM

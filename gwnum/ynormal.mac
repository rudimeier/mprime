; Copyright 2011-2012 - Mersenne Research, Inc.  All rights reserved.
; Author:  George Woltman
; Email: woltman@alum.mit.edu
;
; These macros efficiently implement the normalization to integers
; and multiplication by two-to-phi powers using SSE2 instructions.
;

; Utility macros used in normalization macros

; Compute absolute value of a ymm register.  In 64-bit mode we assume
; ymm15 has been preloaded with the necessary constant.
 
absval MACRO ymmreg
	IFDEF X86_64
	vandpd	ymmreg, ymmreg, ymm15		;; Compute absolute value
	ELSE
	vandpd	ymmreg, ymmreg, YMM_ABSVAL	;; Compute absolute value
	ENDIF
	ENDM

; These macros implement the variants of the normalization routines
; in a non-pipelined way.  It is simply too much work to hand optimize
; all normalization variants.

; Compute the convolution error and if greater than MAXERR, set MAXERR

error_check MACRO ymmreg, tmpreg, errreg
	vroundpd tmpreg, ymmreg, 0		;; Convert to an integer
	vsubpd	tmpreg, tmpreg, ymmreg		;; This is the convolution error
	absval	tmpreg				;; Compute absolute value
	vmaxpd	errreg, errreg, tmpreg		;; Compute maximum error
	ENDM

error_check_interleaved MACRO ymmreg1, tmpreg1, ymmreg2, tmpreg2, errreg
	vroundpd tmpreg1, ymmreg1, 0		;; Convert to an integer
	vroundpd tmpreg2, ymmreg2, 0		;; Convert to an integer
	vsubpd	tmpreg1, tmpreg1, ymmreg1	;; This is the convolution error
	vsubpd	tmpreg2, tmpreg2, ymmreg2	;; This is the convolution error
	absval	tmpreg1				;; Compute absolute value
	absval	tmpreg2				;; Compute absolute value
	vmaxpd	errreg, errreg, tmpreg1		;; Compute maximum error
	vmaxpd	errreg, errreg, tmpreg2		;; Compute maximum error
	ENDM

; In general, normalization routines calculate:
;		newFFTvalue = (FFTvalue * const + carry) % base
;		carry = (FFTvalue * const + carry) / base
; Since FFTvalue * const can exceed 51 bits, we instead split FFTvalue into:
;		hi = FFTvalue / base
;		lo = FFTvalue % base
; and then calculate:
;		newFFTvalue = (lo * const + carry) % base
;		carry = hi * const + (lo * const + carry) / base
;
; For the b = 2 case, we can split hi and lo using any power of 2 larger
; than the FFT base, this allows for some simpler code in this case.



; These routines split an FFT value and then multiplies it by the small constant.
; The previous carry is added in and a new carry is generated from the high
; bits of the FFT value.

mul_by_const MACRO ttp, base2, echk, ymmval, ymmcarry, ymmtmp1, ymmtmp2, biglitreg, errreg
base2	 base2_mul_by_const echk, ymmval, ymmcarry, ymmtmp1, ymmtmp2, errreg
no base2 ttp nobase2_mul_by_const echk, ymmval, ymmcarry, ymmtmp1, ymmtmp2, biglitreg, errreg
no base2 no ttp nobase2_mul_by_const echk, ymmval, ymmcarry, ymmtmp1, ymmtmp2, 0, errreg
	ENDM

base2_mul_by_const MACRO echk, ymmval, ymmcarry, ymmtmp1, ymmtmp2, errreg
IFNDEF X86_64
	vmovapd	ymmtmp2, YMM_BIGBIGVAL		;; Round to nearest multiple of 2^25
	vaddpd	ymmtmp1, ymmval, ymmtmp2
	vsubpd	ymmtmp1, ymmtmp1, ymmtmp2	;; ymmtmp1 is the high bits of FFT value
ELSE
	vaddpd	ymmtmp1, ymmval, ymm14		;; Round to nearest multiple of 2^25
	vsubpd	ymmtmp1, ymmtmp1, ymm14		;; ymmtmp1 is the high bits of FFT value
ENDIF
	vroundpd ymmtmp2, ymmval, 0		;; Round FFT value to an integer
echk	vsubpd	ymmval, ymmval, ymmtmp2		;; This is the convolution error
echk	absval	ymmval				;; Compute absolute value
echk	vmaxpd	errreg, errreg, ymmval		;; Compute maximum error
	vsubpd	ymmval, ymmtmp2, ymmtmp1	;; ymmval now contains low 25 bits of FFT value
IFNDEF X86_64
	vmovapd	ymmtmp2, YMM_MULCONST
	vmulpd	ymmval, ymmval, ymmtmp2		;; Multiply low bits of FFT value by the small constant
	vaddpd	ymmval, ymmval, ymmcarry	;; Add in the previous carry
	vmulpd	ymmcarry, ymmtmp1, ymmtmp2	;; Next carry = high bits of FFT value times the small constant
ELSE
	vmulpd	ymmval, ymmval, ymm13		;; Multiply low bits of FFT value by the small constant
	vaddpd	ymmval, ymmval, ymmcarry	;; Add in the previous carry
	vmulpd	ymmcarry, ymmtmp1, ymm13	;; Next carry = high bits of FFT value times the small constant
ENDIF
	ENDM

nobase2_mul_by_const MACRO echk, ymmval, ymmcarry, ymmtmp1, ymmtmp2, basereg, errreg
	vmulpd	ymmtmp1, ymmval, YMM_LIMIT_INVERSE[basereg]	;; Compute FFT value / base
	vroundpd ymmtmp2, ymmval, 0				;; Round FFT value to an integer
	vroundpd ymmtmp1, ymmtmp1, 0				;; Round FFT value / base to an integer
echk	vsubpd	ymmval, ymmval, ymmtmp2				;; This is the convolution error
echk	absval	ymmval						;; Compute absolute value
echk	vmaxpd	errreg, errreg, ymmval				;; Compute maximum error
	vmulpd	ymmval, ymmtmp1, YMM_LIMIT_BIGMAX[basereg]	;; round (FFT value / base) * base
	vsubpd	ymmval, ymmtmp2, ymmval				;; This is FFTvalue % base
IFNDEF X86_64
	vmovapd	ymmtmp2, YMM_MULCONST
	vmulpd	ymmval, ymmval, ymmtmp2				;; Multiply FFTvalue % base by the small constant
	vaddpd	ymmval, ymmval, ymmcarry			;; Add in the previous carry
	vmulpd	ymmcarry, ymmtmp1, ymmtmp2			;; Next carry = round (FFTvalue / base) * the small constant
ELSE
	vmulpd	ymmval, ymmval, ymm13				;; Multiply FFTvalue % base by the small constant
	vaddpd	ymmval, ymmval, ymmcarry			;; Add in the previous carry
	vmulpd	ymmcarry, ymmtmp1, ymm13			;; Next carry = round (FFTvalue / base) * the small constant
ENDIF
	ENDM


	;; BUG BUG, I think the code below will do the mul by const with one register.  Perfect for interleaving!!
IFDEF  TRY_THIS_CODE
base2
	vmovapd	ymmtmp, ymmval			;; Copy value (can't be avoided without using a 2nd temp reg)
	vroundpd ymmval, ymmval, 0		;; Round FFT value to an integer
echk	vsubpd	ymmtmp, ymmval, ymmtmp		;; This is the convolution error
echk	absval	ymmtmp				;; Compute absolute value
echk	vmaxpd	errreg, errreg, ymmtmp		;; Compute maximum error
	vaddpd	ymmtmp, ymmval, YMM_BIGBIGVAL	;; Round to nearest multiple of 2^25
	vsubpd	ymmtmp, ymmtmp, YMM_BIGBIGVAL	;; ymmtmp is the high bits of FFT value
	vsubpd	ymmval, ymmval, ymmtmp		;; ymmval now contains low 25 bits of FFT value
	vmulpd	ymmval, ymmval, YMM_MULCONST	;; Multiply low bits of FFT value by the small constant
	vaddpd	ymmval, ymmval, ymmcarry	;; Add in the previous carry
	vmulpd	ymmcarry, ymmtmp, YMM_MULCONST	;; Next carry = high bits of FFT value times the small constant

nobase2:
	vmovapd	ymmtmp, ymmval					;; Copy value (can't be avoided without using a 2nd temp reg)
	vroundpd ymmval, ymmval, 0				;; Round FFT value to an integer
echk	vsubpd	ymmtmp, ymmval, ymmtmp				;; This is the convolution error
echk	absval	ymmtmp						;; Compute absolute value
echk	vmaxpd	errreg, errreg, ymmtmp				;; Compute maximum error
	vmulpd	ymmtmp, ymmval, YMM_LIMIT_INVERSE[basereg]	;; Compute FFT value / base
	vroundpd ymmtmp, ymmtmp, 0				;; Round FFT value / base to an integer
	vsubpd	ymmval, ymmval, ymmtmp				;; Compute fractional part of FFT value / base
	vmulpd	ymmval, ymmval, YMM_LIMIT_BIGMAX[basereg]	;; This is FFT value %
	vmulpd	ymmval, ymmval, YMM_MULCONST			;; Multiply FFTvalue % base by the small constant
	vaddpd	ymmval, ymmval, ymmcarry			;; Add in the previous carry
	vmulpd	ymmcarry, ymmtmp, YMM_MULCONST			;; Next carry = round (FFTvalue / base) * the small constant
ENDIF



mul_by_const_interleaved MACRO ttp, base2, echk, ymmval, ymmcarry, ymmtmp, basereg, ymmval2, ymmcarry2, ymmtmp2, basereg2, errreg
base2	 base2_mul_by_const_interleaved echk, ymmval, ymmcarry, ymmtmp, ymmval2, ymmcarry2, ymmtmp2, errreg
no base2 ttp nobase2_mul_by_const_interleaved echk, ymmval, ymmcarry, ymmtmp, basereg, ymmval2, ymmcarry2, ymmtmp2, basereg2, errreg
no base2 no ttp nobase2_mul_by_const_interleaved echk, ymmval, ymmcarry, ymmtmp, 0, ymmval2, ymmcarry2, ymmtmp2, 0, errreg
	ENDM

base2_mul_by_const_interleaved MACRO echk, ymmval, ymmcarry, ymmtmp, ymmval2, ymmcarry2, ymmtmp2, errreg
	base2_mul_by_const echk, ymmval, ymmcarry, ymmtmp, ymmtmp2, errreg
	base2_mul_by_const echk, ymmval2, ymmcarry2, ymmtmp, ymmtmp2, errreg
;; BUG -  we may need a third temporary register to do this
;	vmovapd	ymmtmp, YMM_BIGBIGVAL
;	vaddpd	ymmcarry, ymmval, ymmtmp	;; Round to nearest multiple of 2^25
;	vaddpd	ymmcarry2, ymmval2, ymmtmp	;; Round to nearest multiple of 2^25
;	vsubpd	ymmcarry, ymmcarry, ymmtmp
;	vsubpd	ymmcarry2, ymmcarry2, ymmtmp
;	vroundpd ymmtmp, ymmval, 0		;; Round to an integer
;	vroundpd ymmtmp2, ymmval2, 0		;; Round to an integer
;echk	vsubpd	ymmval, ymmval, ymmtmp		;; This is the convolution error
;echk	vsubpd	ymmval2, ymmval2, ymmtmp2	;; This is the convolution error
;echk	absval	ymmval				;; Compute absolute value
;echk	absval	ymmval2				;; Compute absolute value
;echk	vmaxpd	errreg, errreg, ymmval		;; Compute maximum error
;echk	vmaxpd	errreg, errreg, ymmval2		;; Compute maximum error
;	vsubpd	ymmval, ymmtmp, ymmcarry	;; ymmreg now contains low 25 bits
;	vsubpd	ymmval2, ymmtmp2, ymmcarry2	;; ymmreg2 now contains low 25 bits
;	vmovapd	ymmtmp, YMM_MULCONST
;	vmulpd	ymmcarry, ymmcarry, ymmtmp	;; Multiply by the small constant
;	vmulpd	ymmcarry2, ymmcarry2, ymmtmp	;; Multiply by the small constant
;	vmulpd	ymmval, ymmval, ymmtmp
;	vmulpd	ymmval2, ymmval2, ymmtmp
	ENDM

nobase2_mul_by_const_interleaved MACRO echk, ymmval, ymmcarry, ymmtmp, basereg, ymmval2, ymmcarry2, ymmtmp2, basereg2, errreg
	nobase2_mul_by_const echk, ymmval, ymmcarry, ymmtmp, ymmtmp2, basereg, errreg
	nobase2_mul_by_const echk, ymmval2, ymmcarry2, ymmtmp, ymmtmp2, basereg2, errreg
;;;;; bugs!!!!
;;;xxx	vmulpd	ymmcarry, ymmval, YMM_LIMIT_INVERSE[basereg]	;; Compute FFTvalue / base
;;	vmulpd	ymmcarry2, ymmval2, YMM_LIMIT_INVERSE[basereg2] ;; Compute FFTvalue / base
;;	vroundpd ymmtmp, ymmval, 0				;; Round to an integer
;;	vroundpd ymmtmp2, ymmval2, 0				;; Round to an integer
;;	vroundpd ymmcarry, ymmcarry, 0				;; Round to an integer				;;BUG - isn't reghi already an integer???
;;	vroundpd ymmcarry2, ymmcarry2, 0			;; Round to an integer
;;echk	vsubpd	ymmval, ymmval, ymmtmp				;; This is the convolution error
;;echk	vsubpd	ymmval2, ymmval2, ymmtmp2			;; This is the convolution error
;;echk	absval	ymmval						;; Compute absolute value
;;echk	absval	ymmval2						;; Compute absolute value
;;echk	vmaxpd	errreg, errreg, ymmval				;; Compute maximum error
;;echk	vmaxpd	errreg, errreg, ymmval2				;; Compute maximum error
;;	vmulpd	ymmval, ymmcarry, YMM_LIMIT_BIGMAX[basereg]
;;	vmulpd	ymmval2, ymmcarry2, YMM_LIMIT_BIGMAX[basereg2]
;;	vsubpd	ymmval, ymmtmp, ymmval				;; This is FFTvalue % base
;;	vsubpd	ymmval2, ymmtmp2, ymmval2			;; This is FFTvalue % base
;;	vmovapd	ymmval, YMM_MULCONST
;;	vmulpd	ymmcarry, ymmcarry, ymmtmp			;; Multiply by the small constant
;;	vmulpd	ymmcarry2, ymmcarry2, ymmtmp			;; Multiply by the small constant
;;	vmulpd	ymmval, ymmval, ymmtmp
;;	vmulpd	ymmval2, ymmval2, ymmtmp
	ENDM

;
; These macros do the base2 and nobase2 roundings
; const - set to exec if mul_by_const has already computed part of the next carry.
;	  One can also assume ymmval has been rounded if const is set.
; ymmval - input: number to round, output: value to store in the FFT
; ymmcarry - input: part of the next carry if mulbyconst set, output: the next carry
; ymmtmp - a temporary register
;

rounding MACRO ttp, base2, const, ymmval, ymmcarry, ymmtmp, basereg
base2 const ttp		base2_const_rounding ymmval, ymmcarry, ymmtmp, basereg
base2 const no ttp	base2_const_rounding ymmval, ymmcarry, ymmtmp, 0
base2 no const ttp	base2_noconst_rounding ymmval, ymmcarry, ymmtmp, basereg
base2 no const no ttp	base2_noconst_rounding ymmval, ymmcarry, ymmtmp, 0
no base2 const ttp	nobase2_const_rounding ymmval, ymmcarry, ymmtmp, basereg
no base2 const no ttp	nobase2_const_rounding ymmval, ymmcarry, ymmtmp, 0
no base2 no const ttp	nobase2_noconst_rounding ymmval, ymmcarry, ymmtmp, basereg
no base2 no const no ttp nobase2_noconst_rounding ymmval, ymmcarry, ymmtmp, 0
	ENDM

base2_noconst_rounding MACRO ymmval, ymmcarry, ymmtmp, basereg
	vmovapd	ymmtmp, YMM_LIMIT_BIGMAX[basereg]		;; Load maximum * BIGVAL - BIGVAL
	vaddpd	ymmcarry, ymmval, ymmtmp			;; y = top bits of val
	vsubpd	ymmtmp, ymmcarry, ymmtmp			;; z = y - (maximum * BIGVAL - BIGVAL)
	vsubpd	ymmval, ymmval, ymmtmp				;; rounded val = x - z
	vmulpd	ymmcarry, ymmcarry, YMM_LIMIT_INVERSE[basereg]	;; carry = shifted y
	ENDM

base2_const_rounding MACRO ymmval, ymmcarry, ymmtmp, basereg
	vaddpd	ymmtmp, ymmval, YMM_LIMIT_BIGMAX[basereg]	;; y = top bits of x
	vaddpd	ymmcarry, ymmcarry, ymmtmp			;; next carry = upper mul-by-const bits + y
	vsubpd	ymmtmp, ymmtmp, YMM_LIMIT_BIGMAX[basereg]	;; z = y - (maximum*BIGVAL-BIGVAL)
	vmulpd	ymmcarry, ymmcarry, YMM_LIMIT_INVERSE[basereg]	;; shift next carry appropriately
	vsubpd	ymmval, ymmval, ymmtmp				;; rounded value = x - z
	ENDM

nobase2_noconst_rounding MACRO ymmval, ymmcarry, ymmtmp, basereg
	vmulpd	ymmcarry, ymmval, YMM_LIMIT_INVERSE[basereg]	;; val / base
	vroundpd ymmval, ymmval, 0				;; Round val
	vroundpd ymmcarry, ymmcarry, 0				;; next carry = round (val / base)
	vmulpd	ymmtmp, ymmcarry, YMM_LIMIT_BIGMAX[basereg]	;; z = round (val / base) * base
	vsubpd	ymmval, ymmval, ymmtmp				;; new value = val - z
	ENDM

nobase2_const_rounding MACRO ymmval, ymmcarry, ymmtmp, basereg
	vmulpd	ymmtmp, ymmval, YMM_LIMIT_INVERSE[basereg]	;; val / base
	vroundpd ymmtmp, ymmtmp, 0				;; round (val / base)
	vaddpd	ymmcarry, ymmcarry, ymmtmp			;; next carry = partially computed next carry + round (val / base)
	vmulpd	ymmtmp, ymmtmp, YMM_LIMIT_BIGMAX[basereg]	;; z = round (val / base) * base
	vsubpd	ymmval, ymmval, ymmtmp				;; new value = val - z
	ENDM

; Same as above except interleaved for better scheduling

rounding_interleaved MACRO ttp, base2, const, ymmval, ymmcarry, ymmtmp, basereg, ymmval2, ymmcarry2, ymmtmp2, basereg2
base2 const ttp		base2_const_rounding_interleaved ymmval, ymmcarry, ymmtmp, basereg, ymmval2, ymmcarry2, ymmtmp2, basereg2
base2 const no ttp	base2_const_rounding_interleaved ymmval, ymmcarry, ymmtmp, 0, ymmval2, ymmcarry2, ymmtmp2, 0
base2 no const ttp	base2_noconst_rounding_interleaved ymmval, ymmcarry, ymmtmp, basereg, ymmval2, ymmcarry2, ymmtmp2, basereg2
base2 no const no ttp	base2_noconst_rounding_interleaved ymmval, ymmcarry, ymmtmp, 0, ymmval2, ymmcarry2, ymmtmp2, 0
no base2 const ttp	nobase2_const_rounding_interleaved ymmval, ymmcarry, ymmtmp, basereg, ymmval2, ymmcarry2, ymmtmp2, basereg2
no base2 const no ttp	nobase2_const_rounding_interleaved ymmval, ymmcarry, ymmtmp, 0, ymmval2, ymmcarry2, ymmtmp2, 0
no base2 no const ttp	nobase2_noconst_rounding_interleaved ymmval, ymmcarry, ymmtmp, basereg, ymmval2, ymmcarry2, ymmtmp2, basereg2
no base2 no const no ttp nobase2_noconst_rounding_interleaved ymmval, ymmcarry, ymmtmp, 0, ymmval2, ymmcarry2, ymmtmp2, 0
	ENDM

base2_noconst_rounding_interleaved MACRO ymmval, ymmcarry, ymmtmp, basereg, ymmval2, ymmcarry2, ymmtmp2, basereg2
	vmovapd	ymmtmp, YMM_LIMIT_BIGMAX[basereg]	;; Load maximum * BIGVAL - BIGVAL
	vaddpd	ymmcarry, ymmval, ymmtmp		;; y = top bits of val
	vmovapd	ymmtmp2, YMM_LIMIT_BIGMAX[basereg2]	;; Load maximum * BIGVAL - BIGVAL
	vaddpd	ymmcarry2, ymmval2, ymmtmp2		;; y = top bits of val
	vsubpd	ymmtmp, ymmcarry, ymmtmp		;; z = y - (maximum * BIGVAL - BIGVAL)
	vsubpd	ymmtmp2, ymmcarry2, ymmtmp2		;; z = y - (maximum * BIGVAL - BIGVAL)
	vsubpd	ymmval, ymmval, ymmtmp			;; rounded val = x - z
	vsubpd	ymmval2, ymmval2, ymmtmp2		;; rounded val = x - z
	vmulpd	ymmcarry, ymmcarry, YMM_LIMIT_INVERSE[basereg] ;; carry = shifted y
	vmulpd	ymmcarry2, ymmcarry2, YMM_LIMIT_INVERSE[basereg2] ;; carry = shifted y
	ENDM

base2_const_rounding_interleaved MACRO ymmval, ymmcarry, ymmtmp, basereg, ymmval2, ymmcarry2, ymmtmp2, basereg2
	vaddpd	ymmtmp, ymmval, YMM_LIMIT_BIGMAX[basereg]	;; y = top bits of x
	vaddpd	ymmtmp2, ymmval2, YMM_LIMIT_BIGMAX[basereg2]	;; y = top bits of x
	vaddpd	ymmcarry, ymmcarry, ymmtmp			;; next carry = y + upper mul-by-const bits
	vaddpd	ymmcarry2, ymmcarry2, ymmtmp2			;; next carry = y + upper mul-by-const bits
	vsubpd	ymmtmp, ymmtmp, YMM_LIMIT_BIGMAX[basereg]	;; z = y - (maximum*BIGVAL-BIGVAL)
	vsubpd	ymmtmp2, ymmtmp2, YMM_LIMIT_BIGMAX[basereg2]	;; z = y - (maximum*BIGVAL-BIGVAL)
	vmulpd	ymmcarry, ymmcarry, YMM_LIMIT_INVERSE[basereg]	;; shift next carry appropriately
	vmulpd	ymmcarry2, ymmcarry2, YMM_LIMIT_INVERSE[basereg2] ;; shift next carry appropriately
	vsubpd	ymmval, ymmval, ymmtmp				;; rounded value = x - z
	vsubpd	ymmval2, ymmval2, ymmtmp2			;; rounded value = x - z
	ENDM

nobase2_noconst_rounding_interleaved MACRO ymmval, ymmcarry, ymmtmp, basereg, ymmval2, ymmcarry2, ymmtmp2, basereg2
	vmulpd	ymmcarry, ymmval, YMM_LIMIT_INVERSE[basereg]	;; val / base
	vmulpd	ymmcarry2, ymmval2, YMM_LIMIT_INVERSE[basereg2] ;; val / base
	vroundpd ymmval, ymmval, 0				;; Round val
	vroundpd ymmval2, ymmval2, 0				;; Round val
	vroundpd ymmcarry, ymmcarry, 0				;; next carry = round (val / base)
	vroundpd ymmcarry2, ymmcarry2, 0			;; next carry = round (val / base)
	vmulpd	ymmtmp, ymmcarry, YMM_LIMIT_BIGMAX[basereg]	;; z = round (val / base) * base
	vmulpd	ymmtmp2, ymmcarry2, YMM_LIMIT_BIGMAX[basereg2]	;; z = round (val / base) * base
	vsubpd	ymmval, ymmval, ymmtmp				;; new value = val - z
	vsubpd	ymmval2, ymmval2, ymmtmp2			;; new value = val - z
	ENDM

nobase2_const_rounding_interleaved MACRO ymmval, ymmcarry, ymmtmp, basereg, ymmval2, ymmcarry2, ymmtmp2, basereg2
	vmulpd	ymmtmp, ymmval, YMM_LIMIT_INVERSE[basereg]	;; val / base
	vmulpd	ymmtmp2, ymmval2, YMM_LIMIT_INVERSE[basereg2]	;; val / base
	vroundpd ymmtmp, ymmtmp, 0				;; round (val / base)
	vroundpd ymmtmp2, ymmtmp2, 0				;; round (val / base)
	vaddpd	ymmcarry, ymmcarry, ymmtmp			;; next carry = partially computed next carry + round (val / base)
	vaddpd	ymmcarry2, ymmcarry2, ymmtmp2			;; next carry = partially computed next carry + round (val / base)
	vmulpd	ymmtmp, ymmtmp, YMM_LIMIT_BIGMAX[basereg]	;; z = round (val / base) * base
	vmulpd	ymmtmp2, ymmtmp2, YMM_LIMIT_BIGMAX[basereg2]	;; z = round (val / base) * base
	vsubpd	ymmval, ymmval, ymmtmp				;; new value = val - z
	vsubpd	ymmval2, ymmval2, ymmtmp2			;; new value = val - z
	ENDM

;
; These macros round just one value in an YMM register.  This is done
; as part of the cleanup process where the final carry must be added
; back into the results.
;
			;; this is the preferred interface. use of single_rounding without ttp is discouraged
new_single_rounding MACRO ttp, base2, xmmval, xmmcarry, xmmtmp, basereg
base2 ttp	base2_single_rounding xmmval, xmmcarry, xmmtmp, basereg
base2 no ttp	base2_single_rounding xmmval, xmmcarry, xmmtmp, 0
no base2 ttp	nobase2_single_rounding xmmval, xmmcarry, xmmtmp, basereg
no base2 no ttp	nobase2_single_rounding xmmval, xmmcarry, xmmtmp, 0
	ENDM

single_rounding MACRO base2, xmmval, xmmcarry, xmmtmp, basereg
base2		base2_single_rounding xmmval, xmmcarry, xmmtmp, basereg
no base2	nobase2_single_rounding xmmval, xmmcarry, xmmtmp, basereg
	ENDM

base2_single_rounding MACRO xmmval, xmmcarry, xmmtmp, basereg
	vmovsd	xmmtmp, YMM_LIMIT_BIGMAX[basereg]		;; Load maximum * BIGVAL - BIGVAL
	vaddsd	xmmcarry, xmmval, xmmtmp			;; y1 = top bits of x
	vsubsd	xmmtmp, xmmcarry, xmmtmp			;; z1 = y1-(maximum * BIGVAL - BIGVAL)
	vsubsd	xmmval, xmmval, xmmtmp				;; rounded value = x1 - z1
	vmulsd	xmmcarry, xmmcarry, YMM_LIMIT_INVERSE[basereg]	;; next carry = shifted y1
	ENDM

nobase2_single_rounding MACRO xmmval, xmmcarry, xmmtmp, basereg
	vmulsd	xmmcarry, xmmval, YMM_LIMIT_INVERSE[basereg]	;; val / base
	vroundsd xmmval, xmmval, xmmval, 0			;; Round val
	vroundsd xmmcarry, xmmcarry, xmmcarry, 0		;; next carry = round (val / base)
	vmulsd	xmmtmp, xmmcarry, YMM_LIMIT_BIGMAX[basereg]	;; z = round (val / base) * base
	vsubsd	xmmval, xmmval, xmmtmp				;; new value = val - z
	ENDM

			;; this is the preferred interface. use of single_rounding_interleaved without ttp is discouraged
new_single_rounding_interleaved MACRO ttp, base2, xmmval, xmmcarry, xmmtmp, basereg, xmmval2, xmmcarry2, xmmtmp2, basereg2
base2 ttp	base2_single_rounding_interleaved xmmval, xmmcarry, xmmtmp, basereg, xmmval2, xmmcarry2, xmmtmp2, basereg2
base2 no ttp	base2_single_rounding_interleaved xmmval, xmmcarry, xmmtmp, 0, xmmval2, xmmcarry2, xmmtmp2, 0
no base2 ttp	nobase2_single_rounding_interleaved xmmval, xmmcarry, xmmtmp, basereg, xmmval2, xmmcarry2, xmmtmp2, basereg2
no base2 no ttp	nobase2_single_rounding_interleaved xmmval, xmmcarry, xmmtmp, 0, xmmval2, xmmcarry2, xmmtmp2, 0
	ENDM

single_rounding_interleaved MACRO base2, xmmval, xmmcarry, xmmtmp, basereg, xmmval2, xmmcarry2, xmmtmp2, basereg2
base2		base2_single_rounding_interleaved xmmval, xmmcarry, xmmtmp, basereg, xmmval2, xmmcarry2, xmmtmp2, basereg2
no base2	nobase2_single_rounding_interleaved xmmval, xmmcarry, xmmtmp, basereg, xmmval2, xmmcarry2, xmmtmp2, basereg2
	ENDM

base2_single_rounding_interleaved MACRO xmmval, xmmcarry, xmmtmp, basereg, xmmval2, xmmcarry2, xmmtmp2, basereg2
	vmovsd	xmmtmp, YMM_LIMIT_BIGMAX[basereg]		;; Load maximum * BIGVAL - BIGVAL
	vmovsd	xmmtmp2, YMM_LIMIT_BIGMAX[basereg2]		;; Load maximum * BIGVAL - BIGVAL
	vaddsd	xmmcarry, xmmval, xmmtmp			;; y1 = top bits of x
	vaddsd	xmmcarry2, xmmval2, xmmtmp2			;; y1 = top bits of x
	vsubsd	xmmtmp, xmmcarry, xmmtmp			;; z1 = y1-(maximum * BIGVAL - BIGVAL)
	vsubsd	xmmtmp2, xmmcarry2, xmmtmp2			;; z1 = y1-(maximum * BIGVAL - BIGVAL)
	vsubsd	xmmval, xmmval, xmmtmp				;; rounded value = x1 - z1
	vsubsd	xmmval2, xmmval2, xmmtmp2			;; rounded value = x1 - z1
	vmulsd	xmmcarry, xmmcarry, YMM_LIMIT_INVERSE[basereg]	;; next carry = shifted y1
	vmulsd	xmmcarry2, xmmcarry2, YMM_LIMIT_INVERSE[basereg2] ;; next carry = shifted y1
	ENDM

nobase2_single_rounding_interleaved MACRO xmmval, xmmcarry, xmmtmp, basereg, xmmval2, xmmcarry2, xmmtmp2, basereg2
	vmulsd	xmmcarry, xmmval, YMM_LIMIT_INVERSE[basereg]	;; val / base
	vmulsd	xmmcarry2, xmmval2, YMM_LIMIT_INVERSE[basereg2] ;; val / base
	vroundsd xmmval, xmmval, xmmval, 0			;; Round val
	vroundsd xmmval2, xmmval2, xmmval2, 0			;; Round val
	vroundsd xmmcarry, xmmcarry, xmmcarry, 0		;; next carry = round (val / base)
	vroundsd xmmcarry2, xmmcarry2, xmmcarry2, 0		;; next carry = round (val / base)
	vmulsd	xmmtmp, xmmcarry, YMM_LIMIT_BIGMAX[basereg]	;; z = round (val / base) * base
	vmulsd	xmmtmp2, xmmcarry2, YMM_LIMIT_BIGMAX[basereg2]	;; z = round (val / base) * base
	vsubsd	xmmval, xmmval, xmmtmp				;; new value = val - z
	vsubsd	xmmval2, xmmval2, xmmtmp2			;; new value = val - z
	ENDM

;
; These macros process zero-padded FFT result words.  These FFT results must
; be split into high and low parts with the high part used as a carry into
; the splitting the next FFT result word.
;

split_lower_zpad_word MACRO ttp, base2, echk, ymmvalin, ymmcarry, ymmvalout, basereg
base2 ttp	base2_split_lower_zpad_word echk, ymmvalin, ymmcarry, ymmvalout, basereg
base2 no ttp	base2_split_lower_zpad_word echk, ymmvalin, ymmcarry, ymmvalout, 0
no base2 ttp	nobase2_split_lower_zpad_word echk, ymmvalin, ymmcarry, ymmvalout, basereg
no base2 no ttp	nobase2_split_lower_zpad_word echk, ymmvalin, ymmcarry, ymmvalout, 0
	ENDM

base2_split_lower_zpad_word MACRO echk, ymmvalin, ymmcarry, ymmvalout, basereg
	vaddpd	ymmvalin, ymmvalin, ymmcarry			;; Add in previous high FFT data
IFNDEF X86_64
	vmovapd	ymmvalout, YMM_BIGBIGVAL			;; Big word rounding constant
	vaddpd	ymmcarry, ymmvalin, ymmvalout			;; Round to multiple of big word
	vsubpd	ymmcarry, ymmcarry, ymmvalout
ELSE
	vaddpd	ymmcarry, ymmvalin, ymm14			;; Round to multiple of big word
	vsubpd	ymmcarry, ymmcarry, ymm14
ENDIF
	vroundpd ymmvalout, ymmvalin, 0				;; Round to an integer
echk	vsubpd	ymmvalin, ymmvalin, ymmvalout			;; This is the convolution error
echk	absval	ymmvalin					;; Compute absolute value
echk	vmaxpd	ymm6, ymm6, ymmvalin				;; Compute maximum error
	vsubpd	ymmvalout, ymmvalout, ymmcarry			;; ymmvalout now contains low bigword bits
	vmulpd	ymmcarry, ymmcarry, YMM_LIMIT_INVERSE[basereg]	;; Saved shifted FFT hi data
	ENDM

nobase2_split_lower_zpad_word MACRO echk, ymmvalin, ymmcarry, ymmvalout, basereg
	vaddpd	ymmvalin, ymmvalin, ymmcarry			;; Add in previous high FFT data
	vmulpd	ymmcarry, ymmvalin,YMM_LIMIT_INVERSE[basereg]	;; Compute FFTvalue / base
	vroundpd ymmvalout, ymmvalin, 0				;; Round input to an integer
	vroundpd ymmcarry, ymmcarry, 0				;; Round FFTvalue / base to integer
echk	vsubpd	ymmvalin, ymmvalin, ymmvalout			;; This is the convolution error
echk	absval	ymmvalin					;; Compute absolute value
echk	vmaxpd	ymm6, ymm6, ymmvalin				;; Compute maximum error
	vmulpd	ymmvalin, ymmcarry, YMM_LIMIT_BIGMAX[basereg]
	vsubpd	ymmvalout, ymmvalout, ymmvalin			;; ymmvalout now contains FFTvalue % base
	ENDM

; Split upper is like split lower except that previous carry is not added in and
; result carry is not shifted down.

split_upper_zpad_word MACRO ttp, base2, echk, ymmvalin, ymmcarry, ymmvalout, basereg
base2 ttp	base2_split_upper_zpad_word echk, ymmvalin, ymmcarry, ymmvalout, basereg
base2 no ttp	base2_split_upper_zpad_word echk, ymmvalin, ymmcarry, ymmvalout, 0
no base2 ttp	nobase2_split_upper_zpad_word echk, ymmvalin, ymmcarry, ymmvalout, basereg
no base2 no ttp	nobase2_split_upper_zpad_word echk, ymmvalin, ymmcarry, ymmvalout, 0
	ENDM

base2_split_upper_zpad_word MACRO echk, ymmvalin, ymmcarry, ymmvalout, basereg
IFNDEF X86_64
	vmovapd	ymmvalout, YMM_BIGBIGVAL		;; Big word rounding constant
	vaddpd	ymmcarry, ymmvalin, ymmvalout		;; Round to multiple of big word
	vsubpd	ymmcarry, ymmcarry, ymmvalout
ELSE
	vaddpd	ymmcarry, ymmvalin, ymm14		;; Round to multiple of big word
	vsubpd	ymmcarry, ymmcarry, ymm14
ENDIF
	vroundpd ymmvalout, ymmvalin, 0			;; Round input to an integer
echk	vsubpd	ymmvalin, ymmvalin, ymmvalout		;; This is the convolution error
echk	absval	ymmvalin				;; Compute absolute value
echk	vmaxpd	ymm6, ymm6, ymmvalin			;; Compute maximum error
	vsubpd	ymmvalout, ymmvalout, ymmcarry		;; ymmvalout now contains low bigword bits
	ENDM

nobase2_split_upper_zpad_word MACRO echk, ymmvalin, ymmcarry, ymmvalout, basereg
	vmulpd	ymmcarry, ymmvalin, YMM_LIMIT_INVERSE[basereg]	;; Compute FFTvalue / base
	vroundpd ymmvalout, ymmvalin, 0				;; Round input to an integer
	vroundpd ymmcarry, ymmcarry, 0				;; Round FFTvalue / base to integer
echk	vsubpd	ymmvalin, ymmvalin, ymmvalout			;; This is the convolution error
echk	absval	ymmvalin					;; Compute absolute value
echk	vmaxpd	ymm6, ymm6, ymmvalin				;; Compute maximum error
	vmulpd	ymmvalin, ymmcarry, YMM_LIMIT_BIGMAX[basereg]
	vsubpd	ymmvalout, ymmvalout, ymmvalin			;; ymmvalout now contains FFTvalue % base
	ENDM

; The single word version

new_single_split_lower_zpad_word MACRO ttp, base2, xmmval, xmmcarry, xmmtmp, basereg
base2 ttp	base2_single_split_lower_zpad_word xmmval, xmmcarry, xmmtmp, basereg
base2 no ttp	base2_single_split_lower_zpad_word xmmval, xmmcarry, xmmtmp, 0
no base2 ttp	nobase2_single_split_lower_zpad_word xmmval, xmmcarry, xmmtmp, basereg
no base2 no ttp	nobase2_single_split_lower_zpad_word xmmval, xmmcarry, xmmtmp, 0
	ENDM

single_split_lower_zpad_word MACRO base2, xmmval, xmmcarry, xmmtmp, basereg
base2		base2_single_split_lower_zpad_word xmmval, xmmcarry, xmmtmp, basereg
no base2	nobase2_single_split_lower_zpad_word xmmval, xmmcarry, xmmtmp, basereg
	ENDM

base2_single_split_lower_zpad_word MACRO xmmval, xmmcarry, xmmtmp, basereg
	vaddsd	xmmval, xmmval, xmmcarry			;; Add in previous high FFT data
	vmovsd	xmmtmp, YMM_BIGBIGVAL				;; Big word rounding constant
	vaddsd	xmmcarry, xmmval, xmmtmp			;; Round to multiple of big word
	vsubsd	xmmcarry, xmmcarry, xmmtmp
	vroundsd xmmval, xmmval, xmmval, 0			;; Round to an integer
	vsubsd	xmmval, xmmval, xmmcarry			;; xmmval now contains low bigword bits
	vmulsd	xmmcarry, xmmcarry, YMM_LIMIT_INVERSE[basereg]	;; Next carry = shifted FFT hi data
	ENDM

nobase2_single_split_lower_zpad_word MACRO xmmval, xmmcarry, xmmtmp, basereg
	vaddsd	xmmval, xmmval, xmmcarry			;; Add in previous high FFT data
	vmulsd	xmmcarry, xmmval, YMM_LIMIT_INVERSE[basereg]	;; Compute FFTvalue / base
	vroundsd xmmval, xmmval, xmmval, 0			;; Round input to an integer
	vroundsd xmmcarry, xmmcarry, xmmcarry, 0		;; Next carry = round ( FFTvalue / base )
	vmulsd	xmmtmp, xmmcarry, YMM_LIMIT_BIGMAX[basereg]
	vsubsd	xmmval, xmmval, xmmtmp				;; xmmval now contains FFTvalue % base
	ENDM

; The version for splitting the high FFT carry.  The high carry input has already
; been rounded to an integer.

split_upper_carry_zpad_word MACRO ttp, base2, ymmcarryin, ymmcarryout, ymmtmp, basereg
base2 ttp	base2_split_upper_carry_zpad_word ymmcarryin, ymmcarryout, ymmtmp, basereg
base2 no ttp	base2_split_upper_carry_zpad_word ymmcarryin, ymmcarryout, ymmtmp, 0
no base2 ttp	nobase2_split_upper_carry_zpad_word ymmcarryin, ymmcarryout, ymmtmp, basereg
no base2 no ttp	nobase2_split_upper_carry_zpad_word ymmcarryin, ymmcarryout, ymmtmp, 0
	ENDM

base2_split_upper_carry_zpad_word MACRO ymmcarryin, ymmcarryout, ymmtmp, basereg
	vmovapd	ymmtmp, YMM_BIGBIGVAL			;; Big word rounding constant
	vaddpd	ymmcarryout, ymmcarryin, ymmtmp		;; Round to multiple of big word
	vsubpd	ymmcarryout, ymmcarryout, ymmtmp
	vsubpd	ymmcarryin, ymmcarryin, ymmcarryout	;; ymmcarryin now contains low bigword bits
	vmulpd	ymmcarryout, ymmcarryout, YMM_LIMIT_INVERSE[basereg] ;; Next carry = shifted carry
	ENDM

nobase2_split_upper_carry_zpad_word MACRO ymmcarryin, ymmcarryout, ymmtmp, basereg
	vmulpd	ymmcarryout, ymmcarryin, YMM_LIMIT_INVERSE[basereg]	;; Compute carry / base
	vroundpd ymmcarryout, ymmcarryout, 0				;; Next carry = round(carry / base)
	vmulpd	ymmtmp, ymmcarryout, YMM_LIMIT_BIGMAX[basereg]
	vsubpd	ymmcarryin, ymmcarryin, ymmtmp				;; ymmcarryin now contains carry % base
	ENDM

; Round the ZPAD0 - ZPAD6 values.  Simpler than other rounding macros
; in that we always round to a big word (and input value and output
; carry do not have YMM_BIGVAL added in).

new_round_zpad7_word MACRO ttp, base2, xmmvalin, xmmcarry, xmmvalout, basereg
base2 ttp	base2_round_zpad7_word xmmvalin, xmmcarry, xmmvalout, basereg
base2 no ttp	base2_round_zpad7_word xmmvalin, xmmcarry, xmmvalout, 0
no base2 ttp	nobase2_round_zpad7_word xmmvalin, xmmcarry, xmmvalout, basereg
no base2 no ttp	nobase2_round_zpad7_word xmmvalin, xmmcarry, xmmvalout, 0
	ENDM

round_zpad7_word MACRO base2, xmmvalin, xmmcarry, xmmvalout, basereg
base2		base2_round_zpad7_word xmmvalin, xmmcarry, xmmvalout, basereg
no base2	nobase2_round_zpad7_word xmmvalin, xmmcarry, xmmvalout, basereg
	ENDM

base2_round_zpad7_word MACRO xmmval, xmmcarry, xmmtmp, basereg
	vaddsd	xmmval, xmmval, xmmcarry		;; Add in high part of last calculation
	vmovsd	xmmtmp, YMM_BIGBIGVAL			;; Big word rounding constant
	vaddsd	xmmcarry, xmmval, xmmtmp		;; Round to multiple of big word
	vsubsd	xmmcarry, xmmcarry, xmmtmp
	vsubsd	xmmval, xmmval, xmmcarry		;; xmmval now contains low bigword bits
	vmulsd	xmmcarry, xmmcarry, YMM_LIMIT_INVERSE[basereg] ;; Shift high ZPAD data
	ENDM

nobase2_round_zpad7_word MACRO xmmval, xmmcarry, xmmtmp, basereg
	vaddsd	xmmval, xmmval, xmmcarry			;; Add in high part of last calculation
	vmulsd	xmmcarry, xmmval, YMM_LIMIT_INVERSE[basereg]	;; val / base
	vroundsd xmmcarry, xmmcarry, xmmcarry, 0		;; next carry = round (val / base)
	vmulsd	xmmtmp, xmmcarry, YMM_LIMIT_BIGMAX[basereg]	;; z = round (val / base) * base
	vsubsd	xmmval, xmmval, xmmtmp				;; new value = val - z
	ENDM


;; Rotate the YMM registers right by one double.
;; For example in section 1 of a length 32 FFT:
;; On input ymmreg1 is MSW (1 2 3 4) LSW
;; On output ymmreg1 is (x 1 2 3) and ymmreg2 is (4 x x x)
;; where x is either BIGVAL or zero.

rotate_carries MACRO base2, ymmreg1, ymmreg2, ymmtmp, ymmtmp2
base2	vmovapd	ymmreg2, YMM_BIGVAL
no base2 vxorpd	ymmreg2, ymmreg2, ymmreg2
	vperm2f128 ymmtmp, ymmreg1, ymmreg2, 03h	;; create (x x 1 2)
	vperm2f128 ymmtmp2, ymmreg1, ymmreg2, 21h	;; create (3 4 x x)
	vshufpd ymmreg1, ymmtmp, ymmreg1, 0100b		;; create (x 1 2 3)
	vshufpd	ymmreg2, ymmtmp2, ymmreg2, 0001b	;; create (4 x x x)
	ENDM

rotate_carries_interleaved MACRO base2, ymmreg1, ymmreg2, ymmreg3, ymmreg4, ymmtmp, ymmtmp2
base2	vmovapd	ymmreg4, YMM_BIGVAL
no base2 vxorpd	ymmreg4, ymmreg4, ymmreg4
	vperm2f128 ymmtmp, ymmreg1, ymmreg4, 03h	;; create (x x 1 2)
	vperm2f128 ymmreg2, ymmreg1, ymmreg4, 21h	;; create (3 4 x x)
	vperm2f128 ymmtmp2, ymmreg3, ymmreg4, 21h	;; create (3 4 x x)
	vshufpd ymmreg1, ymmtmp, ymmreg1, 0100b		;; create (x 1 2 3)
	vperm2f128 ymmtmp, ymmreg3, ymmreg4, 03h	;; create (x x 1 2)
	vshufpd	ymmreg2, ymmreg2, ymmreg4, 0001b	;; create (4 x x x)
	vshufpd	ymmreg4, ymmtmp2, ymmreg4, 0001b	;; create (4 x x x)
	vshufpd ymmreg3, ymmtmp, ymmreg3, 0100b		;; create (x 1 2 3)
	ENDM

;; Alternative rotate of YMM registers right by one double.
;; Used in two-pass FFT carry propagation
;; ymmreg1 is MSW (4 3 2 1) LSW
;; ymmreg2 is MSW (x x x 5) LSW
;; output is:
;; ymmreg1 is (3 2 1 5)
;; ymmreg2 is (x x x 4)
rotate_five_carries MACRO ymmreg1, ymmreg2, ymmtmp
	vperm2f128 ymmtmp, ymmreg1, ymmreg2, 02h	;; create (2 1 x 5)
	vperm2f128 ymmreg2, ymmreg1, ymmreg2, 31h	;; create (x x 4 3)
	vshufpd ymmreg1, ymmtmp, ymmreg1, 0100b		;; create (3 2 1 5)
	vshufpd	ymmreg2, ymmreg2, ymmreg2, 0001b	;; create (x x x 4)
	ENDM
;
; Now for the actual normalization macros!
;

; For 1D macros, these registers are set on input:
; ymm7 = sumout
; ymm6 = MAXERR
; rsi = pointer to the FFT data values
; rbp = pointer two-to-phi multipliers
; rdi = pointer to array of big vs. little flags
; ecx = big vs. little word flag #2
; eax = big vs. little word flag #1
; ymm2 = carry #1
; ymm3 = carry #2


; *************** 1D macro ******************
; A pipelined version of this code:
;	movzx	rax, BYTE PTR [rdi]	;; Load big vs. little flags
;	vmovapd	ymm0, [rsi+0*dist1]	;; Load values
;	vmulpd	ymm4, ymm0, [rbp+0]	;; Mul values1 by two-to-minus-phi
;	vaddpd	sumout, sumout, ymm0	;; sumout += values
;	vaddpd	ymm4, ymm4, ymm2	;; x = values + carry
;	vmovpad	ymm0, YMM_LIMIT_BIGMAX[rax*2];; Load maximum * BIGVAL - BIGVAL
;	vaddpd	ymm2, ymm4, ymm0	;; y = top bits of x
;	vaddpd	ymm0, ymm2, ymm0	;; z = y - (maximum * BIGVAL - BIGVAL)
;	vsubpd	ymm4, ymm4, ymm0	;; rounded value = x - z
;	vmulpd	ymm2, ymm2, YMM_LIMIT_INVERSE[rax*2];; next carry = shifted y
;	vmulpd	ymm4, ymm4, [rbp+32]	;; new value = val * two-to-phi
;	vmovapd	[rsi+0*dist1], ymm4	;; Save new value

ynorm_1d_preload MACRO ttp, base2, zero, echk, const
	ENDM

IFDEF X86_64
ynorm_1d_preload MACRO ttp, base2, zero, echk, const
echk		vmovapd	ymm15, YMM_ABSVAL
base2 const	vmovapd	ymm14, YMM_BIGBIGVAL
const		vmovapd	ymm13, YMM_MULCONST
	ENDM
ENDIF

ynorm_1d MACRO ttp, base2, zero, echk, const
ttp		movzx	rax, BYTE PTR [rdi]	;; Load big vs. little flags
ttp		movzx	rcx, BYTE PTR [rdi+1]
		vmovapd	ymm0, [rsi]		;; Load values1
		vmulpd	ymm4, ymm0, [rbp]	;; Mul values1 by two-to-minus-phi
		vmovapd	ymm1, [rsi+32]		;; Load values2
		vmulpd	ymm5, ymm1, [rbp+64]	;; Mul values2 by two-to-minus-phi
		vaddpd	ymm7, ymm7, ymm0	;; sumout += values1
		vaddpd	ymm7, ymm7, ymm1	;; sumout += values2
const		mul_by_const_interleaved ttp, base2, echk, ymm4, ymm2, ymm0, rax*2, ymm5, ymm3, ymm1, rcx*2, ymm6
no const echk	error_check_interleaved ymm4, ymm0, ymm5, ymm1, ymm6
no const	vaddpd	ymm4, ymm4, ymm2	;; x1 = values + carry
no const	vaddpd	ymm5, ymm5, ymm3	;; x2 = values + carry
		rounding_interleaved ttp, base2, const, ymm4, ymm2, ymm0, rax*2, ymm5, ymm3, ymm1, rcx*2
ttp		vmulpd	ymm4, ymm4, [rbp+32]	;; new value1 = val * two-to-phi
ttp no zero	vmulpd	ymm5, ymm5, [rbp+96]	;; new value2 = val * two-to-phi
zero		vxorpd	ymm5, ymm5, ymm5
		vmovapd	[rsi], ymm4		;; Save value1
		vmovapd	[rsi+32], ymm5		;; Save value2
	ENDM


; This is the normalization routine when we are computing modulo k*b^n+c
; with a zero-padded b^2n FFT.  We do this by multiplying the lower FFT
; word by k and adding in the upper word times -c.  Of course, this is made
; very tedious because we have to carefully avoid any loss of precision.
;
; ymm7 = sumout
; ymm6 = MAXERR
; ymm3 = carry #2 (previous high FFT data - not yet mul'ed by K)
; ymm2 = carry #1 (traditional carry)
; rsi = pointer to the FFT data values
; rbp = pointer two-to-phi multipliers
; rdi = pointer to array of big vs. little flags
; eax = big vs. little word flag #1

ynorm_1d_zpad_preload MACRO ttp, base2, echk, const, khi, c1, cm1
	ENDM

ynorm_1d_zpad MACRO ttp, base2, echk, const, khi, c1, cm1
ttp		movzx	rax, BYTE PTR [rdi]	;; Load big vs. little flags
		vmovapd	ymm0, [rsi]		;; Load values1
		vmovapd	ymm1, [rsi+32]		;; Load values2
		vaddpd	ymm7, ymm7, ymm0	;; sumout += values1
		vmulpd	ymm0, ymm0, [rbp]	;; Mul values1 by two-to-minus-phi
		vaddpd	ymm7, ymm7, ymm1	;; sumout += values2
		vmulpd	ymm1, ymm1, [rbp]	;; Mul values2 by two-to-minus-phi

		split_lower_zpad_word ttp, base2, echk, ymm0, ymm3, ymm4, rax*2

no const	vmulpd	ymm0, ymm4, YMM_K_LO
const		vmulpd	ymm0, ymm4, YMM_K_TIMES_MULCONST_LO

khi base2 no const	vmulpd	ymm5, ymm4, YMM_K_HI
khi base2 const		vmulpd	ymm5, ymm4, YMM_K_TIMES_MULCONST_HI
khi no base2 no const	vmovapd	ymm5, YMM_K_HI
khi no base2 const	vmovapd	ymm5, YMM_K_TIMES_MULCONST_HI
khi no base2 ttp	vmulpd	ymm5, ymm5, YMM_LIMIT_INVERSE[rax*2] ;; Non-base2 rounding needs shifted carry
khi no base2 no ttp	vmulpd	ymm5, ymm5, YMM_LIMIT_INVERSE[0] ;; Non-base2 rounding needs shifted carry
khi no base2		vroundpd ymm5, ymm5, 0			;; THIS IS WASTEFUL.  The mul and round should be precomputed!
khi no base2		vmulpd	ymm5, ymm5, ymm4

		vaddpd	ymm0, ymm0, ymm2	;; x1 = values + carry

c1		vmulpd	ymm1, ymm1, YMM_MINUS_C	;; Do one mul before split rather than two after split

		split_upper_zpad_word ttp, base2, echk, ymm1, ymm2, ymm4, rax*2

no const no c1 no cm1	vmulpd	ymm4, ymm4, YMM_MINUS_C
no const no c1 no cm1	vmulpd	ymm2, ymm2, YMM_MINUS_C
const			vmulpd	ymm4, ymm4, YMM_MINUS_C_TIMES_MULCONST
const			vmulpd	ymm2, ymm2, YMM_MINUS_C_TIMES_MULCONST

		vaddpd	ymm0, ymm0, ymm4	;; Add upper FFT word to lower FFT word
khi		vaddpd	ymm2, ymm2, ymm5	;; Add upper FFT word to lower FFT word

		rounding ttp, base2, exec, ymm0, ymm2, ymm4, rax*2

ttp		vmulpd	ymm0, ymm0, [rbp+32]	;; new value1 = val * two-to-phi
		vmovapd	[rsi], ymm0		;; Save value1
		vxorpd	ymm1, ymm1, ymm1	;; new value2 = zero
		vmovapd	[rsi+32], ymm1		;; Zero value2
	ENDM

;; 64-bit version

IFDEF X86_64

ynorm_1d_zpad_preload MACRO ttp, base2, echk, const, khi, c1, cm1
echk			vmovapd	ymm15, YMM_ABSVAL
base2			vmovapd ymm14, YMM_BIGBIGVAL
no const		vmovapd	ymm13, YMM_K_LO
const			vmovapd	ymm13, YMM_K_TIMES_MULCONST_LO
khi no const		vmovapd	ymm12, YMM_K_HI
khi const		vmovapd	ymm12, YMM_K_TIMES_MULCONST_HI
no const no c1 no cm1	vmovapd	ymm11, YMM_MINUS_C
const			vmovapd	ymm11, YMM_MINUS_C_TIMES_MULCONST
	ENDM

ynorm_1d_zpad MACRO ttp, base2, echk, const, khi, c1, cm1
ttp		movzx	rax, BYTE PTR [rdi]	;; Load big vs. little flags
		vmovapd	ymm0, [rsi]		;; Load values1
		vmovapd	ymm1, [rsi+32]		;; Load values2
		vaddpd	ymm7, ymm7, ymm0	;; sumout += values1
		vmulpd	ymm0, ymm0, [rbp]	;; Mul values1 by two-to-minus-phi
		vaddpd	ymm7, ymm7, ymm1	;; sumout += values2
		vmulpd	ymm1, ymm1, [rbp]	;; Mul values2 by two-to-minus-phi

		split_lower_zpad_word ttp, base2, echk, ymm0, ymm3, ymm4, rax*2

		vmulpd	ymm0, ymm4, ymm13	;; Mul by YMM_K_LO or YMM_K_TIMES_MULCONST_LO

khi base2		vmulpd	ymm5, ymm4, ymm12		;; Mul by YMM_K_HI or YMM_K_TIMES_MULCONST_HI
khi no base2 ttp	vmulpd	ymm5, ymm12, YMM_LIMIT_INVERSE[rax*2] ;; Non-base2 rounding needs shifted carry
khi no base2 no ttp	vmulpd	ymm5, ymm12, YMM_LIMIT_INVERSE[0] ;; Non-base2 rounding needs shifted carry
khi no base2		vroundpd ymm5, ymm5, 0			;; THIS IS WASTEFUL.  The mul and round should be precomputed!
khi no base2		vmulpd	ymm5, ymm5, ymm4

		vaddpd	ymm0, ymm0, ymm2	;; x1 = values + carry

c1		vmulpd	ymm1, ymm1, YMM_MINUS_C	;; Do one mul before split rather than two after split

		split_upper_zpad_word ttp, base2, echk, ymm1, ymm2, ymm4, rax*2

no const no c1 no cm1	vmulpd	ymm4, ymm4, ymm11	;; Mul by YMM_MINUS_C
no const no c1 no cm1	vmulpd	ymm2, ymm2, ymm11
const			vmulpd	ymm4, ymm4, ymm11	;; Mul by YMM_MINUS_C_TIMES_MULCONST
const			vmulpd	ymm2, ymm2, ymm11

		vaddpd	ymm0, ymm0, ymm4	;; Add upper FFT word to lower FFT word
khi		vaddpd	ymm2, ymm2, ymm5	;; Add upper FFT word to lower FFT word

		rounding ttp, base2, exec, ymm0, ymm2, ymm4, rax*2

ttp		vmulpd	ymm0, ymm0, [rbp+32]	;; new value1 = val * two-to-phi
		vmovapd	[rsi], ymm0		;; Save value1
		vxorpd	ymm1, ymm1, ymm1	;; new value2 = zero
		vmovapd	[rsi+32], ymm1		;; Zero value2
	ENDM
ENDIF


; *************** 1D followup macros ******************
; This macro finishes the normalize process by adding the section
; carries back into the start of the section.  Three of the YMM section
; carries are added back in, one of the YMM section carries is applied
; to the next section.
; ymm2,ymm3 = carries
; rax, rdx, rsi, rbp, rdi = trash

ynorm_1d_mid_cleanup MACRO ttp, base2, zero, srcptr, biglitptr, ttpptr
	LOCAL	section_start, section_loop, force_done, done

	rotate_carries_interleaved base2, ymm2, ymm4, ymm3, ymm5, ymm0, ymm1

	mov	DWORD PTR YMM_TMP3, 15		;; Propagate carry at most 15 times (should almost never happen)
section_start:
	mov	rsi, srcptr			;; Load section pointers
ttp	mov	rdi, biglitptr
ttp	mov	rbp, ttpptr
	mov	edx, count1			;; Count of cache lines in section or padded group
	vmovapd	YMM_TMP1, ymm4			;; Save carry for next section
	vmovapd	YMM_TMP2, ymm5			;; Save carry for next section

section_loop:
base2	vmovapd	ymm1, YMM_BIGVAL		;; Load comparison value
no base2 vxorpd	ymm1, ymm1, ymm1		;; Create comparison value
	vcmppd	ymm0, ymm2, ymm1, 0Ch		;; Are any carries non-zero
	vmovmskpd rax, ymm0			;; Extract 4 comparison bits
	vcmppd	ymm0, ymm3, ymm1, 0Ch		;; Are any carries non-zero
	vmovmskpd rcx, ymm0			;; Extract 4 comparison bits
	or	rax, rcx			;; Are any bits on?
	jz	done				;; No, we're all done

	sub	DWORD PTR YMM_TMP3, 1		;; There is a bizarre case where adding a zero carry causes a carry (example:
						;; 10^114+1, a data value of 500 becomes -500 with a carry of 1 -- add zero again and
						;; it becomes -500 with a carry of -1).  This can lead to an infinite loop because
						;; the 4 carries never become zero.  After many attempts at getting 4 zero carries,
						;; give up and just add in the carries without propagation.
	jz	force_done

ttp		movzx	rax, BYTE PTR [rdi]		;; Load big vs. little flags
ttp		movzx	rcx, BYTE PTR [rdi+1]
		vmovapd	ymm0, [rsi]			;; Load values1
ttp		vmulpd	ymm0, ymm0, [rbp]		;; Mul values1 by two-to-minus-phi
ttp		vmulpd	ymm0, ymm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
		vmovapd	ymm1, [rsi+32]			;; Load values2
ttp		vmulpd	ymm1, ymm1, [rbp+64]		;; Mul values2 by two-to-minus-phi
ttp		vmulpd	ymm1, ymm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
		vaddpd	ymm0, ymm0, ymm2		;; x1 = values + carry
		vaddpd	ymm1, ymm1, ymm3		;; x2 = values + carry
		rounding_interleaved ttp, base2, noexec, ymm0, ymm2, ymm4, rax*2, ymm1, ymm3, ymm5, rcx*2
ttp		vmulpd	ymm0, ymm0, [rbp+32]		;; new value1 = val * two-to-phi
ttp no zero	vmulpd	ymm1, ymm1, [rbp+96]		;; new value2 = val * two-to-phi
		vmovapd	[rsi], ymm0			;; Save new value1
no zero		vmovapd	[rsi+32], ymm1			;; Save new value2

ttp	bump	rdi, 2				;; Advance pointers
	bump	rsi, 64
ttp	bump	rbp, 128
	sub	rdx, 1				;; Test counter
	jnz	section_loop			;; More cache lines in section, add carry in

	;; Section ended.  Rotate carries again and add the new next section carry values
	;; into the previously calculated next section carry values

	rotate_carries_interleaved base2, ymm2, ymm4, ymm3, ymm5, ymm0, ymm1
base2	vsubpd	ymm4, ymm4, YMM_BIGVAL
base2	vsubpd	ymm5, ymm5, YMM_BIGVAL
	vaddpd	ymm4, ymm4, YMM_TMP1
	vaddpd	ymm5, ymm5, YMM_TMP2
	jmp	section_start

force_done:	
base2		vsubpd	ymm2, ymm2, YMM_BIGVAL		;; Subtract rounding constant from carry
base2		vsubpd	ymm3, ymm3, YMM_BIGVAL
ttp		vmulpd	ymm2, ymm2, [rbp+32]		;; carry1 *= two-to-phi
ttp no zero	vmulpd	ymm3, ymm3, [rbp+96]		;; carry2 *= two-to-phi
		vaddpd	ymm2, ymm2, [rsi]		;; Add in values1
no zero		vaddpd	ymm3, ymm3, [rsi+32]		;; Add in values2
		vmovapd	[rsi], ymm2			;; Save new value1
no zero		vmovapd	[rsi+32], ymm3			;; Save new value2

done:	vmovapd	ymm2, YMM_TMP1			;; Restore carry for next section
	vmovapd	ymm3, YMM_TMP2			;; Restore carry for next section
	ENDM

; This macro is similar to the above, but is for the zero padding case.
; ymm2 = carry #1 (traditional carry)
; ymm3 = carry #2 (previous high FFT data - not yet mul'ed by K)
; ymm2,ymm3 = carries
; rax, rdx, rsi, rbp, rdi = trash

ynorm_1d_zpad_mid_cleanup MACRO ttp, base2, const, srcptr, biglitptr, ttpptr
	LOCAL	section_start, section_loop, force_done, done

	rotate_carries base2, ymm2, ymm4, ymm0, ymm1
	rotate_carries noexec, ymm3, ymm5, ymm0, ymm1

	mov	DWORD PTR YMM_TMP3, 15		;; Propagate carry at most 15 times (should almost never happen)
section_start:
	mov	rsi, srcptr			;; Load section pointers
ttp	mov	rdi, biglitptr
ttp	mov	rbp, ttpptr
	mov	edx, count1			;; Count of cache lines in section or padded group
	vmovapd	YMM_TMP1, ymm4			;; Save carry for next section
	vmovapd	YMM_TMP2, ymm5			;; Save carry for next section

section_loop:
base2	vmovapd	ymm1, YMM_BIGVAL		;; Load comparison value
no base2 vxorpd	ymm1, ymm1, ymm1		;; Create comparison value
	vcmppd	ymm0, ymm2, ymm1, 0Ch		;; Are any carries non-zero
	vmovmskpd rax, ymm0			;; Extract 4 comparison bits
base2	vxorpd	ymm1, ymm1, ymm1		;; High carry words are always compared to zero
	vcmppd	ymm0, ymm3, ymm1, 0Ch		;; Are any carries non-zero
	vmovmskpd rcx, ymm0			;; Extract 4 comparison bits
	or	rax, rcx			;; Are any bits on?
	jz	done				;; No, we're all done

	sub	DWORD PTR YMM_TMP3, 1		;; There is a bizarre case where adding a zero carry causes a carry (example:
						;; 10^114+1, a data value of 500 becomes -500 with a carry of 1 -- add zero again and
						;; it becomes -500 with a carry of -1).  This can lead to an infinite loop because
						;; the 4 carries never become zero.  After many attempts at getting 4 zero carries,
						;; give up and just add in the carries without propagation.
	jz	force_done

	movzx	rax, BYTE PTR [rdi]		;; Load big vs. little flags
	vmovapd	ymm0, [rsi]			;; Load values1
ttp	vmulpd	ymm0, ymm0, [rbp]		;; Mul values1 by two-to-minus-phi
ttp	vmulpd	ymm0, ymm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddpd	ymm0, ymm0, ymm2		;; x1 = values1 + carry
	split_upper_carry_zpad_word ttp, base2, ymm3, ymm1, ymm2, rax*2
no const vmulpd	ymm2, ymm3, YMM_K_LO		;; low bits of high FFT carry * k_lo
const	vmulpd	ymm2, ymm3, YMM_K_TIMES_MULCONST_LO ;; low bits of high_FFT_carry * k_lo
	vaddpd	ymm0, ymm0, ymm2		;; x1 = x1 + low bits of high_FFT_carry * k_lo
no const vmulpd ymm3, ymm3, YMM_K_HI		;; low bits of high FFT carry * k_hi
const	vmulpd	ymm3, ymm3, YMM_K_TIMES_MULCONST_HI ;; low bits of high FFT carry * k_hi
ttp	vmulpd	ymm3, ymm3, YMM_LIMIT_INVERSE[rax*2] ;; shift low bits of high FFT carry * k_hi
no ttp	vmulpd	ymm3, ymm3, YMM_LIMIT_INVERSE[0] ;; shift low bits of high FFT carry * k_hi
	vroundpd ymm3, ymm3, 0			;; WASTEFUL.  Round (k_hi * limit_inverse) should be precomputed
	rounding ttp, base2, noexec, ymm0, ymm2, ymm4, rax*2
	vaddpd	ymm2, ymm2, ymm3		;; Carry += shifted low bits of high_FFT_carry * k_hi
ttp	vmulpd	ymm0, ymm0, [rbp+32]		;; new value1 = val * two-to-phi
	vmovapd	[rsi], ymm0			;; Save new value1
	vmovapd	ymm3, ymm1			;; Next high FFT carry = high bits of current high FFT carry

ttp	bump	rdi, 1				;; Advance pointers
	bump	rsi, 64
ttp	bump	rbp, 64
	sub	rdx, 1				;; Test counter
	jnz	section_loop			;; More cache lines in section, add carry in

	;; Section ended.  Rotate carries again and add the new next section carry values
	;; into the previously calculated next section carry values

	rotate_carries base2, ymm2, ymm4, ymm0, ymm1
	rotate_carries noexec, ymm3, ymm5, ymm0, ymm1
base2	vsubpd	ymm4, ymm4, YMM_BIGVAL
	vaddpd	ymm4, ymm4, YMM_TMP1
	vaddpd	ymm5, ymm5, YMM_TMP2
	jmp	section_start

force_done:
base2	vsubpd	ymm2, ymm2, YMM_BIGVAL		;; Subtract rounding constant from carry
ttp	vmulpd	ymm2, ymm2, [rbp+32]		;; carry1 *= two-to-phi
	vaddpd	ymm2, ymm2, [rsi]		;; Add in values1
	vmovapd	[rsi], ymm2			;; Save new value1

done:	vmovapd	ymm2, YMM_TMP1			;; Restore carry for next section
	vmovapd	ymm3, YMM_TMP2			;; Restore carry for next section
	ENDM


; We could take advantage of the fact that the first two-to-phi multiplier
; and the first two-to-minus-phi multiplier are one.  We also know
; the first data value is a big word
; ymm2,ymm3 = carries
; rsi = pointer to the FFT data values
; rbp = pointer two-to-phi multipliers
; rdi = big vs. little array pointer
; Input arguments are destroyed!

ynorm_1d_cleanup MACRO ttp, base2, zero
	LOCAL	section_begin, section_restart, section_loop, do_more, new_section, force_done, done

ttp	mov	rdi, norm_biglit_array		; Address of the big/little flags array
ttp	mov	rbp, norm_col_mults		; Restart the column multipliers

base2	vsubpd	ymm3, ymm3, YMM_BIGVAL
	vmulpd	ymm3, ymm3, YMM_MINUS_C		;; Adjust wrap-around carry
base2	vaddpd	ymm3, ymm3, YMM_BIGVAL

	mov	DWORD PTR YMM_TMP6, 12		;; Propagate carry at most 12 times (should almost never happen)
section_begin:
	mov	rbx, 4				;; Loop through section at most 4 times
	mov	PPTR YMM_TMP1, rsi		;; Save section pointers
ttp	mov	PPTR YMM_TMP2, rdi
ttp	mov	PPTR YMM_TMP3, rbp

base2	vmovapd	ymm5, YMM_BIGVAL		;; Clear carries for next section
no base2 vxorpd	ymm5, ymm5, ymm5
	vmovapd	ymm4, ymm5

section_restart:
	vmovapd	YMM_TMP5, ymm5			;; Save next section's initial carries
	vmovapd	YMM_TMP4, ymm4
	mov	edx, count1			;; Count of cache lines in section or padded group

section_loop:
	sub	DWORD PTR YMM_TMP6, 1		;; There is a bizarre case where adding a zero carry causes a carry (example:
						;; 10^114+1, a data value of 500 becomes -500 with a carry of 1 -- add zero again and
						;; it becomes -500 with a carry of -1).  This can lead to an infinite loop because
						;; the 4 carries never become zero.  After many attempts at getting 4 zero carries,
						;; give up and just add in the carries without propagation.
	jle	force_done

base2		vmovapd	ymm1, YMM_BIGVAL	;; Load comparison value
no base2	vxorpd	ymm1, ymm1, ymm1	;; Create comparison value
		vcmppd	ymm0, ymm3, ymm1, 0Ch	;; Are any carries non-zero
		vmovmskpd rax, ymm0		;; Extract 4 comparison bits
no zero		vcmppd	ymm0, ymm2, ymm1, 0Ch	;; Are any carries non-zero
no zero		vmovmskpd rcx, ymm0		;; Extract 4 comparison bits
no zero		or	rax, rcx		;; Are any bits on?
zero		test	rax, rax		;; Are any bits on?
		jz	done			;; No, we're all done

ttp		movzx	rax, BYTE PTR [rdi]		;; Load big vs. little flags
no zero ttp	movzx	rcx, BYTE PTR [rdi+1]
		vmovapd	ymm0, [rsi]			;; Load values1
ttp		vmulpd	ymm0, ymm0, [rbp]		;; Mul values1 by two-to-minus-phi
ttp		vmulpd	ymm0, ymm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
no zero		vmovapd	ymm1, [rsi+32]			;; Load values2
no zero ttp	vmulpd	ymm1, ymm1, [rbp+64]		;; Mul values2 by two-to-minus-phi
no zero ttp	vmulpd	ymm1, ymm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
		vaddpd	ymm0, ymm0, ymm3		;; x1 = values + carry
no zero		vaddpd	ymm1, ymm1, ymm2		;; x2 = values + carry
no zero		rounding_interleaved ttp, base2, noexec, ymm0, ymm3, ymm4, rax*2, ymm1, ymm2, ymm5, rcx*2
zero		rounding ttp, base2, noexec, ymm0, ymm3, ymm4, rax*2
ttp		vmulpd	ymm0, ymm0, [rbp+32]		;; new value1 = val * two-to-phi
no zero ttp	vmulpd	ymm1, ymm1, [rbp+96]		;; new value2 = val * two-to-phi
		vmovapd	[rsi], ymm0			;; Save new value1
no zero		vmovapd	[rsi+32], ymm1			;; Save new value2

do_more:
ttp	bump	rdi, 2				;; Advance pointers
	bump	rsi, 64
ttp	bump	rbp, 128
	sub	rdx, 1				;; Test cache line counter
	jnz	section_loop			;; More cache lines in section, go add carry in

	;; Section ended.  Rotate carries and stay in the current section or
	;; move to the next section.  I think only a length 32 FFT will ever
	;; move onto the next section.

	rotate_carries_interleaved base2, ymm3, ymm5, ymm2, ymm4, ymm0, ymm1

base2	vsubpd	ymm5, ymm5, YMM_BIGVAL		;; Add carry for next section
base2	vsubpd	ymm4, ymm4, YMM_BIGVAL
	vaddpd	ymm5, ymm5, YMM_TMP5
	vaddpd	ymm4, ymm4, YMM_TMP4

	sub	rbx, 1				;; See if we should move to the next section
	jz	short new_section		;; Yes, go start next section

	mov	rsi, PPTR YMM_TMP1		;; Restore section pointers
ttp	mov	rdi, PPTR YMM_TMP2
ttp	mov	rbp, PPTR YMM_TMP3
	jmp	section_restart			;; Do the section again

new_section:
	vmovapd	ymm3, ymm5			;; Copy next section carry words
	vmovapd	ymm2, ymm4
	jmp	section_begin			;; Do next section

force_done:
base2		vsubpd	ymm3, ymm3, YMM_BIGVAL		;; Subtract rounding constant from carry
base2		vsubpd	ymm2, ymm2, YMM_BIGVAL
ttp		vmulpd	ymm3, ymm3, [rbp+32]		;; carry1 *= two-to-phi
ttp no zero	vmulpd	ymm2, ymm2, [rbp+96]		;; carry2 *= two-to-phi
		vaddpd	ymm3, ymm3, [rsi]		;; Add in values1
no zero		vaddpd	ymm2, ymm2, [rsi+32]		;; Add in values2
		vmovapd	[rsi], ymm3			;; Save new value1
no zero		vmovapd	[rsi+32], ymm2			;; Save new value2
base2		vmovapd	ymm3, YMM_BIGVAL		;; Clear carry1
no base2	vxorpd	ymm3, ymm3, ymm3
		vmovapd	ymm2, ymm3			;; Clear carry2

		;; Ugh, the next section's carries may not be zero (because adding a zero carry
		;; can cause a carry into the next section).  Keep going until they are zero.
done:		vcmppd	ymm0, ymm3, YMM_TMP5, 0Ch ;; Are any carries non-zero
		vmovmskpd rax, ymm0		;; Extract 4 comparison bits
no zero		vcmppd	ymm0, ymm2, YMM_TMP4, 0Ch ;; Are any carries non-zero
no zero		vmovmskpd rcx, ymm0		;; Extract 4 comparison bits
no zero		or	rax, rcx		;; Are any bits on?
zero		test	rax, rax		;; Are any bits on?
		jnz	do_more			;; Yes, do more carrying
	ENDM

; This macro is similar to the above, but is for the zero padding case.
; xmm2 = carry #1 (traditional carry)
; xmm3 = carry #2 (previous high FFT data - not yet mul'ed by K)
; rax,rsi,rbp,rdi = trashed
; NOTE: If RATIONAL_FFT we could eliminate 8 multiplies.

ynorm_1d_zpad_cleanup MACRO const, base2
	LOCAL	smallk, mediumk, div_k_done

	;; Strip BIGVAL from the traditional carry, we'll add the traditional
	;; carry in later when we are working on the ZPAD0 - ZPAD6 values.
base2	vsubsd	xmm2, xmm2, YMM_BIGVAL		;; Integerize traditional carry

	;; Rather than calculate high FFT carry times k and then later dividing
	;; by k, we multiply FFT high carry by const and we'll add it
	;; to the lower FFT data later (after multiplying by -c).
const	vmulsd	xmm3, xmm3, YMM_MULCONST

	;; Multiply ZPAD0 through ZPAD6 by const * -c.  This, in essense,
	;; wraps this data from above the FFT data area to the halfway point.
	;; Later on we'll divide this by K to decide which data needs wrapping
	;; all the way down to the bottom of the FFT data.

	;; NOTE that ZPAD0's ttp multiplier is 1.0.  Also, ZPAD6 will not
	;; be bigger than a big word.  We must be careful to handle c's up
	;; to about 30 bits

	lea	rdi, YMM_FIRST_BIGLIT_VALUES	;; Address of first big/little flags
	mov	rbp, norm_col_mults		;; Address of the ttmp/ttp multipliers

	movzx	rax, BYTE PTR [rdi]		;; Load big vs. little flags
	vmovsd	xmm0, ZPAD0			;; Load value
	vmovsd	xmm5, ADDIN_VALUE		;; Use ADDIN_VALUE as the initial carry to add in
	single_split_lower_zpad_word base2, xmm0, xmm5, xmm1, rax*2
no const vmulsd	xmm0, xmm0, YMM_MINUS_C
const	vmulsd	xmm0, xmm0, YMM_MINUS_C_TIMES_MULCONST
	round_zpad7_word base2, xmm0, xmm2, xmm1, rax*2
	vmovsd	ZPAD0, xmm0

	add	rbp, YMM_NORM_INCR1		;; Next ttp/ttmp pointer
	movzx	rax, BYTE PTR [rdi+1]		;; Load big vs. little flags
	vmovsd	xmm0, ZPAD1			;; Load value
	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul value by two-to-minus-phi
	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
	single_split_lower_zpad_word base2, xmm0, xmm5, xmm1, rax*2
no const vmulsd	xmm0, xmm0, YMM_MINUS_C
const	vmulsd	xmm0, xmm0, YMM_MINUS_C_TIMES_MULCONST
	round_zpad7_word base2, xmm0, xmm2, xmm1, rax*2
	vmovsd	ZPAD1, xmm0

	add	rbp, YMM_NORM_INCR2		;; Next ttp/ttmp pointer
	movzx	rax, BYTE PTR [rdi+2]		;; Load big vs. little flags
	vmovsd	xmm0, ZPAD2			;; Load value
	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul value by two-to-minus-phi
	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
	single_split_lower_zpad_word base2, xmm0, xmm5, xmm1, rax*2
no const vmulsd	xmm0, xmm0, YMM_MINUS_C
const	vmulsd	xmm0, xmm0, YMM_MINUS_C_TIMES_MULCONST
	round_zpad7_word base2, xmm0, xmm2, xmm1, rax*2
	vmovsd	ZPAD2, xmm0

	add	rbp, YMM_NORM_INCR3		;; Next ttp/ttmp pointer
	movzx	rax, BYTE PTR [rdi+3]		;; Load big vs. little flags
	vmovsd	xmm0, ZPAD3			;; Load value
	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul value by two-to-minus-phi
	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
	single_split_lower_zpad_word base2, xmm0, xmm5, xmm1, rax*2
no const vmulsd	xmm0, xmm0, YMM_MINUS_C
const	vmulsd	xmm0, xmm0, YMM_MINUS_C_TIMES_MULCONST
	round_zpad7_word base2, xmm0, xmm2, xmm1, rax*2
	vmovsd	ZPAD3, xmm0

	add	rbp, YMM_NORM_INCR4		;; Next ttp/ttmp pointer
	movzx	rax, BYTE PTR [rdi+4]		;; Load big vs. little flags
	vmovsd	xmm0, ZPAD4			;; Load value
	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul value by two-to-minus-phi
	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
	single_split_lower_zpad_word base2, xmm0, xmm5, xmm1, rax*2
no const vmulsd	xmm0, xmm0, YMM_MINUS_C
const	vmulsd	xmm0, xmm0, YMM_MINUS_C_TIMES_MULCONST
	round_zpad7_word base2, xmm0, xmm2, xmm1, rax*2
	vmovsd	ZPAD4, xmm0

	add	rbp, YMM_NORM_INCR5		;; Next ttp/ttmp pointer
	movzx	rax, BYTE PTR [rdi+5]		;; Load big vs. little flags
	vmovsd	xmm0, ZPAD5			;; Load value
	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul value by two-to-minus-phi
	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
	single_split_lower_zpad_word base2, xmm0, xmm5, xmm1, rax*2
no const vmulsd	xmm0, xmm0, YMM_MINUS_C
const	vmulsd	xmm0, xmm0, YMM_MINUS_C_TIMES_MULCONST
	round_zpad7_word base2, xmm0, xmm2, xmm1, rax*2
	vmovsd	ZPAD5, xmm0

	add	rbp, YMM_NORM_INCR6		;; Next ttp/ttmp pointer
	vmovsd	xmm0, ZPAD6			;; Load value
	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul value by two-to-minus-phi
	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vroundsd xmm0, xmm0, xmm0, 0		;; Round to an integer
	vaddsd	xmm0, xmm0, xmm5		;; Add in shifted high ZPAD data
no const vmulsd	xmm0, xmm0, YMM_MINUS_C
const	vmulsd	xmm0, xmm0, YMM_MINUS_C_TIMES_MULCONST
	vaddsd	xmm0, xmm0, xmm2		;; Add in high part of last calculation
	vmovsd	ZPAD6, xmm0

	;; Divide the zpad data by k.  Store the integer part in YMM_TMP
	;; and the remainder in ZPAD0.  Later we will wrap the integer part
	;; down to the bottom of the FFT data area (and multiply by -c).
	;; And we will store the remainder in the upper half of the FFT
	;; data area.

	;; Note there are three cases to handle.  K is smaller than a big word.
	;; K is between one and 2 big words in size.  And K is more than
	;; 2 big words in size.

	cmp	ZPAD_TYPE, 2			;; Are we dealing with case 1,2,or 3
	jl	smallk				;; One word case
	je	mediumk				;; Two word case

	;; This case does the divide by k where k is three words

	vmovsd	xmm0, ZPAD6			;; Load zpad word (high bits)
	vmovsd	xmm1, ZPAD5			;; Load zpad word (middle bits)
	vmovsd	xmm2, ZPAD4			;; Load zpad word (low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K6	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K6_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K6_MID		;; Mul by middle bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K6_LO		;; Mul by low bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP5, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT6		;; Shift previous zpad word
	vaddsd	xmm1, xmm1, xmm0		;; Add to create new high zpad bits
	vmovsd	xmm0, ZPAD3			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm1, ZPAD_SHIFT5		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm2
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K5	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K5_HI		;; Mul by high bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K5_MID		;; Mul by middle bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K5_LO		;; Mul by low bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP4, xmm4			;; Save word of zpad / k

	vmulsd	xmm1, xmm1, ZPAD_SHIFT5		;; Shift previous zpad word
	vaddsd	xmm2, xmm2, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD2			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm2, ZPAD_SHIFT4		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm0
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K4	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K4_HI		;; Mul by high bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K4_MID		;; Mul by middle bits of k
	vsubsd	xmm0,xmm0,xmm5			;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K4_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP3, xmm4			;; Save word of zpad / k

	vmulsd	xmm2, xmm2, ZPAD_SHIFT4		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm2		;; Add to create new high zpad bits
	vmovsd	xmm2, ZPAD1			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm0, ZPAD_SHIFT3		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm1
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K3	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K3_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K3_MID		;; Mul by middle bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K3_LO		;; Mul by low bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP2, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT3		;; Shift previous zpad word
	vaddsd	xmm1, xmm1, xmm0		;; Add to create new high zpad bits
	vmovsd	xmm0, ZPAD0			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm1, ZPAD_SHIFT2		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm2
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K2	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K2_HI		;; Mul by high bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K2_MID		;; Load middle bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K2_LO		;; Load low bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP1, xmm4			;; Save word of zpad / k

	vmulsd	xmm1, xmm1, ZPAD_SHIFT2		;; Shift previous zpad word
	vaddsd	xmm2, xmm2, xmm1		;; Add to create new high zpad bits
	vmulsd	xmm2, xmm2, ZPAD_SHIFT1		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm2		;; Add to create new high zpad bits
	vmovsd	ZPAD0, xmm0			;; Save remainder of zpad / k

	vxorpd	xmm1, xmm1, xmm1		;; Zero words that other cases set
	vmovsd	YMM_TMP6, xmm1
	
	jmp	div_k_done

	;; This case does the divide by k where k is two words
mediumk:
	vmovsd	xmm0, ZPAD6			;; Load zpad word (high bits)
	vmovsd	xmm1, ZPAD5			;; Load zpad word (low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K6	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K6_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K6_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP6, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT6		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD4			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K5	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K5_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K5_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP5, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT5		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD3			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K4	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K4_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K4_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP4, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT4		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD2			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K3	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K3_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K3_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP3, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT3		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD1			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K2	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K2_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K2_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP2, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT2		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD0			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K1_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K1_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP1, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT1		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	ZPAD0, xmm0			;; Save remainder of zpad / k

	jmp	div_k_done

	;; This case does the divide by k where k is one word
	;; Assume ZPAD5 and ZPAD6 are zero.
smallk:	vmovsd	xmm0, ZPAD4			;; Load zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP5, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT4		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD3		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP4, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT3		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD2		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP3, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT2		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD1		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP2, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT1		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD0		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP1, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4
	vmovsd	ZPAD0, xmm0			;; Save remainder

	vxorpd	xmm1, xmm1, xmm1		;; Zero words that other cases set
	vmovsd	YMM_TMP6, xmm1

div_k_done:

	;; Now normalize the data above the halfway point.  Remember that the
	;; two-to-phi multiplier for the first value will be 1.0.

	lea	rdi, YMM_FIRST_BIGLIT_VALUES	;; Address of first big/little flags
	mov	rsi, DESTARG			;; Address of squared number
	mov	rbp, norm_col_mults		;; Address of the ttmp/ttp multipliers

	movzx	rax, BYTE PTR [rdi]		;; First word
	vmovsd	xmm0, ZPAD0			;; Load remainder of divide by k
base2	vaddsd	xmm0, xmm0, YMM_BIGVAL
	single_rounding base2, xmm0, xmm2, xmm4, rax*2
	vmovsd	Q [rsi+32], xmm0		;; Save value1

	add	rbp, YMM_NORM_INCR1		;; Next ttp/ttmp pointer
	add	rsi, YMM_SRC_INCR1		;; Next source pointer
	movzx	rax, BYTE PTR [rdi+1]		;; Load big vs. little flags
	single_rounding base2, xmm2, xmm0, xmm4, rax*2
	vmulsd	xmm2, xmm2, Q [rbp+32]		;; new value2 = val * two-to-phi
	vmovsd	Q [rsi+32], xmm2		;; Save value2

	add	rbp, YMM_NORM_INCR2		;; Next ttp/ttmp pointer
	add	rsi, YMM_SRC_INCR2		;; Next source pointer
	movzx	rax, BYTE PTR [rdi+2]		;; Load big vs. little flags
	single_rounding base2, xmm0, xmm2, xmm4, rax*2
	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value3 = val * two-to-phi
	vmovsd	Q [rsi+32], xmm0		;; Save value3

	add	rbp, YMM_NORM_INCR3		;; Next ttp/ttmp pointer
	add	rsi, YMM_SRC_INCR3		;; Next source pointer
base2	vsubsd	xmm2, xmm2, YMM_BIGVAL		;; Remove integer rounding constant
	vmulsd	xmm2, xmm2, Q [rbp+32]		;; value4 = carry * two-to-phi
	vmovsd	Q [rsi+32], xmm2		;; Save new value4

	;; Mul the integer part of (ZPAD data divided by k) by -c in
	;; preparation for adding it into the lower FFT data area.
	;; Also add in the shifted high FFT carry at this time.

	;; Now add in and normalize the bottom FFT data.  Remember that the
	;; two-to-phi multiplier for the first value will be 1.0.  We 
	;; must go 6 words deep in case k is 48-50 bits and c is 32 bits.

	mov	rsi, DESTARG			;; Address of squared number
	mov	rbp, norm_col_mults		;; Address of ttmp/ttp multipliers

	movzx	rax, BYTE PTR [rdi]		;; First word
	vmovsd	xmm0, YMM_TMP1			;; Load integer part of divide by k
	vaddsd	xmm0, xmm0, xmm3		;; Add in shifted high FFT carry
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
base2	vaddsd	xmm0, xmm0, YMM_BIGVAL
	vaddsd	xmm0, xmm0, Q [rsi]		;; Add in the FFT data
	single_rounding base2, xmm0, xmm2, xmm4, rax*2
	vmovsd	Q [rsi], xmm0			;; Save value1

	add	rsi, YMM_SRC_INCR1		;; Next source pointer
	add	rbp, YMM_NORM_INCR1		;; Next ttp/ttmp pointer
	movzx	rax, BYTE PTR [rdi+1]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP2			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vmovsd	xmm1, Q [rsi]			;; Load FFT data
	vmulsd	xmm1, xmm1, Q [rbp]		;; Mul values2 by two-to-minus-phi
	vmulsd	xmm1, xmm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x2 = value + carry
	vaddsd	xmm0, xmm0, xmm1		;; Add in the FFT data
	single_rounding base2, xmm0, xmm2, xmm4, rax*2
	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value2 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value2

	add	rsi, YMM_SRC_INCR2		;; Next source pointer
	add	rbp, YMM_NORM_INCR2		;; Next ttp/ttmp pointer
	movzx	rax, BYTE PTR [rdi+2]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP3			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vmovsd	xmm1, Q [rsi]			;; Load FFT data
	vmulsd	xmm1, xmm1, Q [rbp]		;; Mul values3 by two-to-minus-phi
	vmulsd	xmm1, xmm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x3 = value + carry
	vaddsd	xmm0, xmm0, xmm1		;; Add in the FFT data
	single_rounding base2, xmm0, xmm2, xmm4, rax*2
	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value3 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value3

	add	rsi, YMM_SRC_INCR3		;; Next source pointer
	add	rbp, YMM_NORM_INCR3		;; Next ttp/ttmp pointer
	movzx	rax, BYTE PTR [rdi+3]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP4			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vmovsd	xmm1, Q [rsi]			;; Load FFT data
	vmulsd	xmm1, xmm1, Q [rbp]		;; Mul values4 by two-to-minus-phi
	vmulsd	xmm1, xmm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x4 = value + carry
	vaddsd	xmm0, xmm0, xmm1		;; Add in the FFT data
	single_rounding base2, xmm0, xmm2, xmm4, rax*2
	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value4 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value4

	add	rsi, YMM_SRC_INCR4		;; Next source pointer
	add	rbp, YMM_NORM_INCR4		;; Next ttp/ttmp pointer
	movzx	rax, BYTE PTR [rdi+4]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP5			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vmovsd	xmm1, Q [rsi]			;; Load FFT data
	vmulsd	xmm1, xmm1, Q [rbp]		;; Mul values4 by two-to-minus-phi
	vmulsd	xmm1, xmm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x5 = value + carry
	vaddsd	xmm0, xmm0, xmm1		;; Add in the FFT data
	single_rounding base2, xmm0, xmm2, xmm4, rax*2
	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value5 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value5

	add	rsi, YMM_SRC_INCR5		;; Next source pointer
	add	rbp, YMM_NORM_INCR5		;; Next ttp/ttmp pointer
	movzx	rax, BYTE PTR [rdi+5]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP6			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vmovsd	xmm1, Q [rsi]			;; Load FFT data
	vmulsd	xmm1, xmm1, Q [rbp]		;; Mul values6 by two-to-minus-phi
	vmulsd	xmm1, xmm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x6 = value + carry
	vaddsd	xmm0, xmm0, xmm1		;; Add in the FFT data
	single_rounding base2, xmm0, xmm2, xmm4, rax*2
	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value6 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value6

	add	rsi, YMM_SRC_INCR6		;; Next source pointer
	add	rbp, YMM_NORM_INCR6		;; Next ttp/ttmp pointer
	movzx	rax, BYTE PTR [rdi+6]		;; Load big vs. little flags
	vmovsd	xmm0, Q [rsi]			;; Load FFT data
	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul values7 by two-to-minus-phi
	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x7 = value + carry
	single_rounding base2, xmm0, xmm2, xmm4, rax*2
	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value7 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value7

	add	rsi, YMM_SRC_INCR7		;; Next source pointer
	add	rbp, YMM_NORM_INCR7		;; Next ttp/ttmp pointer
base2	vsubsd	xmm2, xmm2, YMM_BIGVAL		;; Remove rounding constant
	vmulsd	xmm2, xmm2, Q [rbp+32]		;; new value8 = val * two-to-phi
	vaddsd	xmm2, xmm2, Q [rsi]		;; Add in FFT data
	vmovsd	Q [rsi], xmm2			;; Save value8
	ENDM



; *************** Top carry adjust macro ******************
; This macro corrects the carry out of the topmost word when k is not 1.
; The problem is the top carry is from b^ceil(logb(k)+n) rather than at k*b^n.
; So we recompute the top carry by multiplying by b^ceil(logb(k)) and then
; dividing by k.  The integer part is the new carry and the remainder is
; added back to the top three words.

; The single-pass case, the top carry is in low word of ymm3
ynorm_top_carry_1d MACRO ttp, base2
	ynorm_top_carry_cmn base2, rsi, xmm3, 0
	ENDM

; The multi-pass case.  The top carry is loaded into xmm7 from the carries array.
ynorm_top_carry_wpn MACRO ttp, base2
	ynorm_top_carry_cmn base2, rsi, xmm7, 1
	ENDM

ynorm_top_carry_cmn MACRO base2, srcreg, xreg, twopass
	LOCAL	kok
	cmp	TOP_CARRY_NEEDS_ADJUSTING, 1 ;; Does top carry need work?
	jne	kok			;; Skip this code if K is 1

	IF twopass EQ 1			;; Two pass case - load the carry
	mov	rdi, carries		;; Addr of the carries
	mov	eax, addcount1		;; Load count of carry rows
	shl	rax, 6			;; Compute addr of the high carries
	add	rdi, rax
	vmovsd	xreg, Q [rdi-8]		;; Load very last carry
	ENDIF

	IF twopass EQ 0
	vmovapd	YMM_TMP6, ymm3		;; Save the full carry register
	ELSE
	vmovsd	YMM_TMP6, xreg		;; Save the carry for later use
	ENDIF

base2	vsubsd	xreg, xreg, YMM_BIGVAL	;; Convert top carry from int+BIGVAL state

	;; We want to calculate carry * b^ceil(logb(k)) / k and
	;; carry * b^ceil(logb(k)) % k.  This must be done very carefully as
	;; carry * b^ceil(logb(k)) may not fit in 53 bits.

	;; Here is a strategy that works for k values up to and including 34 bits.
	;; We do lots of modulo k operations along the way to insure all intermediate
	;; results are 51 bits or less.
	;; Calculate y = carry % k.  This will fit in 34 bits.
	;; Let z = b^ceil(logb(k)) % k.  Precalculate high_17_bits(z) and low_17_bits(z)
	;; Remainder is (high_17_bits(z) * y % k * 2^17 + low_17_bits(z) * y) % k

	vmulsd	xmm0, xreg, INVERSE_K		;; Mul top carry by 1/k
	vroundsd xmm0, xmm0, xmm0, 0		;; Integer part
	vmulsd	xmm0, xmm0, K
	vsubsd	xreg, xreg, xmm0		;; y = carry % k

	vmulsd	xmm0, xreg, CARRY_ADJUST1_HI	;; y * high_17_bits(z)
	vmulsd	xmm1, xmm0, INVERSE_K		;; Mul y * high_17_bits(z) by 1/k
	vroundsd xmm1, xmm1, xmm1, 0		;; Integer part
	vmulsd	xmm1, xmm1, K
	vsubsd	xmm0, xmm0, xmm1		;; y * high_17_bits(z) % k
	vmulsd	xmm0, xmm0, TWO_TO_17		;; y * high_17_bits(z) % k * 2^17
	vmulsd	xreg, xreg, CARRY_ADJUST1_LO	;; y * low_17_bits(z)
	vaddsd	xmm0, xmm0, xreg		;; y * high_17_bits(z) % k * 2^17 + y * low_17_bits(z)

	vmulsd	xmm1, xmm0, INVERSE_K		;; Mul by 1/k
	vroundsd xmm1, xmm1, xmm1, 0		;; Integer part
	vmulsd	xmm1, xmm1, K
	vsubsd	xmm0, xmm0, xmm1		;; Remainder!!!

	;; Finally calculate integer_part = (carry * b^ceil(logb(k)) - remainder) / k

	vmovsd	xreg, YMM_TMP6			;; Reload top carry
base2	vsubsd	xreg, xreg, YMM_BIGVAL		;; Convert top carry from int+BIGVAL state
	vmulsd	xreg, xreg, CARRY_ADJUST1	;; Mul by b^ceil(logb(k))
	vsubsd	xreg, xreg, xmm0		;; Subtract the remainder
	vmulsd	xreg, xreg, INVERSE_K		;; Mul by 1/k
	vroundsd xreg, xreg, xreg, 0		;; Integer part of top carry over k

	;; Now add the remainder to the top words

	vmulsd	xmm0, xmm0, CARRY_ADJUST2	;; Shift remainder
	vroundsd xmm1, xmm0, xmm0, 0		;; Integer part of shifted remainder
	vsubsd	xmm0, xmm0, xmm1		;; Fractional part of shifted remainder
	vmulsd	xmm1, xmm1, CARRY_ADJUST3	;; Weight integer part

	;; Two pass scratch area case
	IF twopass EQ 1
	mov	eax, HIGH_SCRATCH1_OFFSET	;; Add integer part to top word
	vaddsd	xmm1, xmm1, Q [srcreg][rax]
	vmovsd	Q [srcreg][rax], xmm1
	vmulsd	xmm0, xmm0, CARRY_ADJUST4	;; Shift fractional part
	vroundsd xmm0, xmm0, xmm0, 0
	vmulsd	xmm0, xmm0, CARRY_ADJUST5	;; Weight fractional part
	mov	eax, HIGH_SCRATCH2_OFFSET	;; Add frac part to top-1 word
	vaddsd	xmm0, xmm0, Q [srcreg][rax]
	vmovsd	Q [srcreg][rax], xmm0
	ENDIF

	;; Two pass FFT data case
	IF twopass EQ 2
	mov	eax, HIGH_WORD1_OFFSET		;; Add integer part to top word
	vaddsd	xmm1, xmm1, Q [srcreg][rax]
	vmovsd	Q [srcreg][rax], xmm1
	vmulsd	xmm0, xmm0, CARRY_ADJUST4	;; Shift fractional part
	vroundsd xmm0, xmm0, xmm0, 0
	vmulsd	xmm0, xmm0, CARRY_ADJUST5	;; Weight fractional part
	mov	eax, HIGH_WORD2_OFFSET		;; Add frac part to top-1 word
	vaddsd	xmm0, xmm0, Q [srcreg][rax]
	vmovsd	Q [srcreg][rax], xmm0
	ENDIF

	;; Single pass case
	IF twopass EQ 0
	mov	eax, HIGH_WORD1_OFFSET		;; Add integer part to top word
	vaddsd	xmm1, xmm1, Q [srcreg][rax]
	vmovsd	Q [srcreg][rax], xmm1
	vmulsd	xmm0, xmm0, CARRY_ADJUST4	;; Shift fractional part
	vroundsd xmm1, xmm0, xmm0, 0		;; Integer part of shifted fractional
	vsubsd	xmm0, xmm0, xmm1		;; Fractional part
	vmulsd	xmm1, xmm1, CARRY_ADJUST5	;; Weight integer part
	mov	eax, HIGH_WORD2_OFFSET		;; Add frac part to top-1 word
	vaddsd	xmm1, xmm1, Q [srcreg][rax]
	vmovsd	Q [srcreg][rax], xmm1
	vmulsd	xmm0, xmm0, CARRY_ADJUST6	;; Shift fractional part
	vroundsd xmm0, xmm0, xmm0, 0
	vmulsd	xmm0, xmm0, CARRY_ADJUST7	;; Weight fractional part
	mov	eax, HIGH_WORD3_OFFSET		;; Add frac part to top-2 word
	vaddsd	xmm0, xmm0, Q [srcreg][rax]
	vmovsd	Q [srcreg][rax], xmm0
	ENDIF

base2	vaddsd	xreg, xreg, YMM_BIGVAL		;; Restore carry to int+BIGVAL state

	IF twopass EQ 0
	vblendpd ymm3, ymm3, YMM_TMP6, 1110b	;; Blend in the new carry
	ENDIF

	IF twopass EQ 1
	vmovsd	Q [rdi-8], xreg			;; Save very last carry
	ENDIF

kok:
	ENDM


; For WPN macros, these registers are set on input:
; ymm6 = maxerr
; rbp = pointer to carries
; rdi = pointer to big/little flags
; rsi = pointer to the FFT data
; rdx = pointer two-to-phi group multipliers
; ebx = big vs. little & fudge flags
; ecx = big vs. little word flag #2
; eax = big vs. little word flag #1
; ymm2,ymm3 = carries

ynorm_wpn_preload MACRO ttp, base2, zero, echk, const
	ENDM

IFDEF X86_64
ynorm_wpn_preload MACRO ttp, base2, zero, echk, const
echk		vmovapd	ymm15, YMM_ABSVAL
base2 const	vmovapd	ymm14, YMM_BIGBIGVAL
const		vmovapd	ymm13, YMM_MULCONST
	ENDM
ENDIF

ynorm_wpn MACRO ttp, base2, zero, echk, const
		L1prefetchw rsi+64, L1PREFETCH_ALL
ttp		movzx	ecx, bl				;; Fudge flags 1
ttp		and	rcx, 0e0h
ttp		movzx	eax, bh				;; Big/lit flags 1-2
ttp		and	rbx, 01ch			;; Fudge flags 2
		vmovapd	ymm0, [rsi+0*32]		;; Load values1
ttp		vmulpd	ymm0, ymm0, [rdx+0*YMM_GMD][rcx] ;; Mul by fudged grp two-to-minus-phi
		vmovapd	ymm1, [rsi+1*32]		;; Load values2
ttp		vmulpd	ymm1, ymm1, [rdx+1*YMM_GMD][rbx*8] ;; Mul by fudged grp two-to-minus-phi
IFDEF X86_64
ttp		L1prefetch r9, L1PREFETCH_ALL
ttp		bump	r9, 64
ENDIF
const		mul_by_const_interleaved ttp, base2, echk, ymm0, ymm2, ymm4, rax*8, ymm1, ymm3, ymm5, rax*8+32, ymm6
no const echk	error_check_interleaved ymm0, ymm4, ymm1, ymm5, ymm6
no const	vaddpd	ymm0, ymm0, ymm2		;; x1 = values + carry
no const	vaddpd	ymm1, ymm1, ymm3		;; x2 = values + carry
		rounding_interleaved ttp, base2, const, ymm0, ymm2, ymm4, rax*8, ymm1, ymm3, ymm5, rax*8+32
ttp		vmulpd	ymm0, ymm0, [rdx+0*YMM_GMD+YMM_GMD/2][rcx] ;; value1 = rounded value * fudged grp two-to-phi
ttp no zero	vmulpd	ymm1, ymm1, [rdx+1*YMM_GMD+YMM_GMD/2][rbx*8] ;; value2 = rounded value * fudged grp two-to-phi
zero		vxorpd	ymm1, ymm1, ymm1
ttp		movzx	rbx, WORD PTR [rdi+2]		;; Load next 4 big vs. little & fudge flags
		vmovapd	[rsi+0*32], ymm0		;; Save new value1
		vmovapd	[rsi+1*32], ymm1		;; Save new value2
	ENDM

; This is the normalization routine when we are computing modulo k*b^n+c
; with a zero-padded b^2n FFT.  We do this by multiplying the lower FFT
; word by k and adding in the upper word times -c.  Of course, this is made
; very tedious because we have to carefully avoid any loss of precision.
;
;; NOTE: In zero pad FFTs, big/lit and fudge factor flags 1 and 2 are identical

; For WPN zpad macros, these registers are set on input:
; ymm6 = maxerr
; rbp = pointer to carries
; rdi = pointer to big/little flags
; rsi = pointer to the FFT data
; rdx = pointer two-to-phi group multipliers
; ebx = big vs. little & fudge flags
; eax = big vs. little word flag #1
; ymm2,ymm3 = carries

ynorm_wpn_zpad_preload MACRO ttp, base2, echk, const, khi, c1, cm1
	ENDM

ynorm_wpn_zpad MACRO ttp, base2, echk, const, khi, c1, cm1
			L1prefetchw rsi+64, L1PREFETCH_ALL
ttp			movzx	eax, bh				;; Big/lit flags 1-2
ttp			and	rbx, 0e0h			;; Fudge flags 2
			vmovapd	ymm0, [rsi+0*32]		;; Load values1
ttp			vmulpd	ymm0, ymm0, [rdx+0*YMM_GMD][rbx] ;; Mul by fudged grp two-to-minus-phi
			vmovapd	ymm1, [rsi+1*32]		;; Load values2
ttp			vmulpd	ymm1, ymm1, [rdx+0*YMM_GMD][rbx] ;; Mul by fudged grp two-to-minus-phi

			split_lower_zpad_word ttp, base2, echk, ymm0, ymm3, ymm4, rax*8

no const		vmulpd	ymm0, ymm4, YMM_K_LO
const			vmulpd	ymm0, ymm4, YMM_K_TIMES_MULCONST_LO

khi base2 no const	vmulpd	ymm5, ymm4, YMM_K_HI
khi base2 const		vmulpd	ymm5, ymm4, YMM_K_TIMES_MULCONST_HI
khi no base2 no const	vmovapd	ymm5, YMM_K_HI
khi no base2 const	vmovapd	ymm5, YMM_K_TIMES_MULCONST_HI
khi no base2 ttp	vmulpd	ymm5, ymm5, YMM_LIMIT_INVERSE[rax*8] ;; Non-base2 rounding needs shifted carry
khi no base2 no ttp	vmulpd	ymm5, ymm5, YMM_LIMIT_INVERSE[0] ;; Non-base2 rounding needs shifted carry
khi no base2		vroundpd ymm5, ymm5, 0			;; THIS IS WASTEFUL.  The mul and round should be precomputed!
khi no base2		vmulpd	ymm5, ymm5, ymm4

			vaddpd	ymm0, ymm0, ymm2		;; x1 = values + carry

c1			vmulpd	ymm1, ymm1, YMM_MINUS_C		;; Do one mul before split rather than two after split

			split_upper_zpad_word ttp, base2, echk, ymm1, ymm2, ymm4, rax*8

no const no c1 no cm1	vmulpd	ymm4, ymm4, YMM_MINUS_C
no const no c1 no cm1	vmulpd	ymm2, ymm2, YMM_MINUS_C
const			vmulpd	ymm4, ymm4, YMM_MINUS_C_TIMES_MULCONST
const			vmulpd	ymm2, ymm2, YMM_MINUS_C_TIMES_MULCONST

			vaddpd	ymm0, ymm0, ymm4		;; Add upper FFT word to lower FFT word
khi			vaddpd	ymm2, ymm2, ymm5		;; Add upper FFT word to lower FFT word

			rounding ttp, base2, exec, ymm0, ymm2, ymm4, rax*8

ttp			vmulpd	ymm0, ymm0, [rdx+0*YMM_GMD+YMM_GMD/2][rbx] ;; new value1 *= fudged grp two-to-phi
			vmovapd	[rsi], ymm0			;; Save new value1
			vxorpd	ymm1, ymm1, ymm1		;; new value2 = zero
			vmovapd	[rsi+32], ymm1			;; Zero value2

ttp			movzx	rbx, WORD PTR [rdi+2]		;; Load next big vs. little & fudge flags
	ENDM

;; 64-bit version using extra registers

IFDEF X86_64

ynorm_wpn_zpad_preload MACRO ttp, base2, echk, const, khi, c1, cm1
echk			vmovapd	ymm15, YMM_ABSVAL
base2			vmovapd	ymm14, YMM_BIGBIGVAL
no const		vmovapd	ymm13, YMM_K_LO
const			vmovapd	ymm13, YMM_K_TIMES_MULCONST_LO
khi no const		vmovapd	ymm12, YMM_K_HI
khi const		vmovapd	ymm12, YMM_K_TIMES_MULCONST_HI
no const no c1 no cm1	vmovapd	ymm11, YMM_MINUS_C
const			vmovapd	ymm11, YMM_MINUS_C_TIMES_MULCONST
	ENDM

ynorm_wpn_zpad MACRO ttp, base2, echk, const, khi, c1, cm1
			L1prefetchw rsi+64, L1PREFETCH_ALL
ttp			movzx	eax, bh				;; Big/lit flags 1-2
ttp			and	rbx, 0e0h			;; Fudge flags 2
			vmovapd	ymm0, [rsi+0*32]		;; Load values1
ttp			vmulpd	ymm0, ymm0, [rdx+0*YMM_GMD][rbx] ;; Mul by fudged grp two-to-minus-phi
			vmovapd	ymm1, [rsi+1*32]		;; Load values2
ttp			vmulpd	ymm1, ymm1, [rdx+0*YMM_GMD][rbx] ;; Mul by fudged grp two-to-minus-phi

			split_lower_zpad_word ttp, base2, echk, ymm0, ymm3, ymm4, rax*8

			vmulpd	ymm0, ymm4, ymm13		;; Mul by YMM_K_LO or YMM_K_TIMES_MULCONST_LO

khi base2		vmulpd	ymm5, ymm4, ymm12		;; Mul by YMM_K_HI or YMM_K_TIMES_MULCONST_HI
khi no base2 ttp	vmulpd	ymm5, ymm12, YMM_LIMIT_INVERSE[rax*8] ;; Non-base2 rounding needs shifted carry
khi no base2 no ttp	vmulpd	ymm5, ymm12, YMM_LIMIT_INVERSE[0] ;; Non-base2 rounding needs shifted carry
khi no base2		vroundpd ymm5, ymm5, 0			;; THIS IS WASTEFUL.  The mul and round should be precomputed!
khi no base2		vmulpd	ymm5, ymm5, ymm4

			vaddpd	ymm0, ymm0, ymm2		;; x1 = values + carry

c1			vmulpd	ymm1, ymm1, YMM_MINUS_C		;; Do one mul before split rather than two after split

			split_upper_zpad_word ttp, base2, echk, ymm1, ymm2, ymm4, rax*8

no const no c1 no cm1	vmulpd	ymm4, ymm4, ymm11		;; Mul by YMM_MINUS_C
no const no c1 no cm1	vmulpd	ymm2, ymm2, ymm11
const			vmulpd	ymm4, ymm4, ymm11		;; Mul by YMM_MINUS_C_TIMES_MULCONST
const			vmulpd	ymm2, ymm2, ymm11

			vaddpd	ymm0, ymm0, ymm4		;; Add upper FFT word to lower FFT word
khi			vaddpd	ymm2, ymm2, ymm5		;; Add upper FFT word to lower FFT word

			rounding ttp, base2, exec, ymm0, ymm2, ymm4, rax*8

ttp			vmulpd	ymm0, ymm0, [rdx+0*YMM_GMD+YMM_GMD/2][rbx] ;; new value1 *= fudged grp two-to-phi
			vmovapd	[rsi], ymm0			;; Save new value1
			vxorpd	ymm1, ymm1, ymm1		;; new value2 = zero
			vmovapd	[rsi+32], ymm1			;; Zero value2

ttp			movzx	rbx, WORD PTR [rdi+2]		;; Load next big vs. little & fudge flags
	ENDM

ENDIF


; *************** WPN followup macros ******************

; This macro does initial prep work by shuffling the carries from the last pass 1 block.
; rsi = start of the carries array

ynorm012_wpn_part1 MACRO base2
	LOCAL	shuflp, done
	cmp	THIS_BLOCK, 0		;; Are we carrying into the first data block?
	jne	done			;; If not, skip wrapping carries and negating the last carry
	mov	eax, addcount1		;; Load count of cache lines in the carries array
shuflp:	vmovsd	xmm4, Q [rsi+0*32]	;; Shift 3 carry words up
	vmovsd	Q [rsi+0*32], xmm0	;; Save prev cache line's high carry in low word
	vmovsd	xmm5, Q [rsi+0*32+8]
	vmovsd	Q [rsi+0*32+8], xmm4
	vmovsd	xmm6, Q [rsi+0*32+16]
	vmovsd	Q [rsi+0*32+16], xmm5
	vmovsd	xmm0, Q [rsi+0*32+24]
	vmovsd	Q [rsi+0*32+24], xmm6
	vmovsd	xmm4, Q [rsi+1*32]	;; Shift 3 carry words up
	vmovsd	Q [rsi+1*32], xmm1	;; Save prev cache line's high carry in low word
	vmovsd	xmm5, Q [rsi+1*32+8]
	vmovsd	Q [rsi+1*32+8], xmm4
	vmovsd	xmm6, Q [rsi+1*32+16]
	vmovsd	Q [rsi+1*32+16], xmm5
	vmovsd	xmm1, Q [rsi+1*32+24]
	vmovsd	Q [rsi+1*32+24], xmm6
	bump	rsi, 64			;; Next carry cache line
	dec	rax			;; Decement count of cache lines
	jnz	short shuflp
base2	vsubsd	xmm1, xmm1, YMM_BIGVAL
	vmulsd	xmm1, xmm1, YMM_MINUS_C	;; Negate the very last carry
base2	vaddsd	xmm1, xmm1, YMM_BIGVAL
	mov	rsi, carries		;; Reload carries array pointer
	vmovsd	Q [rsi+0*32], xmm1	;; Move last cache line's high carries into first cache line
	vmovsd	Q [rsi+1*32], xmm0
done:
	ENDM

; This macro finishes the normalize process by adding back the carries
; from each pass 1 block.  Three of the YMM carries are shifted and added
; back in to the current block, one of the YMM section carries is applied
; to the next block.
; The num_postfft_blocks setting in gwdata controls how many words we can safely carry into.
; We access this information in asm_data's SPREAD_CARRY_OVER_EXTRA_WORDS variable.
; rsi = pointer to carries
; rbp = pointer to FFT data
; rdi = pointer to big/little flags
; rdx = pointer two-to-phi group multipliers
; rax,rbx,rcx = destroyed

ynorm012_wpn MACRO ttp, base2, srcptr, carryptr, biglitptr
	LOCAL	nz, cloop, last_iter, done

	vmovapd	ymm2, [rsi]			;; Load low carry word
	vmovapd	ymm3, [rsi+32]			;; Load hi carry word

base2	cmp	zero_fft, 1			;; Are we zeroing high words?
base2	jne	short nz			;; No, leave top carry alone
base2	vmovapd	ymm3, YMM_BIGVAL		;; Yes, don't add carries into upper words
nz:

	mov	srcptr, rbp			;; Save pointers
	mov	carryptr, rsi
ttp	mov	biglitptr, rdi
	mov	al, SPREAD_CARRY_OVER_EXTRA_WORDS ;; True if spreading carry over 8 words

cloop:

ttp		movzx	rbx, WORD PTR [rdi]		;; Load big vs. little flags
ttp		movzx	ecx, bl				;; Fudge flags 1
ttp		and	rcx, 0e0h
ttp		movzx	esi, bh				;; Big/lit flags 1-2
ttp		and	rbx, 01ch			;; Fudge flags 2

		vmovapd	ymm0, [rbp]			;; Load values1
		vmovapd	ymm1, [rbp+32]			;; Load values2

		cmp	al, 3*(256/4)			;; Is this the last iteration
		je	last_iter			;; Yes, last iteration requires special handling

ttp		vmulpd	ymm0, ymm0, [rdx+0*YMM_GMD][rcx] ;; Mul values1 by fudged two-to-minus-phi
ttp		vmulpd	ymm1, ymm1, [rdx+1*YMM_GMD][rbx*8] ;; Mul values2 by fudged two-to-minus-phi
		vaddpd	ymm0, ymm0, ymm2		;; x1 = values1 + carry
		vaddpd	ymm1, ymm1, ymm3		;; x2 = values2 + carry
		rounding_interleaved ttp, base2, noexec, ymm0, ymm2, ymm4, rsi*8, ymm1, ymm3, ymm5, rsi*8+32
ttp		vmulpd	ymm0, ymm0, [rdx+0*YMM_GMD+YMM_GMD/2][rcx] ;; value1 = rounded value * fudged grp two-to-phi
ttp		vmulpd	ymm1, ymm1, [rdx+1*YMM_GMD+YMM_GMD/2][rbx*8] ;; value2 = rounded value * fudged grp two-to-phi
		vmovapd	[rbp], ymm0			;; Save new value1
		vmovapd	[rbp+32], ymm1			;; Save new value2

base2		vmovapd	ymm1, YMM_BIGVAL		;; Load comparison value
no base2	vxorpd	ymm1, ymm1, ymm1		;; Create comparison value
		vcmppd	ymm0, ymm2, ymm1, 0Ch		;; Test for non-zero carries
		vmovmskpd rbx, ymm0			;; Extract 4 comparison bits
		vcmppd	ymm0, ymm3, ymm1, 0Ch		;; Test for non-zero carries
		vmovmskpd rcx, ymm0			;; Extract 4 comparison bits
		or	rbx, rcx			;; Are any bits on?
		jz	short done			;; No, we are done

ttp	bump	rdi, 2				;; Advance pointers
	add	rbp, pass2blkdst		;; Next FFT data ptr
 	add	al, 256/4			;; Special increment needed every 4th iteration
	jnc	cloop				;; Loop if not 4th iteration
	sub	rbp, pass1blkdst		;; Each pass1 block consists of 4 pass 2 blocks
	bump	rbp, 64				;; Next cache line
	dec	al				;; Decrement count of sets of 4 carries we've processed
ttp	cmp	cache_line_multiplier, 4	;; We must increment rdi by a funky amount every 4*clm cache lines
ttp	jne	cloop				;; No, repeat carry propagation loop
ttp	add	rdi, normval2			;; Adjust ptr to little/big flags
	jmp	cloop				;; Repeat carry propagation loop

		;; Do last iteration without propagating carries out
last_iter:
base2		vsubpd	ymm2, ymm2, YMM_BIGVAL		;; Subtract rounding constant from carries
base2		vsubpd	ymm3, ymm3, YMM_BIGVAL
ttp		vmulpd	ymm2, ymm2, [rdx+0*YMM_GMD+YMM_GMD/2][rcx] ;; Carry *= fudged grp two-to-phi
ttp		vmulpd	ymm3, ymm3, [rdx+1*YMM_GMD+YMM_GMD/2][rbx*8]
		vaddpd	ymm0, ymm0, ymm2		;; Values1 += carry
		vaddpd	ymm1, ymm1, ymm3		;; Values2 += carry
		vmovapd	[rbp], ymm0			;; Save new value1
		vmovapd	[rbp+32], ymm1			;; Save new value2

done:	mov	rbp, srcptr			;; Restore pointers
	mov	rsi, carryptr
ttp	mov	rdi, biglitptr

base2	vmovapd	ymm4, YMM_BIGVAL		;; Clear conventional carry
no base2 vxorpd ymm4, ymm4, ymm4
	vmovapd	[rsi], ymm4
	vmovapd	[rsi+32], ymm4
	ENDM


;; Significantly different cleanup code for zero-padded FFTs.
;; Note: The group multiplier should be 1.0 for both the bottom FFT words and
;; the FFT words just above the half-way point.
;; rbx = pointer past the end of the carries array

ynorm012_wpn_zpad_part1 MACRO ttp, base2
	LOCAL	shuflp, noncon, zpaddn, done
	cmp	THIS_BLOCK, 0		;; Are we carrying into the first data block?
	jne	done			;; If not, skip wrapping carries and negating the last carry
	mov	eax, addcount1		;; Load count of cache lines in the carries array
shuflp:	vmovsd	xmm4, Q [rsi+0*32]	;; Shift 3 carry words up
	vmovsd	Q [rsi+0*32], xmm2	;; Save prev cache line's high carry in low word
	vmovsd	xmm5, Q [rsi+0*32+8]
	vmovsd	Q [rsi+0*32+8], xmm4
	vmovsd	xmm6, Q [rsi+0*32+16]
	vmovsd	Q [rsi+0*32+16], xmm5
	vmovsd	xmm2, Q [rsi+0*32+24]
	vmovsd	Q [rsi+0*32+24], xmm6
	vmovsd	xmm4, Q [rsi+1*32]	;; Shift 3 carry words up
	vmovsd	Q [rsi+1*32], xmm3	;; Save prev cache line's high carry in low word
	vmovsd	xmm5, Q [rsi+1*32+8]
	vmovsd	Q [rsi+1*32+8], xmm4
	vmovsd	xmm6, Q [rsi+1*32+16]
	vmovsd	Q [rsi+1*32+16], xmm5
	vmovsd	xmm3, Q [rsi+1*32+24]
	vmovsd	Q [rsi+1*32+24], xmm6
	bump	rsi, 64			;; Next carry cache line
	dec	rax			;; Decement count of cache lines
	jnz	short shuflp
;;	mov	rsi, carries		;; Reload carries array pointer
;;	vmovsd	Q [rsi+0*32], xmm3	;; Move last cache line's high carries into first cache line
;;	vmovsd	Q [rsi+1*32], xmm2

	mov	rsi, DESTARG		;; Address of FFT data
ttp	lea	rdi, YMM_FIRST_BIGLIT_VALUES ;; Address of first big/little flags

	cmp	const_fft, 0		;; Are we also multiplying by a constant?
	je	noncon			;; Jump if not const
	ynorm012_wpn_zpad_part1_cmn ttp, base2, exec
	jmp	zpaddn
noncon:	ynorm012_wpn_zpad_part1_cmn ttp, base2, noexec

zpaddn:	mov	rsi, carries		;; Reload carries array pointer
base2	vmovsd	xmm6, YMM_BIGVAL	;; Clear two carries just processed by ynorm012_wpn_zpad_part1_cmn
no base2 vxorpd	xmm6, xmm6, xmm6
	vxorpd	xmm7, xmm7, xmm7
	vmovsd	Q [rsi], xmm6
	vmovsd	Q [rsi+32], xmm7
done:
	ENDM

;; On input, xmm2 and xmm3 contain high carries from the last carries array row
ynorm012_wpn_zpad_part1_cmn MACRO ttp, base2, const
	LOCAL	smallk, mediumk, div_k_done

	;; Strip BIGVAL from the traditional carry, we'll add the traditional
	;; carry in later when we are working on the ZPAD0 - ZPAD6 values.
base2	vsubsd	xmm2, xmm2, YMM_BIGVAL	;; Integerize traditional carry

	;; Rather than calculate high FFT carry times k and then later dividing
	;; by k, we multiply FFT high carry by const and we'll add it
	;; to the lower FFT data later (after multiplying by -c).
const	vmulsd	xmm3, xmm3, YMM_MULCONST

	;; Multiply ZPAD0 through ZPAD6 by const * -C.  This, in essense,
	;; wraps this data from above the FFT data area to the halfway point.
	;; Later on we'll divide this by K to decide which data needs wrapping
	;; all the way down to the bottom of the FFT data.

	;; NOTE: ZPAD0's grp multiplier is 1.0.  Also, ZPAD6 will not
	;; be bigger than a big word.  We must be careful to handle c's up
	;; to about 30 bits

ttp	movzx	rax, BYTE PTR [rdi]		;; Load big vs. little flags
	vmovsd	xmm0, ZPAD0			;; Load value
	vmovsd	xmm5, ADDIN_VALUE		;; Use ADDIN_VALUE as the initial carry to add in
	new_single_split_lower_zpad_word ttp, base2, xmm0, xmm5, xmm1, rax*8
no const vmulsd	xmm0, xmm0, YMM_MINUS_C
const	vmulsd	xmm0, xmm0, YMM_MINUS_C_TIMES_MULCONST
	new_round_zpad7_word ttp, base2, xmm0, xmm2, xmm1, rax*8
	vmovsd	ZPAD0, xmm0

ttp	movzx	rax, BYTE PTR [rdi+1]		;; Load big vs. little flags
	vmovsd	xmm0, ZPAD1			;; Load value
	new_single_split_lower_zpad_word ttp, base2, xmm0, xmm5, xmm1, rax*8
no const vmulsd	xmm0, xmm0, YMM_MINUS_C
const	vmulsd	xmm0, xmm0, YMM_MINUS_C_TIMES_MULCONST
	new_round_zpad7_word ttp, base2, xmm0, xmm2, xmm1, rax*8
	vmovsd	ZPAD1, xmm0

ttp	movzx	rax, BYTE PTR [rdi+2]		;; Load big vs. little flags
	vmovsd	xmm0, ZPAD2			;; Load value
	new_single_split_lower_zpad_word ttp, base2, xmm0, xmm5, xmm1, rax*8
no const vmulsd	xmm0, xmm0, YMM_MINUS_C
const	vmulsd	xmm0, xmm0, YMM_MINUS_C_TIMES_MULCONST
	new_round_zpad7_word ttp, base2, xmm0, xmm2, xmm1, rax*8
	vmovsd	ZPAD2, xmm0

ttp	movzx	rax, BYTE PTR [rdi+3]		;; Load big vs. little flags
	vmovsd	xmm0, ZPAD3			;; Load value
	new_single_split_lower_zpad_word ttp, base2, xmm0, xmm5, xmm1, rax*8
no const vmulsd	xmm0, xmm0, YMM_MINUS_C
const	vmulsd	xmm0, xmm0, YMM_MINUS_C_TIMES_MULCONST
	new_round_zpad7_word ttp, base2, xmm0, xmm2, xmm1, rax*8
	vmovsd	ZPAD3, xmm0

ttp	movzx	rax, BYTE PTR [rdi+4]		;; Load big vs. little flags
	vmovsd	xmm0, ZPAD4			;; Load value
	new_single_split_lower_zpad_word ttp, base2, xmm0, xmm5, xmm1, rax*8
no const vmulsd	xmm0, xmm0, YMM_MINUS_C
const	vmulsd	xmm0, xmm0, YMM_MINUS_C_TIMES_MULCONST
	new_round_zpad7_word ttp, base2, xmm0, xmm2, xmm1, rax*8
	vmovsd	ZPAD4, xmm0

ttp	movzx	rax, BYTE PTR [rdi+5]		;; Load big vs. little flags
	vmovsd	xmm0, ZPAD5			;; Load value
	new_single_split_lower_zpad_word ttp, base2, xmm0, xmm5, xmm1, rax*8
no const vmulsd	xmm0, xmm0, YMM_MINUS_C
const	vmulsd	xmm0, xmm0, YMM_MINUS_C_TIMES_MULCONST
	new_round_zpad7_word ttp, base2, xmm0, xmm2, xmm1, rax*8
	vmovsd	ZPAD5, xmm0

	vmovsd	xmm0, ZPAD6			;; Load value
	vroundsd xmm0, xmm0, xmm0, 0		;; Round to an integer
	vaddsd	xmm0, xmm0, xmm5		;; Add in shifted high ZPAD data
no const vmulsd	xmm0, xmm0, YMM_MINUS_C
const	vmulsd	xmm0, xmm0, YMM_MINUS_C_TIMES_MULCONST
	vaddsd	xmm0, xmm0, xmm2		;; Add in high part of last calculation
	vmovsd	ZPAD6, xmm0

	;; Divide the zpad data by k.  Store the integer part in YMM_TMP
	;; and the remainder in ZPAD0.  Later we will wrap the integer part
	;; down to the bottom of the FFT data area (and multiply by -c).
	;; And we will store the remainder in the upper half of the FFT
	;; data area.

	;; Note there are three cases to handle.  K is smaller than a big word.
	;; K is between one and 2 big words in size.  And K is more than
	;; 2 big words in size.

	cmp	ZPAD_TYPE, 2		;; Are we dealing with case 1,2,or 3
	jl	smallk			;; One word case
	je	mediumk			;; Two word case

	;; This case does the divide by k where k is three words

	vmovsd	xmm0, ZPAD6			;; Load zpad word (high bits)
	vmovsd	xmm1, ZPAD5			;; Load zpad word (middle bits)
	vmovsd	xmm2, ZPAD4			;; Load zpad word (low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K6	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K6_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K6_MID		;; Mul by middle bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K6_LO		;; Mul by low bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP5, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT6		;; Shift previous zpad word
	vaddsd	xmm1, xmm1, xmm0		;; Add to create new high zpad bits
	vmovsd	xmm0, ZPAD3			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm1, ZPAD_SHIFT5		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm2
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K5	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K5_HI		;; Mul by high bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K5_MID		;; Mul by middle bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K5_LO		;; Mul by low bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP4, xmm4			;; Save word of zpad / k

	vmulsd	xmm1, xmm1, ZPAD_SHIFT5		;; Shift previous zpad word
	vaddsd	xmm2, xmm2, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD2			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm2, ZPAD_SHIFT4		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm0
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K4	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K4_HI		;; Mul by high bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K4_MID		;; Mul by middle bits of k
	vsubsd	xmm0,xmm0,xmm5			;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K4_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP3, xmm4			;; Save word of zpad / k

	vmulsd	xmm2, xmm2, ZPAD_SHIFT4		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm2		;; Add to create new high zpad bits
	vmovsd	xmm2, ZPAD1			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm0, ZPAD_SHIFT3		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm1
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K3	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K3_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K3_MID		;; Mul by middle bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K3_LO		;; Mul by low bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP2, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT3		;; Shift previous zpad word
	vaddsd	xmm1, xmm1, xmm0		;; Add to create new high zpad bits
	vmovsd	xmm0, ZPAD0			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm1, ZPAD_SHIFT2		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm2
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K2	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K2_HI		;; Mul by high bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K2_MID		;; Load middle bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K2_LO		;; Load low bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP1, xmm4			;; Save word of zpad / k

	vmulsd	xmm1, xmm1, ZPAD_SHIFT2		;; Shift previous zpad word
	vaddsd	xmm2, xmm2, xmm1		;; Add to create new high zpad bits
	vmulsd	xmm2, xmm2, ZPAD_SHIFT1		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm2		;; Add to create new high zpad bits
	vmovsd	ZPAD0, xmm0			;; Save remainder of zpad / k

	vxorpd	xmm1, xmm1, xmm1		;; Zero words that other cases set
	vmovsd	YMM_TMP6, xmm1
	
	jmp	div_k_done

	;; This case does the divide by k where k is two words
mediumk:
	vmovsd	xmm0, ZPAD6			;; Load zpad word (high bits)
	vmovsd	xmm1, ZPAD5			;; Load zpad word (low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K6	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K6_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K6_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP6, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT6		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD4			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K5	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K5_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K5_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP5, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT5		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD3			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K4	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K4_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K4_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP4, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT4		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD2			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K3	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K3_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K3_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP3, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT3		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD1			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K2	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K2_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K2_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP2, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT2		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD0			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K1_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K1_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP1, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT1		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	ZPAD0, xmm0			;; Save remainder of zpad / k

	jmp	div_k_done

	;; This case does the divide by k where k is one word
	;; Assume ZPAD5 and ZPAD6 are zero.
smallk:	vmovsd	xmm0, ZPAD4			;; Load zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP5, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT4		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD3		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP4, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT3		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD2		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP3, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT2		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD1		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP2, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT1		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD0		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP1, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4
	vmovsd	ZPAD0, xmm0			;; Save remainder

	vxorpd	xmm1, xmm1, xmm1		;; Zero words that other cases set
	vmovsd	YMM_TMP6, xmm1

div_k_done:

	;; Now normalize the data above the halfway point.  Remember that the
	;; two-to-phi multipliers will be applied by the FFT.

ttp	movzx	rax, BYTE PTR [rdi]		;; First word
	vmovsd	xmm0, ZPAD0			;; Load remainder of divide by k
base2	vaddsd	xmm0, xmm0, YMM_BIGVAL
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi+32], xmm0		;; Save value1

	add	rsi, YMM_SRC_INCR1		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+1]		;; Load big vs. little flags
	new_single_rounding ttp, base2, xmm2, xmm0, xmm4, rax*8
	vmovsd	Q [rsi+32], xmm2		;; Save value2

	add	rsi, YMM_SRC_INCR2		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+2]		;; Load big vs. little flags
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi+32], xmm0		;; Save value3

	add	rsi, YMM_SRC_INCR3		;; Next source pointer
base2	vsubsd	xmm2, xmm2, YMM_BIGVAL		;; Remove integer rounding constant
	vmovsd	Q [rsi+32], xmm2		;; Save new value4

	;; Mul the integer part of (ZPAD data divided by k) by -c in
	;; preparation for adding it into the lower FFT data area.
	;; Also add in the shifted high FFT carry at this time.

	;; Now add in and normalize the bottom FFT data.  Remember that the
	;; column two-to-phi multiplier for the first value will be 1.0.  We 
	;; must go 6 words deep in case k is 48-50 bits and c is 32 bits.

	mov	rsi, DESTARG			;; Restore address of FFT data

ttp	movzx	rax, BYTE PTR [rdi]		;; First word
	vmovsd	xmm0, YMM_TMP1			;; Load integer part of divide by k
	vaddsd	xmm0, xmm0, xmm3		;; Add in shifted high FFT carry
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
base2	vaddsd	xmm0, xmm0, YMM_BIGVAL
	vaddsd	xmm0, xmm0, Q [rsi]		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi], xmm0			;; Save value1

	add	rsi, YMM_SRC_INCR1		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+1]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP2			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vaddsd	xmm0, xmm0, xmm2		;; x2 = value + carry
	vaddsd	xmm0, xmm0, Q [rsi]		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi], xmm0			;; Save value2

	add	rsi, YMM_SRC_INCR2		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+2]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP3			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vaddsd	xmm0, xmm0, xmm2		;; x3 = value + carry
	vaddsd	xmm0, xmm0, Q [rsi]		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi], xmm0			;; Save value3

	add	rsi, YMM_SRC_INCR3		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+3]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP4			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vaddsd	xmm0, xmm0, xmm2		;; x4 = value + carry
	vaddsd	xmm0, xmm0, Q [rsi]		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi], xmm0			;; Save value4

	add	rsi, YMM_SRC_INCR4		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+4]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP5			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vaddsd	xmm0, xmm0, xmm2		;; x5 = value + carry
	vaddsd	xmm0, xmm0, Q [rsi]		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi], xmm0			;; Save value5

	add	rsi, YMM_SRC_INCR5		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+5]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP6			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vaddsd	xmm0, xmm0, xmm2		;; x6 = value + carry
	vaddsd	xmm0, xmm0, Q [rsi]		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi], xmm0			;; Save value6

	add	rsi, YMM_SRC_INCR6		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+6]		;; Load big vs. little flags
	vaddsd	xmm0, xmm2, Q [rsi]		;; x7 = value + carry
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi], xmm0			;; Save value7

	add	rsi, YMM_SRC_INCR7		;; Next source pointer
base2	vsubsd	xmm2, xmm2, YMM_BIGVAL		;; Remove rounding constant
	vaddsd	xmm2, xmm2, Q [rsi]		;; Add in FFT data
	vmovsd	Q [rsi], xmm2			;; Save value8
	ENDM


; Process one of the cache lines from the carries table for a two-pass zero-padded FFT.
; rsi = pointer to carries
; rbp = pointer to FFT data
; rdi = pointer to big/little flags
; rdx = pointer two-to-phi group multipliers
; rax,rbx,rcx = destroyed

ynorm012_wpn_zpad MACRO ttp, base2, srcptr, carryptr, biglitptr
	LOCAL	noncon, done
	cmp	const_fft, 0		;; Are we also multiplying by a constant?
	je	noncon			;; Jump if not const
	ynorm012_wpn_zpad_cmn ttp, base2, exec, srcptr, carryptr, biglitptr
	jmp	done
noncon:	ynorm012_wpn_zpad_cmn ttp, base2, noexec, srcptr, carryptr, biglitptr
done:
	ENDM
ynorm012_wpn_zpad_cmn MACRO ttp, base2, const, srcptr, carryptr, biglitptr
	LOCAL	cloop, last_iter, done

	vmovapd	ymm2, [rsi]			;; Load one carry word
	vmovapd	ymm3, [rsi+32]			;; Load other carry word

	mov	srcptr, rbp			;; Save pointers
	mov	carryptr, rsi
ttp	mov	biglitptr, rdi
	mov	al, 1				;; Spreading carry over 8 words

cloop:

ttp	movzx	rcx, WORD PTR [rdi]		;; Load big vs. little flags
ttp	movzx	esi, ch				;; Big/lit flags 1-2
ttp	and	rcx, 0e0h			;; Fudge flags 1 & 2

	vmovapd	ymm0, [rbp]			;; Load values1

	cmp	al, 3*(256/4)			;; Is this the last iteration
	je	last_iter			;; Yes, last iteration requires special handling

ttp	vmulpd	ymm0, ymm0, [rdx+0*YMM_GMD][rcx] ;; mul by fudged grp two-to-minus-phi
	vaddpd	ymm0, ymm0, ymm2		;; x1 = values1 + carry
	split_upper_carry_zpad_word ttp, base2, ymm3, ymm1, ymm2, rsi*8
no const vmulpd	ymm2, ymm3, YMM_K_LO		;; low bits of high FFT carry * k_lo
const	vmulpd	ymm2, ymm3, YMM_K_TIMES_MULCONST_LO ;; low bits of high_FFT_carry * k_lo
	vaddpd	ymm0, ymm0, ymm2		;; x1 = x1 + low bits of high_FFT_carry * k_lo
no const vmulpd ymm3, ymm3, YMM_K_HI		;; low bits of high FFT carry * k_hi
const	vmulpd	ymm3, ymm3, YMM_K_TIMES_MULCONST_HI ;; low bits of high FFT carry * k_hi
ttp	vmulpd	ymm3, ymm3, YMM_LIMIT_INVERSE[rsi*8] ;; shift low bits of high FFT carry * k_hi
no ttp	vmulpd	ymm3, ymm3, YMM_LIMIT_INVERSE[0] ;; shift low bits of high FFT carry * k_hi
	vroundpd ymm3, ymm3, 0			;; WASTEFUL.  round(k_hi * inverse) should be precomputed.
	rounding ttp, base2, noexec, ymm0, ymm2, ymm4, rsi*8
	vaddpd	ymm2, ymm2, ymm3		;; Carry += shifted low bits of high_FFT_carry * k_hi
ttp	vmulpd	ymm0, ymm0, [rdx+0*YMM_GMD+YMM_GMD/2][rcx] ;; mul by fudged grp two-to-phi
	vmovapd	[rbp], ymm0			;; Save FFT data
	vmovapd	ymm3, ymm1			;; Next high FFT carry = high bits of high FFT carry

base2	vmovapd	ymm1, YMM_BIGVAL		;; Load comparison value
no base2 vxorpd	ymm1, ymm1, ymm1		;; Create comparison value
	vcmppd	ymm0, ymm2, ymm1, 0Ch		;; Test for non-zero carries
	vmovmskpd rbx, ymm0			;; Extract 4 comparison bits
base2	vxorpd	ymm1, ymm1, ymm1		;; Create comparison value
	vcmppd	ymm0, ymm3, ymm1, 0Ch		;; Test for non-zero carries
	vmovmskpd rcx, ymm0			;; Extract 4 comparison bits
	or	rbx, rcx			;; Are any bits on?
	jz	short done			;; No, we are done

ttp	bump	rdi, 2				;; Advance pointers
	add	rbp, pass2blkdst		;; Next FFT data ptr
 	add	al, 256/4			;; Special increment needed every 4th iteration
	jnc	cloop				;; Loop if not 4th iteration
	sub	rbp, pass1blkdst		;; Each pass1 block consists of 4 pass 2 blocks
	bump	rbp, 64				;; Next cache line
	dec	al				;; Decrement count of sets of 4 carries we've processed
ttp	cmp	cache_line_multiplier, 4	;; We must increment rdi by a funky amount every 4*clm cache lines
ttp	jne	cloop				;; No, repeat carry propagation loop
ttp	add	rdi, normval2			;; Adjust ptr to little/big flags
	jmp	cloop				;; Repeat carry propagation loop

		;; Do last iteration without propagating carries out
last_iter:
base2	vsubpd	ymm2, ymm2, YMM_BIGVAL		;; Subtract rounding constant from carry
ttp	vmulpd	ymm2, ymm2, [rdx+0*YMM_GMD+YMM_GMD/2][rcx] ;; Carry *= fudged grp two-to-phi
	vaddpd	ymm0, ymm0, ymm2		;; Values1 += carry
	vmovapd	[rbp], ymm0			;; Save new values1

done:	mov	rbp, srcptr			;; Restore pointers
	mov	rsi, carryptr
ttp	mov	rdi, biglitptr

base2	vmovapd	ymm4, YMM_BIGVAL		;; Clear conventional carry
no base2 vxorpd ymm4, ymm4, ymm4
	vmovapd	[rsi], ymm4
base2	vxorpd ymm4, ymm4, ymm4			;; Clear high carry
	vmovapd	[rsi+32], ymm4
	ENDM


; *************** 1D normalized add/sub macro ******************
; This macro adds or subtracts, then "normalizes" eight FFT
; data values.  This involves multiplying the summed values by
; two-to-minus-phi.  Rounding the value to an integer.  Making sure
; the integer is smaller than the maximum allowable integer, generating
; a carry if necessary. Finally, the value is multiplied by two-to-phi
; and stored.
; ymm3 = carry #2
; ymm2 = carry #1
; rcx = pointer to the first number
; rdx = pointer to the second number
; rsi = pointer to destination
; rbp = pointer two-to-phi multipliers
; rdi = pointer to array of big vs. little flags
; rbx = big vs. little word flag #2
; eax = big vs. little word flag #1
; A pipelined version of this code:
;	movzx	rax, BYTE PTR [rdi]	;; Load big vs. little flags
;	xload	ymm0, [rdx+0*dist1]	;; Load second number
;	fop	ymm0, [rcx]		;; Add/sub first number
;	mulpd	ymm0, [rbp+0]		;; Mul values1 by two-to-minus-phi
;	addpd	ymm0, ymm4		;; x = values + carry
;	xload	ymm2, YMM_LIMIT_BIGMAX[rax*2];; Load maximum * BIGVAL - BIGVAL
;	addpd	ymm2, ymm0		;; y = top bits of x
;	xload	ymm6, YMM_LIMIT_BIGMAX_NEG[rax*2];; Load -(maximum*BIGVAL-BIGVAL)
;	addpd	ymm6, ymm2		;; z = y - (maximum * BIGVAL - BIGVAL)
;	subpd	ymm0, ymm6		;; rounded value = x - z
;	mulpd	ymm2, YMM_LIMIT_INVERSE[rax*2];; next carry = shifted y
;	mulpd	ymm0, [rbp+16]		;; new value = val * two-to-phi
;	xstore	[rsi+0*dist1], ymm0	;; Save new value

ynorm_op_1d MACRO fop, ttp, base2
ttp	movzx	rax, BYTE PTR [rdi]	;; Load big vs. little flags
ttp	movzx	rbx, BYTE PTR [rdi+1]
	vmovapd	ymm0, [rdx]		;; Load second number
	fop	ymm0, ymm0, [rcx]	;; Add/sub first number
ttp	vmovapd	ymm5, YMM_NORM012_FF	;; Load FFTLEN/2
ttp	vmulpd	ymm4, ymm5, [rbp]	;; Create fudged two-to-minus-phi
	vmovapd	ymm1, [rdx+32]		;; Load second number
	fop	ymm1, ymm1, [rcx+32]	;; Add/sub first number
ttp	vmulpd	ymm5, ymm5, [rbp+64]	;; Create fudged two-to-minus-phi
ttp	vmulpd	ymm0, ymm0, ymm4	;; Mul value1 by fudged two-to-minus-phi
ttp	vmulpd	ymm1, ymm1, ymm5	;; Mul value2 by fudged two-to-minus-phi
	vaddpd	ymm0, ymm0, ymm2	;; x1 = value1 + carry
	vaddpd	ymm1, ymm1, ymm3	;; x2 = value2 + carry
	rounding_interleaved ttp, base2, noexec, ymm0, ymm2, ymm4, rax*2, ymm1, ymm3, ymm5, rbx*2
ttp	vmulpd	ymm0, ymm0, [rbp+32]	;; new value1 = val * two-to-phi
ttp	vmulpd	ymm1, ymm1, [rbp+96]	;; new value2 = val * two-to-phi
	vmovapd	[rsi], ymm0		;; Save value1
	vmovapd	[rsi+32], ymm1		;; Save value2
ttp	bump	rdi, 2			;; Next flags ptr
	bump	rcx, 64			;; Next src ptr
	bump	rdx, 64			;; Next src ptr
ttp	bump	rbp, 128		;; Next two-to-phi ptr
	bump	rsi, 64			;; Next dest ptr
	ENDM

; The irrational zpad case which uses half the ttp/ttmp data
ynorm_op_1d_zpad MACRO fop, ttp, base2
ttp	movzx	rax, BYTE PTR [rdi]	;; Load big vs. little flags
	vmovapd	ymm0, [rdx]		;; Load second number
	fop	ymm0, ymm0, [rcx]	;; Add/sub first number
ttp	vmovapd	ymm5, YMM_NORM012_FF	;; Load FFTLEN/2
ttp	vmulpd	ymm5, ymm5, [rbp]	;; Create fudged two-to-minus-phi
	vmovapd	ymm1, [rdx+32]		;; Load second number
	fop	ymm1, ymm1, [rcx+32]	;; Add/sub first number
ttp	vmulpd	ymm0, ymm0, ymm5	;; Mul value1 by fudged two-to-minus-phi
ttp	vmulpd	ymm1, ymm1, ymm5	;; Mul value2 by fudged two-to-minus-phi
	vaddpd	ymm0, ymm0, ymm2	;; x1 = value1 + carry
	vaddpd	ymm1, ymm1, ymm3	;; x2 = value2 + carry
	rounding_interleaved ttp, base2, noexec, ymm0, ymm2, ymm4, rax*2, ymm1, ymm3, ymm5, rax*2
ttp	vmulpd	ymm0, ymm0, [rbp+32]	;; new value1 = val * two-to-phi
ttp	vmulpd	ymm1, ymm1, [rbp+32]	;; new value2 = val * two-to-phi
	vmovapd	[rsi], ymm0		;; Save value1
	vmovapd	[rsi+32], ymm1		;; Save value2
ttp	bump	rdi, 1			;; Next flags ptr
	bump	rcx, 64			;; Next src ptr
	bump	rdx, 64			;; Next src ptr
ttp	bump	rbp, 64			;; Next two-to-phi ptr
	bump	rsi, 64			;; Next dest ptr
	ENDM


; This macro finishes the normalize process by adding the final
; carry from the first pass back into the lower two data values.
; These carries (from an add or subtract operation) are always
; very small.  There is no need to do any further carry propagation.
; ymm2,ymm3 = carries
; rax = pointer to the FFT data values
; rbx = pointer two-to-phi multipliers

ynorm_op_1d_mid_cleanup MACRO ttp, base2
	rotate_carries_interleaved base2, ymm2, ymm4, ymm3, ymm5, ymm0, ymm1 ;; Rotate carries
	vmovapd	ymm0, [rax]				;; Load values1
	vmovapd	ymm1, [rax+32]				;; Load values2
base2	vsubpd	ymm2, ymm2, YMM_BIGVAL			;; Remove BIGVAL
base2	vsubpd	ymm3, ymm3, YMM_BIGVAL			;; Remove BIGVAL
ttp	vmulpd	ymm2, ymm2, [rbx+32]			;; carry1 *= two-to-phi
ttp	vmulpd	ymm3, ymm3, [rbx+96]			;; carry2 *= two-to-phi
	vaddpd	ymm0, ymm0, ymm2			;; x1 = values + carry
	vaddpd	ymm1, ymm1, ymm3			;; x2 = values + carry
	vmovapd	[rax], ymm0				;; Save new value1
	vmovapd	[rax+32], ymm1				;; Save new value2
	vmovapd	ymm2, ymm4				;; Install next section's carry
	vmovapd	ymm3, ymm5				;; Install next section's carry
	ENDM

; The zpad case where we have half the ttp/ttmp data
ynorm_op_1d_zpad_mid_cleanup MACRO ttp, base2
	rotate_carries_interleaved base2, ymm2, ymm4, ymm3, ymm5, ymm0, ymm1 ;; Rotate carries
	vmovapd	ymm0, [rax]				;; Load values1
	vmovapd	ymm1, [rax+32]				;; Load values2
base2	vsubpd	ymm2, ymm2, YMM_BIGVAL			;; Remove BIGVAL
base2	vsubpd	ymm3, ymm3, YMM_BIGVAL			;; Remove BIGVAL
ttp	vmulpd	ymm2, ymm2, [rbx+32]			;; carry1 *= two-to-phi
ttp	vmulpd	ymm3, ymm3, [rbx+32]			;; carry2 *= two-to-phi
	vaddpd	ymm0, ymm0, ymm2			;; x1 = values + carry
	vaddpd	ymm1, ymm1, ymm3			;; x2 = values + carry
	vmovapd	[rax], ymm0				;; Save new value1
	vmovapd	[rax+32], ymm1				;; Save new value2
	vmovapd	ymm2, ymm4				;; Install next section's carry
	vmovapd	ymm3, ymm5				;; Install next section's carry
	ENDM

; This macro finishes the add/sub/addsub normalize process by adding
; the final carry from the first pass back into the lower two data values.
; xmm2,xmm3 = carries (will be very, very small)
; rax,rsi,rbp,rdi = trash

ynorm_op_1d_cleanup MACRO ttp, base2, destptr
	mov	rsi, destptr			;; Address of FFT data
	ynorm_top_carry_1d ttp, base2		;; No, do a very standard carry

ttp	mov	rbp, norm_col_mults		;; Address of the ttp/ttmp multipliers
base2	vsubsd	xmm3, xmm3, YMM_BIGVAL
	vmulsd	xmm3, xmm3, YMM_MINUS_C		;; Adjust wrap-around carry
	vaddsd	xmm0, xmm3, Q [rsi]		;; wrap-around carry + values1
	vmovsd	xmm1, Q [rsi+32]		;; Load values2
ttp	vmulsd	xmm1, xmm1, Q [rbp+64]		;; Mul values2 by two-to-minus-phi
ttp	vmulsd	xmm1, xmm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
base2	vsubsd	xmm2, xmm2, YMM_BIGVAL
	vaddsd	xmm1, xmm1, xmm2		;; x2 = values + carry
ttp	vmulsd	xmm1, xmm1, Q [rbp+96]		;; new value2 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save new value1
	vmovsd	Q [rsi+32], xmm1		;; Save new value2
	ENDM

; This macro is similar to ynorm_1d_zpad_cleanup for handling zpad carries in an
; add/sub/addsub operation.
; xmm2 = carry into the top half (carry is tiny)
; rsi = pointer to the FFT data values
; rbp = pointer two-to-phi multipliers
; rdi = big vs. little array pointer

ynorm_op_1d_zpad_cleanup MACRO ttp, base2, destptr
	LOCAL	smallk, mediumk, div_k_done

base2	vsubsd	xmm2, xmm2, YMM_BIGVAL		;; Integerize carry

	;; Copy and integerize data from 7 words above halfway point to ZPAD0-ZPAD6
	;; Clear high words as we go
	;; Then we can make an almost exact copy of the ynorm_1d_zpad_cleanup code

	mov	rsi, destptr			;; Address of FFT data
ttp	mov	rbp, norm_col_mults		;; Address of the multipliers

	vaddsd	xmm0, xmm2, Q [rsi+32]		;; Carry + value1
	vmovsd	ZPAD0, xmm0

	add	rsi, YMM_SRC_INCR1		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR1		;; Next ttp/ttmp pointer
	vmovsd	xmm0, Q [rsi+32]		;; Value2
ttp	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul values2 by two-to-minus-phi
ttp	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
ttp	vroundsd xmm0, xmm0, xmm0, 0		;; Round to an integer
	vmovsd	ZPAD1, xmm0

	add	rsi, YMM_SRC_INCR2		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR2		;; Next ttp/ttmp pointer
	vmovsd	xmm0, Q [rsi+32]		;; Value3
ttp	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul values3 by two-to-minus-phi
ttp	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
ttp	vroundsd xmm0, xmm0, xmm0, 0		;; Round to an integer
	vmovsd	ZPAD2, xmm0

	add	rsi, YMM_SRC_INCR3		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR3		;; Next ttp/ttmp pointer
	vmovsd	xmm0, Q [rsi+32]		;; Value4
ttp	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul values4 by two-to-minus-phi
ttp	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
ttp	vroundsd xmm0, xmm0, xmm0, 0		;; Round to an integer
	vmovsd	ZPAD3, xmm0

	add	rsi, YMM_SRC_INCR4		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR4		;; Next ttp/ttmp pointer
	vmovsd	xmm0, Q [rsi+32]		;; Value5
ttp	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul values5 by two-to-minus-phi
ttp	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
ttp	vroundsd xmm0, xmm0, xmm0, 0		;; Round to an integer
	vmovsd	ZPAD4, xmm0
	vxorpd	xmm1, xmm1, xmm1		;; Clear highest words
	vmovsd	Q [rsi+32], xmm1		;; Clear value5

	vmovsd	ZPAD5, xmm1
	vmovsd	ZPAD6, xmm1

	;; Divide the zpad data by k.  Store the integer part in YMM_TMP
	;; and the remainder in ZPAD0.  Later we will wrap the integer part
	;; down to the bottom of the FFT data area (and multiply by -c).
	;; And we will store the remainder in the upper half of the FFT
	;; data area.

	;; Note there are three cases to handle.  K is smaller than a big word.
	;; K is between one and 2 big words in size.  And K is more than
	;; 2 big words in size.

	cmp	ZPAD_TYPE, 2			;; Are we dealing with case 1,2,or 3
	jl	smallk				;; One word case
	je	mediumk				;; Two word case

	;; This case does the divide by k where k is three words

	vmovsd	xmm0, ZPAD6			;; Load zpad word (high bits)
	vmovsd	xmm1, ZPAD5			;; Load zpad word (middle bits)
	vmovsd	xmm2, ZPAD4			;; Load zpad word (low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K6	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K6_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K6_MID		;; Mul by middle bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K6_LO		;; Mul by low bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP5, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT6		;; Shift previous zpad word
	vaddsd	xmm1, xmm1, xmm0		;; Add to create new high zpad bits
	vmovsd	xmm0, ZPAD3			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm1, ZPAD_SHIFT5		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm2
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K5	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K5_HI		;; Mul by high bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K5_MID		;; Mul by middle bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K5_LO		;; Mul by low bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP4, xmm4			;; Save word of zpad / k

	vmulsd	xmm1, xmm1, ZPAD_SHIFT5		;; Shift previous zpad word
	vaddsd	xmm2, xmm2, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD2			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm2, ZPAD_SHIFT4		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm0
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K4	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K4_HI		;; Mul by high bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K4_MID		;; Mul by middle bits of k
	vsubsd	xmm0,xmm0,xmm5			;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K4_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP3, xmm4			;; Save word of zpad / k

	vmulsd	xmm2, xmm2, ZPAD_SHIFT4		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm2		;; Add to create new high zpad bits
	vmovsd	xmm2, ZPAD1			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm0, ZPAD_SHIFT3		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm1
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K3	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K3_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K3_MID		;; Mul by middle bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K3_LO		;; Mul by low bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP2, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT3		;; Shift previous zpad word
	vaddsd	xmm1, xmm1, xmm0		;; Add to create new high zpad bits
	vmovsd	xmm0, ZPAD0			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm1, ZPAD_SHIFT2		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm2
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K2	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K2_HI		;; Mul by high bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K2_MID		;; Load middle bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K2_LO		;; Load low bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP1, xmm4			;; Save word of zpad / k

	vmulsd	xmm1, xmm1, ZPAD_SHIFT2		;; Shift previous zpad word
	vaddsd	xmm2, xmm2, xmm1		;; Add to create new high zpad bits
	vmulsd	xmm2, xmm2, ZPAD_SHIFT1		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm2		;; Add to create new high zpad bits
	vmovsd	ZPAD0, xmm0			;; Save remainder of zpad / k

	vxorpd	xmm1, xmm1, xmm1		;; Zero words that other cases set
	vmovsd	YMM_TMP6, xmm1
	
	jmp	div_k_done

	;; This case does the divide by k where k is two words
mediumk:
	vmovsd	xmm0, ZPAD6			;; Load zpad word (high bits)
	vmovsd	xmm1, ZPAD5			;; Load zpad word (low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K6	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K6_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K6_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP6, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT6		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD4			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K5	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K5_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K5_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP5, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT5		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD3			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K4	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K4_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K4_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP4, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT4		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD2			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K3	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K3_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K3_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP3, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT3		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD1			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K2	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K2_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K2_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP2, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT2		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD0			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K1_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K1_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP1, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT1		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	ZPAD0, xmm0			;; Save remainder of zpad / k

	jmp	div_k_done

	;; This case does the divide by k where k is one word
	;; Assume ZPAD5 and ZPAD6 are zero.
smallk:	vmovsd	xmm0, ZPAD4			;; Load zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP5, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT4		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD3		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP4, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT3		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD2		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP3, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT2		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD1		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP2, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT1		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD0		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP1, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4
	vmovsd	ZPAD0, xmm0			;; Save remainder

	vxorpd	xmm1, xmm1, xmm1		;; Zero words that other cases set
	vmovsd	YMM_TMP6, xmm1

div_k_done:

	;; Now normalize the data above the halfway point.  Remember that the
	;; two-to-phi multiplier for the first value will be 1.0.

	lea	rdi, YMM_FIRST_BIGLIT_VALUES	;; Address of first big/little flags
	mov	rsi, destptr			;; Address of squared number
ttp	mov	rbp, norm_col_mults		;; Address of the ttmp/ttp multipliers

	movzx	rax, BYTE PTR [rdi]		;; First word
	vmovsd	xmm0, ZPAD0			;; Load remainder of divide by k
base2	vaddsd	xmm0, xmm0, YMM_BIGVAL
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
	vmovsd	Q [rsi+32], xmm0		;; Save value1

ttp	add	rbp, YMM_NORM_INCR1		;; Next ttp/ttmp pointer
	add	rsi, YMM_SRC_INCR1		;; Next source pointer
	movzx	rax, BYTE PTR [rdi+1]		;; Load big vs. little flags
	new_single_rounding ttp, base2, xmm2, xmm0, xmm4, rax*2
ttp	vmulsd	xmm2, xmm2, Q [rbp+32]		;; new value2 = val * two-to-phi
	vmovsd	Q [rsi+32], xmm2		;; Save value2

ttp	add	rbp, YMM_NORM_INCR2		;; Next ttp/ttmp pointer
	add	rsi, YMM_SRC_INCR2		;; Next source pointer
	movzx	rax, BYTE PTR [rdi+2]		;; Load big vs. little flags
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
ttp	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value3 = val * two-to-phi
	vmovsd	Q [rsi+32], xmm0		;; Save value3

ttp	add	rbp, YMM_NORM_INCR3		;; Next ttp/ttmp pointer
	add	rsi, YMM_SRC_INCR3		;; Next source pointer
base2	vsubsd	xmm2, xmm2, YMM_BIGVAL		;; Remove integer rounding constant
ttp	vmulsd	xmm2, xmm2, Q [rbp+32]		;; value4 = carry * two-to-phi
	vmovsd	Q [rsi+32], xmm2		;; Save new value4

	;; Mul the integer part of (ZPAD data divided by k) by -c in
	;; preparation for adding it into the lower FFT data area.
	;; Also add in the shifted high FFT carry at this time.

	;; Now add in and normalize the bottom FFT data.  Remember that the
	;; two-to-phi multiplier for the first value will be 1.0.  We 
	;; must go 6 words deep in case k is 48-50 bits and c is 32 bits.

	mov	rsi, destptr			;; Address of squared number
ttp	mov	rbp, norm_col_mults		;; Address of ttmp/ttp multipliers

	movzx	rax, BYTE PTR [rdi]		;; First word
	vmovsd	xmm0, YMM_TMP1			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
base2	vaddsd	xmm0, xmm0, YMM_BIGVAL
	vaddsd	xmm0, xmm0, Q [rsi]		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
	vmovsd	Q [rsi], xmm0			;; Save value1

	add	rsi, YMM_SRC_INCR1		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR1		;; Next ttp/ttmp pointer
ttp	movzx	rax, BYTE PTR [rdi+1]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP2			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vmovsd	xmm1, Q [rsi]			;; Load FFT data
ttp	vmulsd	xmm1, xmm1, Q [rbp]		;; Mul values2 by two-to-minus-phi
ttp	vmulsd	xmm1, xmm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x2 = value + carry
	vaddsd	xmm0, xmm0, xmm1		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
ttp	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value2 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value2

	add	rsi, YMM_SRC_INCR2		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR2		;; Next ttp/ttmp pointer
ttp	movzx	rax, BYTE PTR [rdi+2]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP3			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vmovsd	xmm1, Q [rsi]			;; Load FFT data
ttp	vmulsd	xmm1, xmm1, Q [rbp]		;; Mul values3 by two-to-minus-phi
ttp	vmulsd	xmm1, xmm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x3 = value + carry
	vaddsd	xmm0, xmm0, xmm1		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
ttp	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value3 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value3

	add	rsi, YMM_SRC_INCR3		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR3		;; Next ttp/ttmp pointer
ttp	movzx	rax, BYTE PTR [rdi+3]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP4			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vmovsd	xmm1, Q [rsi]			;; Load FFT data
ttp	vmulsd	xmm1, xmm1, Q [rbp]		;; Mul values4 by two-to-minus-phi
ttp	vmulsd	xmm1, xmm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x4 = value + carry
	vaddsd	xmm0, xmm0, xmm1		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
ttp	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value4 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value4

	add	rsi, YMM_SRC_INCR4		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR4		;; Next ttp/ttmp pointer
ttp	movzx	rax, BYTE PTR [rdi+4]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP5			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vmovsd	xmm1, Q [rsi]			;; Load FFT data
ttp	vmulsd	xmm1, xmm1, Q [rbp]		;; Mul values4 by two-to-minus-phi
ttp	vmulsd	xmm1, xmm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x5 = value + carry
	vaddsd	xmm0, xmm0, xmm1		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
ttp	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value5 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value5

	add	rsi, YMM_SRC_INCR5		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR5		;; Next ttp/ttmp pointer
ttp	movzx	rax, BYTE PTR [rdi+5]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP6			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vmovsd	xmm1, Q [rsi]			;; Load FFT data
ttp	vmulsd	xmm1, xmm1, Q [rbp]		;; Mul values6 by two-to-minus-phi
ttp	vmulsd	xmm1, xmm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x6 = value + carry
	vaddsd	xmm0, xmm0, xmm1		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
ttp	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value6 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value6

	add	rsi, YMM_SRC_INCR6		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR6		;; Next ttp/ttmp pointer
ttp	movzx	rax, BYTE PTR [rdi+6]		;; Load big vs. little flags
	vmovsd	xmm0, Q [rsi]			;; Load FFT data
ttp	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul values7 by two-to-minus-phi
ttp	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x7 = value + carry
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
ttp	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value7 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value7

	add	rsi, YMM_SRC_INCR7		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR7		;; Next ttp/ttmp pointer
base2	vsubsd	xmm2, xmm2, YMM_BIGVAL		;; Remove rounding constant
ttp	vmulsd	xmm2, xmm2, Q [rbp+32]		;; new value8 = val * two-to-phi
	vaddsd	xmm2, xmm2, Q [rsi]		;; Add in FFT data
	vmovsd	Q [rsi], xmm2			;; Save value8
	ENDM


; *************** 1D normalized add/sub macro ******************
; This macro adds and subtracts, then "normalizes" eight FFT
; data values.  This involves multiplying the sum values by
; two-to-minus-phi.  Adding, subtracting and rounding the value to an
; integer.  Make sure the integer is smaller than the maximum allowable
; integer, generating carries if necessary.  Finally, the values are
; multiplied by two-to-phi and stored.
; ymm7 = sub carry #2
; ymm6 = sub carry #1
; ymm3 = add carry #2
; ymm2 = add carry #1
; rcx = pointer to the first number
; rdx = pointer to the second number
; rsi = pointer to destination #1
; rbp = pointer to destination #2
; rbx = pointer two-to-phi multipliers
; rdi = pointer to array of big vs. little flags
; eax = big vs. little word flag #1

ynorm_addsub_1d MACRO ttp, base2
ttp	movzx	rax, BYTE PTR [rdi]	;; Load big vs. little flags
	vmovapd	ymm0, [rcx]		;; Load first number
	vaddpd	ymm0, ymm0, [rdx]	;; Add second number
ttp	vmovapd	ymm5, [rbx]		;; Load fudged two-to-minus-phi
ttp	vmulpd	ymm5, ymm5, YMM_NORM012_FF ;; Mul by FFTLEN/2
ttp	vmulpd	ymm0, ymm0, ymm5	;; Mul values1 by two-to-minus-phi
	vmovapd	ymm1, [rcx]		;; Load first number
	vsubpd	ymm1, ymm1, [rdx]	;; Sub second number
ttp	vmulpd	ymm1, ymm1, ymm5	;; Mul values2 by two-to-minus-phi
	vaddpd	ymm0, ymm0, ymm2	;; x1 = values + carry
	vaddpd	ymm1, ymm1, ymm6	;; x2 = values + carry
	rounding_interleaved ttp, base2, noexec, ymm0, ymm2, ymm4, rax*2, ymm1, ymm6, ymm5, rax*2
ttp	vmulpd	ymm0, ymm0, [rbx+32]	;; new value1 = val * two-to-phi
ttp	vmulpd	ymm1, ymm1, [rbx+32]	;; new value2 = val * two-to-phi
	vmovapd	[rsi], ymm0		;; Save value1
	vmovapd	[rbp], ymm1		;; Save value2

ttp	movzx	rax, BYTE PTR [rdi+1]	;; Load big vs. little flags
	vmovapd	ymm0, [rcx+32]		;; Load first number
	vaddpd	ymm0, ymm0, [rdx+32]	;; Add second number
ttp	vmovapd	ymm5, [rbx+64]		;; Load fudged two-to-minus-phi
ttp	vmulpd	ymm5, ymm5, YMM_NORM012_FF ;; Mul by FFTLEN/2
ttp	vmulpd	ymm0, ymm0, ymm5	;; Mul values1 by two-to-minus-phi
	vmovapd	ymm1, [rcx+32]		;; Load first number
	vsubpd	ymm1, ymm1, [rdx+32]	;; Sub second number
ttp	vmulpd	ymm1, ymm1, ymm5	;; Mul values2 by two-to-minus-phi
	vaddpd	ymm0, ymm0, ymm3	;; x1 = values + carry
	vaddpd	ymm1, ymm1, ymm7	;; x2 = values + carry
	rounding_interleaved ttp, base2, noexec, ymm0, ymm3, ymm4, rax*2, ymm1, ymm7, ymm5, rax*2
ttp	vmulpd	ymm0, ymm0, [rbx+96]	;; new value1 = val * two-to-phi
ttp	vmulpd	ymm1, ymm1, [rbx+96]	;; new value2 = val * two-to-phi
	vmovapd	[rsi+32], ymm0		;; Save value1
	vmovapd	[rbp+32], ymm1		;; Save value2

ttp	bump	rdi, 2			;; Next flags ptr
	bump	rcx, 64			;; Next src ptr
	bump	rdx, 64			;; Next src ptr
ttp	bump	rbx, 128		;; Next two-to-phi ptr
	bump	rsi, 64			;; Next dest ptr
	bump	rbp, 64			;; Next dest ptr
	ENDM

; The zero pad case which has half the ttp/ttmp data
ynorm_addsub_1d_zpad MACRO ttp, base2
ttp	movzx	rax, BYTE PTR [rdi]	;; Load big vs. little flags
	vmovapd	ymm0, [rcx]		;; Load first number
	vaddpd	ymm0, ymm0, [rdx]	;; Add second number
ttp	vmovapd	ymm5, [rbx]		;; Load fudged two-to-minus-phi
ttp	vmulpd	ymm5, ymm5, YMM_NORM012_FF ;; Mul by FFTLEN/2
ttp	vmulpd	ymm0, ymm0, ymm5	;; Mul values1 by two-to-minus-phi
	vmovapd	ymm1, [rcx]		;; Load first number
	vsubpd	ymm1, ymm1, [rdx]	;; Sub second number
ttp	vmulpd	ymm1, ymm1, ymm5	;; Mul values2 by two-to-minus-phi
	vaddpd	ymm0, ymm0, ymm2	;; x1 = values + carry
	vaddpd	ymm1, ymm1, ymm6	;; x2 = values + carry
	rounding_interleaved ttp, base2, noexec, ymm0, ymm2, ymm4, rax*2, ymm1, ymm6, ymm5, rax*2
ttp	vmulpd	ymm0, ymm0, [rbx+32]	;; new value1 = val * two-to-phi
ttp	vmulpd	ymm1, ymm1, [rbx+32]	;; new value2 = val * two-to-phi
	vmovapd	[rsi], ymm0		;; Save value1
	vmovapd	[rbp], ymm1		;; Save value2

	vmovapd	ymm0, [rcx+32]		;; Load first number
	vaddpd	ymm0, ymm0, [rdx+32]	;; Add second number
ttp	vmovapd	ymm5, [rbx]		;; Load fudged two-to-minus-phi
ttp	vmulpd	ymm5, ymm5, YMM_NORM012_FF ;; Mul by FFTLEN/2
ttp	vmulpd	ymm0, ymm0, ymm5	;; Mul values1 by two-to-minus-phi
	vmovapd	ymm1, [rcx+32]		;; Load first number
	vsubpd	ymm1, ymm1, [rdx+32]	;; Sub second number
ttp	vmulpd	ymm1, ymm1, ymm5	;; Mul values2 by two-to-minus-phi
	vaddpd	ymm0, ymm0, ymm3	;; x1 = values + carry
	vaddpd	ymm1, ymm1, ymm7	;; x2 = values + carry
	rounding_interleaved ttp, base2, noexec, ymm0, ymm3, ymm4, rax*2, ymm1, ymm7, ymm5, rax*2
ttp	vmulpd	ymm0, ymm0, [rbx+32]	;; new value1 = val * two-to-phi
ttp	vmulpd	ymm1, ymm1, [rbx+32]	;; new value2 = val * two-to-phi
	vmovapd	[rsi+32], ymm0		;; Save value1
	vmovapd	[rbp+32], ymm1		;; Save value2

ttp	bump	rdi, 1			;; Next flags ptr
	bump	rcx, 64			;; Next src ptr
	bump	rdx, 64			;; Next src ptr
ttp	bump	rbx, 64			;; Next two-to-phi ptr
	bump	rsi, 64			;; Next dest ptr
	bump	rbp, 64			;; Next dest ptr
	ENDM

; This macro finishes the normalize process by adding the final
; carry from the first pass back into the lower two data values.
; ymm2,ymm3 = carries #1
; ymm6,ymm7 = carries #2
; rax = pointer to the FFT data values #1
; top of stack = pointer to the FFT destination #1 and #2
; rbx = pointer two-to-phi multipliers

ynorm_addsub_1d_mid_cleanup MACRO ttp, base2, dest1, dest2
	mov	rax, dest1				;; Restore dest #1 pointer
	rotate_carries_interleaved base2, ymm2, ymm4, ymm3, ymm5, ymm0, ymm1 ;; Rotate carries
	vmovapd	ymm0, [rax]				;; Load values1
	vmovapd	ymm1, [rax+32]				;; Load values2
base2	vsubpd	ymm2, ymm2, YMM_BIGVAL			;; Remove BIGVAL
base2	vsubpd	ymm3, ymm3, YMM_BIGVAL			;; Remove BIGVAL
ttp	vmulpd	ymm2, ymm2, [rbx+32]			;; carry1 *= two-to-phi
ttp	vmulpd	ymm3, ymm3, [rbx+96]			;; carry2 *= two-to-phi
	vaddpd	ymm0, ymm0, ymm2			;; x1 = values + carry
	vaddpd	ymm1, ymm1, ymm3			;; x2 = values + carry
	vmovapd	[rax], ymm0				;; Save new value1
	vmovapd	[rax+32], ymm1				;; Save new value2
	vmovapd	ymm2, ymm4				;; Install next section's carry
	vmovapd	ymm3, ymm5				;; Install next section's carry

	mov	rax, dest2				;; Get FFT data pointer #2
	rotate_carries_interleaved base2, ymm6, ymm4, ymm7, ymm5, ymm0, ymm1 ;; Rotate carries
	vmovapd	ymm0, [rax]				;; Load values1
	vmovapd	ymm1, [rax+32]				;; Load values2
base2	vsubpd	ymm6, ymm6, YMM_BIGVAL			;; Remove BIGVAL
base2	vsubpd	ymm7, ymm7, YMM_BIGVAL			;; Remove BIGVAL
ttp	vmulpd	ymm6, ymm6, [rbx+32]			;; carry1 *= two-to-phi
ttp	vmulpd	ymm7, ymm7, [rbx+96]			;; carry2 *= two-to-phi
	vaddpd	ymm0, ymm0, ymm6			;; x1 = values + carry
	vaddpd	ymm1, ymm1, ymm7			;; x2 = values + carry
	vmovapd	[rax], ymm0				;; Save new value1
	vmovapd	[rax+32], ymm1				;; Save new value2
	vmovapd	ymm6, ymm4				;; Install next section's carry
	vmovapd	ymm7, ymm5				;; Install next section's carry
	ENDM

; The zpad case where we have half the ttp/ttmp data
ynorm_addsub_1d_zpad_mid_cleanup MACRO ttp, base2, dest1, dest2
	mov	rax, dest1				;; Restore dest #1 pointer
	rotate_carries_interleaved base2, ymm2, ymm4, ymm3, ymm5, ymm0, ymm1 ;; Rotate carries
	vmovapd	ymm0, [rax]				;; Load values1
	vmovapd	ymm1, [rax+32]				;; Load values2
base2	vsubpd	ymm2, ymm2, YMM_BIGVAL			;; Remove BIGVAL
base2	vsubpd	ymm3, ymm3, YMM_BIGVAL			;; Remove BIGVAL
ttp	vmulpd	ymm2, ymm2, [rbx+32]			;; carry1 *= two-to-phi
ttp	vmulpd	ymm3, ymm3, [rbx+32]			;; carry2 *= two-to-phi
	vaddpd	ymm0, ymm0, ymm2			;; x1 = values + carry
	vaddpd	ymm1, ymm1, ymm3			;; x2 = values + carry
	vmovapd	[rax], ymm0				;; Save new value1
	vmovapd	[rax+32], ymm1				;; Save new value2
	vmovapd	ymm2, ymm4				;; Install next section's carry
	vmovapd	ymm3, ymm5				;; Install next section's carry

	mov	rax, dest2				;; Get FFT data pointer #2
	rotate_carries_interleaved base2, ymm6, ymm4, ymm7, ymm5, ymm0, ymm1 ;; Rotate carries
	vmovapd	ymm0, [rax]				;; Load values1
	vmovapd	ymm1, [rax+32]				;; Load values2
base2	vsubpd	ymm6, ymm6, YMM_BIGVAL			;; Remove BIGVAL
base2	vsubpd	ymm7, ymm7, YMM_BIGVAL			;; Remove BIGVAL
ttp	vmulpd	ymm6, ymm6, [rbx+32]			;; carry1 *= two-to-phi
ttp	vmulpd	ymm7, ymm7, [rbx+32]			;; carry2 *= two-to-phi
	vaddpd	ymm0, ymm0, ymm6			;; x1 = values + carry
	vaddpd	ymm1, ymm1, ymm7			;; x2 = values + carry
	vmovapd	[rax], ymm0				;; Save new value1
	vmovapd	[rax+32], ymm1				;; Save new value2
	vmovapd	ymm6, ymm4				;; Install next section's carry
	vmovapd	ymm7, ymm5				;; Install next section's carry
	ENDM

; *************** 1D normalized smalladd macro ******************
; This macro adds a small constant, then propagates the carry
; xmm7 = addin value
; rsi = pointer to destination
; rbp = pointer two-to-phi multipliers
; rdi = pointer to array of big vs. little flags
; rax = big vs. little word flag #1

ynorm_smalladd_1d MACRO base2
	lea	rdi, YMM_FIRST_BIGLIT_VALUES	;; Address of first big/little flags
	mov	rbp, norm_col_mults		;; Address of the multipliers

	movzx	rax, BYTE PTR [rdi]		;; First biglit flag
	vmovsd	xmm0, Q [rsi]			;; Load value1
	vaddsd	xmm0, xmm0, xmm7		;; Add in carry
base2	vaddsd	xmm0, xmm0, YMM_BIGVAL
	single_rounding base2, xmm0, xmm2, xmm4, rax*2
	vmovsd	Q [rsi], xmm0			;; Save value1

	add	rbp, YMM_NORM_INCR1		;; Next ttp/ttmp pointer
	add	rsi, YMM_SRC_INCR1		;; Next source pointer
	movzx	rax, BYTE PTR [rdi+1]		;; Load big vs. little flags
	vmovsd	xmm0, Q [rsi]			;; Load FFT data
	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul values2 by two-to-minus-phi
	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x2 = value + carry
	single_rounding base2, xmm0, xmm2, xmm4, rax*2
	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value2 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value2

	add	rbp, YMM_NORM_INCR2		;; Next ttp/ttmp pointer
	add	rsi, YMM_SRC_INCR2		;; Next source pointer
	movzx	rax, BYTE PTR [rdi+2]		;; Load big vs. little flags
	vmovsd	xmm0, Q [rsi]			;; Load FFT data
	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul values3 by two-to-minus-phi
	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x3 = value + carry
	single_rounding base2, xmm0, xmm2, xmm4, rax*2
	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value3 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value3

	add	rbp, YMM_NORM_INCR3		;; Next ttp/ttmp pointer
	add	rsi, YMM_SRC_INCR3		;; Next source pointer
	movzx	rax, BYTE PTR [rdi+3]		;; Load big vs. little flags
	vmovsd	xmm0, Q [rsi]			;; Load FFT data
	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul values2 by two-to-minus-phi
	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x4 = value + carry
	single_rounding base2, xmm0, xmm2, xmm4, rax*2
	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value4 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value4

	add	rbp, YMM_NORM_INCR4		;; Next ttp/ttmp pointer
	add	rsi, YMM_SRC_INCR4		;; Next source pointer
base2	vsubsd	xmm2, xmm2, YMM_BIGVAL
	vmulsd	xmm2, xmm2, Q [rbp+32]		;; carry *= two-to-phi
	vaddsd	xmm2, xmm2, Q [rsi]		;; Add in FFT data
	vmovsd	Q [rsi], xmm2			;; Save value5
	ENDM

; *************** 1D normalized smallmul macro ******************
; This macro multiplies by a small constant, then "normalizes" eight FFT
; data values.
; ymm7 = small multiplier value
; ymm3 = carry #2
; ymm2 = carry #1
; rsi = pointer to destination
; rbp = pointer two-to-phi multipliers
; rdi = pointer to array of big vs. little flags
; rax = big vs. little word flag #1
; rcx = big vs. little word flag #2

ynorm_smallmul_1d MACRO ttp, base2
ttp	movzx	rax, BYTE PTR [rdi]		;; Load big vs. little flags
ttp	movzx	rcx, BYTE PTR [rdi+1]
	vmovapd	ymm0, [rsi]			;; Load values1
	vmulpd	ymm0, ymm0, ymm7		;; Mul by small value * FFTLEN/2
ttp	vmulpd	ymm0, ymm0, [rbp]		;; Mul values1 by two-to-minus-phi
	vmovapd	ymm1, [rsi+32]			;; Load values2
	vmulpd	ymm1, ymm1, ymm7		;; Mul by small value * FFTLEN/2
ttp	vmulpd	ymm1, ymm1, [rbp+64]		;; Mul values2 by two-to-minus-phi
	vaddpd	ymm0, ymm0, ymm2		;; x1 = values + carry
	vaddpd	ymm1, ymm1, ymm3		;; x2 = values + carry
	rounding_interleaved ttp, base2, noexec, ymm0, ymm2, ymm4, rax*2, ymm1, ymm3, ymm5, rcx*2
ttp	vmulpd	ymm0, ymm0, [rbp+32]		;; new value1 = val * two-to-phi
ttp	vmulpd	ymm1, ymm1, [rbp+96]		;; new value2 = val * two-to-phi
	vmovapd	[rsi], ymm0			;; Save value1
	vmovapd	[rsi+32], ymm1			;; Save value2
ttp	bump	rdi, 2				;; Next flags ptr
	bump	rsi, 64				;; Next dest ptr
ttp	bump	rbp, 128			;; Next two-to-phi ptr
	ENDM

; The zero pad case where we have half the ttp/ttmp data
ynorm_smallmul_1d_zpad MACRO ttp, base2
ttp	movzx	rax, BYTE PTR [rdi]		;; Load big vs. little flags
	vmovapd	ymm0, [rsi]			;; Load values1
	vmulpd	ymm0, ymm0, ymm7		;; Mul by small value * FFTLEN/2
ttp	vmulpd	ymm0, ymm0, [rbp]		;; Mul values1 by two-to-minus-phi
	vmovapd	ymm1, [rsi+32]			;; Load values2
	vmulpd	ymm1, ymm1, ymm7		;; Mul by small value * FFTLEN/2
ttp	vmulpd	ymm1, ymm1, [rbp]		;; Mul values2 by two-to-minus-phi
	vaddpd	ymm0, ymm0, ymm2		;; x1 = values + carry
	vaddpd	ymm1, ymm1, ymm3		;; x2 = values + carry
	rounding_interleaved ttp, base2, noexec, ymm0, ymm2, ymm4, rax*2, ymm1, ymm3, ymm5, rax*2
ttp	vmulpd	ymm0, ymm0, [rbp+32]		;; new value1 = val * two-to-phi
ttp	vmulpd	ymm1, ymm1, [rbp+32]		;; new value2 = val * two-to-phi
	vmovapd	[rsi], ymm0			;; Save value1
	vmovapd	[rsi+32], ymm1			;; Save value2
ttp	bump	rdi, 1				;; Next flags ptr
	bump	rsi, 64				;; Next dest ptr
ttp	bump	rbp, 64				;; Next two-to-phi ptr
	ENDM


; This macro performs the smallmul normalize process by adding the section
; carries back into the start of the section.  Three of the YMM section
; carries are added back in, one of the YMM section carries is applied
; to the next section.
; ymm2,ymm3 = carries
; rax, rdx, rsi, rbp, rdi = trash

ynorm_smallmul_1d_mid_cleanup MACRO ttp, base2, srcptr, biglitptr, ttpptr
	ynorm_1d_mid_cleanup ttp, base2, noexec, srcptr, biglitptr, ttpptr
	ENDM

; This macro performs the smallmul normalize process by adding the section
; carries back into the start of the section.  Three of the YMM section
; carries are added back in, one of the YMM section carries is applied
; to the next section.
; This macro is similar to ynorm_smallmul_1d_mid_cleanup except that
; we only half half as much ttp/ttmp data.
; ymm2,ymm3 = carries
; rax, rdx, rsi, rbp, rdi = trash

ynorm_smallmul_1d_zpad_mid_cleanup MACRO ttp, base2, srcptr, biglitptr, ttpptr
	LOCAL	section_start, section_loop, force_done, done

	rotate_carries_interleaved base2, ymm2, ymm4, ymm3, ymm5, ymm0, ymm1

	mov	DWORD PTR YMM_TMP3, 15		;; Propagate carry at most 15 times (should almost never happen)
section_start:
	mov	rsi, srcptr			;; Load section pointers
ttp	mov	rdi, biglitptr
ttp	mov	rbp, ttpptr
	mov	edx, count1			;; Count of cache lines in section padded group
	vmovapd	YMM_TMP1, ymm4			;; Save carry for next section
	vmovapd	YMM_TMP2, ymm5			;; Save carry for next section

section_loop:
base2	vmovapd	ymm1, YMM_BIGVAL		;; Load comparison value
no base2 vxorpd	ymm1, ymm1, ymm1		;; Create comparison value
	vcmppd	ymm0, ymm2, ymm1, 0Ch		;; Are any carries non-zero
	vmovmskpd rax, ymm0			;; Extract 4 comparison bits
	vcmppd	ymm0, ymm3, ymm1, 0Ch		;; Are any carries non-zero
	vmovmskpd rcx, ymm0			;; Extract 4 comparison bits
	or	rax, rcx			;; Are any bits on?
	jz	done				;; No, we're all done

	sub	DWORD PTR YMM_TMP3, 1		;; There is a bizarre case where adding a zero carry causes a carry (example:
						;; 10^114+1, a data value of 500 becomes -500 with a carry of 1 -- add zero again and
						;; it becomes -500 with a carry of -1).  This can lead to an infinite loop because
						;; the 4 carries never become zero.  After many attempts at getting 4 zero carries,
						;; give up and just add in the carries without propagation.
	jz	force_done

ttp		movzx	rax, BYTE PTR [rdi]		;; Load big vs. little flags
		vmovapd	ymm0, [rsi]			;; Load values1
ttp		vmulpd	ymm0, ymm0, [rbp]		;; Mul values1 by two-to-minus-phi
ttp		vmulpd	ymm0, ymm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
		vmovapd	ymm1, [rsi+32]			;; Load values2
ttp		vmulpd	ymm1, ymm1, [rbp]		;; Mul values2 by two-to-minus-phi
ttp		vmulpd	ymm1, ymm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
		vaddpd	ymm0, ymm0, ymm2		;; x1 = values + carry
		vaddpd	ymm1, ymm1, ymm3		;; x2 = values + carry
		rounding_interleaved ttp, base2, noexec, ymm0, ymm2, ymm4, rax*2, ymm1, ymm3, ymm5, rax*2
ttp		vmulpd	ymm0, ymm0, [rbp+32]		;; new value1 = val * two-to-phi
ttp		vmulpd	ymm1, ymm1, [rbp+32]		;; new value2 = val * two-to-phi
		vmovapd	[rsi], ymm0			;; Save new value1
		vmovapd	[rsi+32], ymm1			;; Save new value2

ttp	bump	rdi, 1				;; Advance pointers
	bump	rsi, 64
ttp	bump	rbp, 64

	sub	rdx, 1				;; Test counter
	jnz	section_loop			;; More cache lines in section, add carry in

	;; Section ended.  Rotate carries again and add the new next section carry values
	;; into the previously calculated next section carry values

	rotate_carries_interleaved base2, ymm2, ymm4, ymm3, ymm5, ymm0, ymm1
base2	vsubpd	ymm4, ymm4, YMM_BIGVAL
base2	vsubpd	ymm5, ymm5, YMM_BIGVAL
	vaddpd	ymm4, ymm4, YMM_TMP1
	vaddpd	ymm5, ymm5, YMM_TMP2
	jmp	section_start

force_done:	
base2	vsubpd	ymm2, ymm2, YMM_BIGVAL		;; Subtract rounding constant from carry
base2	vsubpd	ymm3, ymm3, YMM_BIGVAL
ttp	vmulpd	ymm2, ymm2, [rbp+32]		;; carry1 *= two-to-phi
ttp	vmulpd	ymm3, ymm3, [rbp+32]		;; carry2 *= two-to-phi
	vaddpd	ymm2, ymm2, [rsi]		;; Add in values1
	vaddpd	ymm3, ymm3, [rsi+32]		;; Add in values2
	vmovapd	[rsi], ymm2			;; Save new value1
	vmovapd	[rsi+32], ymm3			;; Save new value2

done:	vmovapd	ymm2, YMM_TMP1			;; Restore carry for next section
	vmovapd	ymm3, YMM_TMP2			;; Restore carry for next section
	ENDM


; This macro finishes the smallmul normalize process by adding
; the final carry from the first pass back into the lower two data values.
; ymm2,ymm3 = carries
; rax,rsi,rbp,rdi = trash

ynorm_smallmul_1d_cleanup MACRO ttp, base2, destptr
	mov	rsi, destptr			;; Address of FFT data
	mov	rbp, norm_col_mults		;; Address of the multipliers
	mov	rdi, norm_biglit_array		;; Addr of the big/little flags array
	ynorm_top_carry_1d ttp, base2		;; No, do a very standard carry
	ynorm_1d_cleanup ttp, base2, noexec
	ENDM

; This macro is similar to ynorm_op_1d_zpad_cleanup for handling zpad carries in an
; smallmul operation.  The difference is the carry can be much larger requiring
; proper rounding.
; xmm2 = carry into the top half
; rsi = pointer to the FFT data values
; rbp = pointer two-to-phi multipliers
; rdi = big vs. little array pointer

ynorm_smallmul_1d_zpad_cleanup MACRO ttp, base2, destptr
	LOCAL	smallk, mediumk, div_k_done

base2	vsubsd	xmm2, xmm2, YMM_BIGVAL		;; Integerize carry

	;; Copy and integerize data from 7 words above halfway point to ZPAD0-ZPAD6
	;; Clear high words as we go
	;; Then we can make an almost exact copy of the ynorm_1d_zpad_cleanup code

	mov	rsi, destptr			;; Address of FFT data
ttp	mov	rbp, norm_col_mults		;; Address of the multipliers
ttp	lea	rdi, YMM_FIRST_BIGLIT_VALUES	;; Address of first big/little flags

ttp	movzx	rax, BYTE PTR [rdi]		;; Load big vs. little flags
	vmovsd	xmm0, Q [rsi+32]		;; Value1
	new_single_split_lower_zpad_word ttp, base2, xmm0, xmm2, xmm4, rax*2
	vmovsd	ZPAD0, xmm0

	add	rsi, YMM_SRC_INCR1		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR1		;; Next ttp/ttmp pointer
ttp	movzx	rax, BYTE PTR [rdi+1]		;; Load big vs. little flags
	vmovsd	xmm0, Q [rsi+32]		;; Value2
ttp	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul values2 by two-to-minus-phi
ttp	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
	new_single_split_lower_zpad_word ttp, base2, xmm0, xmm2, xmm4, rax*2
	vmovsd	ZPAD1, xmm0

	add	rsi, YMM_SRC_INCR2		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR2		;; Next ttp/ttmp pointer
ttp	movzx	rax, BYTE PTR [rdi+2]		;; Load big vs. little flags
	vmovsd	xmm0, Q [rsi+32]		;; Value3
ttp	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul values3 by two-to-minus-phi
ttp	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
	new_single_split_lower_zpad_word ttp, base2, xmm0, xmm2, xmm4, rax*2
	vmovsd	ZPAD2, xmm0

	add	rsi, YMM_SRC_INCR3		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR3		;; Next ttp/ttmp pointer
	vmovsd	xmm0, Q [rsi+32]		;; Value4
ttp	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul values4 by two-to-minus-phi
ttp	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
ttp	vroundsd xmm0, xmm0, xmm0, 0		;; Round to an integer
	vaddsd	xmm0, xmm0, xmm2		;; Value4 + carry
	vmovsd	ZPAD3, xmm0

	add	rsi, YMM_SRC_INCR4		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR4		;; Next ttp/ttmp pointer
	vmovsd	xmm0, Q [rsi+32]		;; Value5
ttp	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul values5 by two-to-minus-phi
ttp	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
ttp	vroundsd xmm0, xmm0, xmm0, 0		;; Round to an integer
	vmovsd	ZPAD4, xmm0
	vxorpd	xmm1, xmm1, xmm1		;; Clear highest words
	vmovsd	Q [rsi+32], xmm1		;; Clear value5

	add	rsi, YMM_SRC_INCR5		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR5		;; Next ttp/ttmp pointer
	vmovsd	xmm0, Q [rsi+32]		;; Value6
ttp	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul values6 by two-to-minus-phi
ttp	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
ttp	vroundsd xmm0, xmm0, xmm0, 0		;; Round to an integer
	vmovsd	ZPAD5, xmm0
	vmovsd	Q [rsi+32], xmm1		;; Clear value6

	add	rsi, YMM_SRC_INCR6		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR6		;; Next ttp/ttmp pointer
	vmovsd	xmm0, Q [rsi+32]		;; Value7
ttp	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul values7 by two-to-minus-phi
ttp	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
ttp	vroundsd xmm0, xmm0, xmm0, 0		;; Round to an integer
	vmovsd	ZPAD6, xmm0
	vmovsd	Q [rsi+32], xmm1		;; Clear value7

	;; Divide the zpad data by k.  Store the integer part in YMM_TMP
	;; and the remainder in ZPAD0.  Later we will wrap the integer part
	;; down to the bottom of the FFT data area (and multiply by -c).
	;; And we will store the remainder in the upper half of the FFT
	;; data area.

	;; Note there are three cases to handle.  K is smaller than a big word.
	;; K is between one and 2 big words in size.  And K is more than
	;; 2 big words in size.

	cmp	ZPAD_TYPE, 2			;; Are we dealing with case 1,2,or 3
	jl	smallk				;; One word case
	je	mediumk				;; Two word case

	;; This case does the divide by k where k is three words

	vmovsd	xmm0, ZPAD6			;; Load zpad word (high bits)
	vmovsd	xmm1, ZPAD5			;; Load zpad word (middle bits)
	vmovsd	xmm2, ZPAD4			;; Load zpad word (low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K6	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K6_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K6_MID		;; Mul by middle bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K6_LO		;; Mul by low bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP5, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT6		;; Shift previous zpad word
	vaddsd	xmm1, xmm1, xmm0		;; Add to create new high zpad bits
	vmovsd	xmm0, ZPAD3			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm1, ZPAD_SHIFT5		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm2
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K5	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K5_HI		;; Mul by high bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K5_MID		;; Mul by middle bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K5_LO		;; Mul by low bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP4, xmm4			;; Save word of zpad / k

	vmulsd	xmm1, xmm1, ZPAD_SHIFT5		;; Shift previous zpad word
	vaddsd	xmm2, xmm2, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD2			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm2, ZPAD_SHIFT4		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm0
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K4	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K4_HI		;; Mul by high bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K4_MID		;; Mul by middle bits of k
	vsubsd	xmm0,xmm0,xmm5			;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K4_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP3, xmm4			;; Save word of zpad / k

	vmulsd	xmm2, xmm2, ZPAD_SHIFT4		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm2		;; Add to create new high zpad bits
	vmovsd	xmm2, ZPAD1			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm0, ZPAD_SHIFT3		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm1
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K3	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K3_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K3_MID		;; Mul by middle bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K3_LO		;; Mul by low bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP2, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT3		;; Shift previous zpad word
	vaddsd	xmm1, xmm1, xmm0		;; Add to create new high zpad bits
	vmovsd	xmm0, ZPAD0			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm1, ZPAD_SHIFT2		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm2
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K2	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K2_HI		;; Mul by high bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K2_MID		;; Load middle bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K2_LO		;; Load low bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP1, xmm4			;; Save word of zpad / k

	vmulsd	xmm1, xmm1, ZPAD_SHIFT2		;; Shift previous zpad word
	vaddsd	xmm2, xmm2, xmm1		;; Add to create new high zpad bits
	vmulsd	xmm2, xmm2, ZPAD_SHIFT1		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm2		;; Add to create new high zpad bits
	vmovsd	ZPAD0, xmm0			;; Save remainder of zpad / k

	vxorpd	xmm1, xmm1, xmm1		;; Zero words that other cases set
	vmovsd	YMM_TMP6, xmm1
	
	jmp	div_k_done

	;; This case does the divide by k where k is two words
mediumk:
	vmovsd	xmm0, ZPAD6			;; Load zpad word (high bits)
	vmovsd	xmm1, ZPAD5			;; Load zpad word (low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K6	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K6_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K6_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP6, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT6		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD4			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K5	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K5_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K5_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP5, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT5		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD3			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K4	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K4_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K4_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP4, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT4		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD2			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K3	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K3_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K3_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP3, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT3		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD1			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K2	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K2_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K2_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP2, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT2		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD0			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K1_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K1_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP1, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT1		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	ZPAD0, xmm0			;; Save remainder of zpad / k

	jmp	div_k_done

	;; This case does the divide by k where k is one word
	;; Assume ZPAD5 and ZPAD6 are zero.
smallk:	vmovsd	xmm0, ZPAD4			;; Load zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP5, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT4		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD3		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP4, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT3		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD2		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP3, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT2		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD1		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP2, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT1		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD0		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP1, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4
	vmovsd	ZPAD0, xmm0			;; Save remainder

	vxorpd	xmm1, xmm1, xmm1		;; Zero words that other cases set
	vmovsd	YMM_TMP6, xmm1

div_k_done:

	;; Now normalize the data above the halfway point.  Remember that the
	;; two-to-phi multiplier for the first value will be 1.0.

ttp	lea	rdi, YMM_FIRST_BIGLIT_VALUES	;; Address of first big/little flags
	mov	rsi, destptr			;; Address of squared number
ttp	mov	rbp, norm_col_mults		;; Address of the ttmp/ttp multipliers

ttp	movzx	rax, BYTE PTR [rdi]		;; First word
	vmovsd	xmm0, ZPAD0			;; Load remainder of divide by k
base2	vaddsd	xmm0, xmm0, YMM_BIGVAL
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
	vmovsd	Q [rsi+32], xmm0		;; Save value1

ttp	add	rbp, YMM_NORM_INCR1		;; Next ttp/ttmp pointer
	add	rsi, YMM_SRC_INCR1		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+1]		;; Load big vs. little flags
	new_single_rounding ttp, base2, xmm2, xmm0, xmm4, rax*2
ttp	vmulsd	xmm2, xmm2, Q [rbp+32]		;; new value2 = val * two-to-phi
	vmovsd	Q [rsi+32], xmm2		;; Save value2

ttp	add	rbp, YMM_NORM_INCR2		;; Next ttp/ttmp pointer
	add	rsi, YMM_SRC_INCR2		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+2]		;; Load big vs. little flags
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
ttp	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value3 = val * two-to-phi
	vmovsd	Q [rsi+32], xmm0		;; Save value3

ttp	add	rbp, YMM_NORM_INCR3		;; Next ttp/ttmp pointer
	add	rsi, YMM_SRC_INCR3		;; Next source pointer
base2	vsubsd	xmm2, xmm2, YMM_BIGVAL		;; Remove integer rounding constant
ttp	vmulsd	xmm2, xmm2, Q [rbp+32]		;; value4 = carry * two-to-phi
	vmovsd	Q [rsi+32], xmm2		;; Save new value4

	;; Mul the integer part of (ZPAD data divided by k) by -c in
	;; preparation for adding it into the lower FFT data area.
	;; Also add in the shifted high FFT carry at this time.

	;; Now add in and normalize the bottom FFT data.  Remember that the
	;; two-to-phi multiplier for the first value will be 1.0.  We 
	;; must go 6 words deep in case k is 48-50 bits and c is 32 bits.

	mov	rsi, destptr			;; Address of squared number
ttp	mov	rbp, norm_col_mults		;; Address of ttmp/ttp multipliers

ttp	movzx	rax, BYTE PTR [rdi]		;; First word
	vmovsd	xmm0, YMM_TMP1			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
base2	vaddsd	xmm0, xmm0, YMM_BIGVAL
	vaddsd	xmm0, xmm0, Q [rsi]		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
	vmovsd	Q [rsi], xmm0			;; Save value1

	add	rsi, YMM_SRC_INCR1		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR1		;; Next ttp/ttmp pointer
ttp	movzx	rax, BYTE PTR [rdi+1]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP2			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vmovsd	xmm1, Q [rsi]			;; Load FFT data
ttp	vmulsd	xmm1, xmm1, Q [rbp]		;; Mul values2 by two-to-minus-phi
ttp	vmulsd	xmm1, xmm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x2 = value + carry
	vaddsd	xmm0, xmm0, xmm1		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
ttp	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value2 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value2

	add	rsi, YMM_SRC_INCR2		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR2		;; Next ttp/ttmp pointer
ttp	movzx	rax, BYTE PTR [rdi+2]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP3			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vmovsd	xmm1, Q [rsi]			;; Load FFT data
ttp	vmulsd	xmm1, xmm1, Q [rbp]		;; Mul values3 by two-to-minus-phi
ttp	vmulsd	xmm1, xmm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x3 = value + carry
	vaddsd	xmm0, xmm0, xmm1		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
ttp	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value3 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value3

	add	rsi, YMM_SRC_INCR3		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR3		;; Next ttp/ttmp pointer
ttp	movzx	rax, BYTE PTR [rdi+3]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP4			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vmovsd	xmm1, Q [rsi]			;; Load FFT data
ttp	vmulsd	xmm1, xmm1, Q [rbp]		;; Mul values4 by two-to-minus-phi
ttp	vmulsd	xmm1, xmm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x4 = value + carry
	vaddsd	xmm0, xmm0, xmm1		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
ttp	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value4 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value4

	add	rsi, YMM_SRC_INCR4		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR4		;; Next ttp/ttmp pointer
ttp	movzx	rax, BYTE PTR [rdi+4]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP5			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vmovsd	xmm1, Q [rsi]			;; Load FFT data
ttp	vmulsd	xmm1, xmm1, Q [rbp]		;; Mul values4 by two-to-minus-phi
ttp	vmulsd	xmm1, xmm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x5 = value + carry
	vaddsd	xmm0, xmm0, xmm1		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
ttp	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value5 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value5

	add	rsi, YMM_SRC_INCR5		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR5		;; Next ttp/ttmp pointer
ttp	movzx	rax, BYTE PTR [rdi+5]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP6			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vmovsd	xmm1, Q [rsi]			;; Load FFT data
ttp	vmulsd	xmm1, xmm1, Q [rbp]		;; Mul values6 by two-to-minus-phi
ttp	vmulsd	xmm1, xmm1, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x6 = value + carry
	vaddsd	xmm0, xmm0, xmm1		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
ttp	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value6 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value6

	add	rsi, YMM_SRC_INCR6		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR6		;; Next ttp/ttmp pointer
ttp	movzx	rax, BYTE PTR [rdi+6]		;; Load big vs. little flags
	vmovsd	xmm0, Q [rsi]			;; Load FFT data
ttp	vmulsd	xmm0, xmm0, Q [rbp]		;; Mul values7 by two-to-minus-phi
ttp	vmulsd	xmm0, xmm0, YMM_NORM012_FF	;; Mul by FFTLEN/2
	vaddsd	xmm0, xmm0, xmm2		;; x7 = value + carry
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*2
ttp	vmulsd	xmm0, xmm0, Q [rbp+32]		;; new value7 = val * two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save value7

	add	rsi, YMM_SRC_INCR7		;; Next source pointer
ttp	add	rbp, YMM_NORM_INCR7		;; Next ttp/ttmp pointer
base2	vsubsd	xmm2, xmm2, YMM_BIGVAL		;; Remove rounding constant
ttp	vmulsd	xmm2, xmm2, Q [rbp+32]		;; new value8 = val * two-to-phi
	vaddsd	xmm2, xmm2, Q [rsi]		;; Add in FFT data
	vmovsd	Q [rsi], xmm2			;; Save value8
	ENDM


; *************** WPN normalized add/sub macro ******************
; This macro adds or subtracts, then "normalizes" eight FFT
; data values.  This involves multiplying the summed values by
; two-to-minus-phi.  Rounding the value to an integer.  Making sure
; the integer is smaller than the maximum allowable integer, generating
; a carry if necessary. Finally, the value is multiplied by two-to-phi
; and stored.
; rsi = pointer to the first number
; rdx = pointer to the second number
; rdi = pointer to array of big vs. little flags
; rbp = pointer two-to-phi group multipliers
; ymm2,ymm3 = carries
; dist_to_dest = distance from first number to destination
; rax,rbx,rcx destroyed if ttp
; NOTE: caller can assume xmm6,xmm7 are left untouched (for better prolog/epilog handling)
; A pipelined version of this code:
;	xload	ymm0, [rdx]		;; Load second number
;	fop	ymm0, [rsi]		;; Add/sub first number
;	movzx	rax, BYTE PTR [rdi]	;; Load big vs. little flags
;	mulpd	ymm0, [rbp+0*YMM_GMD][rax*2] ;; Mul by fudged grp two-to-minus-phi
;	addpd	ymm0, YMM_TMP1		;; x1 = values + carry
;	xload	ymm2, YMM_LIMIT_BIGMAX[rax*2];; Load maximum * BIGVAL - BIGVAL
;	addpd	ymm2, ymm0		;; y1 = top bits of x
;	xload	ymm6, YMM_LIMIT_BIGMAX_NEG[rax*2];; Load -(maximum*BIGVAL-BIGVAL)
;	addpd	ymm6, ymm2		;; z1 = y1-(maximum * BIGVAL - BIGVAL)
;	subpd	ymm0, ymm6		;; rounded value = x1 - z1
;	mulpd	ymm2, YMM_LIMIT_INVERSE[rax*2];; next carry = shifted y1
;	mulpd	ymm0, [rbp+0*YMM_GMD+YMM_GMD/2][rax*2] ;; new value1 = val * fudged grp two-to-phi
;	xstore	[rsi+dist_to_dest+0*dist1], ymm0 ;; Save new value1
;	xstore	YMM_TMP1, ymm2		;; Save carry

ynorm_op_wpn MACRO fop, ttp, base2, dist_to_dest
		vmovapd	ymm0, [rdx]			;; Load second number
		fop	ymm0, ymm0, [rsi]		;; Add/sub first number
		vmovapd	ymm1, [rdx+32]			;; Load second number
		fop	ymm1, ymm1, [rsi+32]		;; Add/sub first number

ttp		movzx	rbx, WORD PTR [rdi]		;; Load big vs. little & fudge flags
ttp		movzx	ecx, bl				;; Fudge flags 1
ttp		and	rcx, 0e0h
ttp		movzx	eax, bh				;; Big/lit flags 1-2
ttp		and	rbx, 01ch			;; Fudge flags 2
		add	rsi, dist_to_dest		;; Change to a destination pointer
ttp		vmulpd	ymm0, ymm0, [rbp+0*YMM_GMD][rcx] ;; Mul by fudged grp two-to-minus-phi
ttp		vmulpd	ymm1, ymm1, [rbp+1*YMM_GMD][rbx*8] ;; Mul by fudged grp two-to-minus-phi
		vaddpd	ymm0, ymm0, ymm2		;; x1 = values + carry
		vaddpd	ymm1, ymm1, ymm3		;; x2 = values + carry
		rounding_interleaved ttp, base2, noexec, ymm0, ymm2, ymm4, rax*8, ymm1, ymm3, ymm5, rax*8+32
ttp		vmulpd	ymm0, ymm0, [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; value1 *= fudged grp two-to-phi
ttp		vmulpd	ymm1, ymm1, [rbp+1*YMM_GMD+YMM_GMD/2][rbx*8] ;; value2 *= fudged grp two-to-phi
		vmovapd	[rsi+0*32], ymm0		;; Save new value1
		vmovapd	[rsi+1*32], ymm1		;; Save new value2
		sub	rsi, dist_to_dest		;; Change back to a pointer into the first input number
	ENDM

; Zero-padded version of ynorm_op_wpn
; rsi = pointer to the first number
; rdx = pointer to the second number
; rsi+rbx = pointer to the destination
; rdi = pointer to array of big vs. little flags
; rbp = pointer two-to-phi group multipliers
; ymm2,ymm3 = carries
; rax,rcx destroyed if ttp
; NOTE: caller can assume xmm6,xmm7 are left untouched (for better prolog/epilog handling)
ynorm_op_wpn_zpad MACRO fop, ttp, base2
		vmovapd	ymm0, [rdx]			;; Load second number
		fop	ymm0, ymm0, [rsi]		;; Add/sub first number
		vmovapd	ymm1, [rdx+32]			;; Load second number
		fop	ymm1, ymm1, [rsi+32]		;; Add/sub first number

ttp		movzx	rcx, WORD PTR [rdi]		;; Load big vs. little & fudge flags
ttp		movzx	eax, ch				;; Big/lit flags 1-2
ttp		and	rcx, 0e0h			;; Fudge flags 1 & 2
ttp		vmulpd	ymm0, ymm0, [rbp+0*YMM_GMD][rcx] ;; Mul by fudged grp two-to-minus-phi
ttp		vmulpd	ymm1, ymm1, [rbp+0*YMM_GMD][rcx] ;; Mul by fudged grp two-to-minus-phi
		vaddpd	ymm0, ymm0, ymm2		;; x1 = values + carry
		vaddpd	ymm1, ymm1, ymm3		;; x2 = values + carry
		rounding_interleaved ttp, base2, noexec, ymm0, ymm2, ymm4, rax*8, ymm1, ymm3, ymm5, rax*8
ttp		vmulpd	ymm0, ymm0, [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; value1 *= fudged grp two-to-phi
ttp		vmulpd	ymm1, ymm1, [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; value2 *= fudged grp two-to-phi
		vmovapd	[rsi+rbx+0*32], ymm0		;; Save new value1
		vmovapd	[rsi+rbx+1*32], ymm1		;; Save new value2
	ENDM

; *************** WPN followup macros ******************
; This macro finishes the normalize add/sub process by adding three carry values
; from the two carry registers from the end of a pass 1 block back to the start
; of the pass 1 block.  The remaining two carry values are rotated for starting the next block.
; rsi = pointer to the FFT data values
; rbp = pointer two-to-phi multipliers
; rdi = pointer to biglit flags
; ymm2,ymm3 or ymm6,ymm7 = carries

ynorm_op_wpn_blk MACRO ttp, base2, carry1, carry2
	rotate_carries_interleaved base2, carry1, ymm4, carry2, ymm5, ymm0, ymm1 ;; Rotate carries
base2	vsubpd	carry1, carry1, YMM_BIGVAL		;; Remove BIGVAL
base2	vsubpd	carry2, carry2, YMM_BIGVAL		;; Remove BIGVAL
ttp	movzx	rbx, WORD PTR [rdi]			;; Load big vs. little & fudge flags
ttp	movzx	ecx, bl					;; Fudge flags 1
ttp	and	rcx, 0e0h
ttp	and	rbx, 01ch				;; Fudge flags 2
ttp	vmulpd	carry1, carry1, [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; carry1 *= fudged grp two-to-phi
ttp	vmulpd	carry2, carry2, [rbp+1*YMM_GMD+YMM_GMD/2][rbx*8] ;; carry2 *= fudged grp two-to-phi
	vaddpd	ymm0, carry1, [rsi]			;; x1 = carry1 + values1
	vaddpd	ymm1, carry2, [rsi+32]			;; x2 = carry2 + values2
	vmovapd	[rsi], ymm0				;; Save new value1
	vmovapd	[rsi+32], ymm1				;; Save new value2
	vmovapd	carry1, ymm4				;; Set next block's carry
	vmovapd	carry2, ymm5				;; Set next block's carry
	ENDM

; Zero-padded FFT version.  There cannot be any carry out of the top word.
ynorm_op_wpn_zpad_blk MACRO ttp, base2, carry1, carry2
	rotate_carries base2, carry1, ymm4, ymm0, ymm1	;; Rotate carries
base2	vsubpd	carry1, carry1, YMM_BIGVAL		;; Remove BIGVAL
ttp	movzx	rcx, WORD PTR [rdi]			;; Load big vs. little & fudge flags
ttp	and	rcx, 0e0h				;; Fudge flags 1 & 2
ttp	vmulpd	carry1, carry1, [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; carry1 *= fudged grp two-to-phi
	vaddpd	ymm0, carry1, [rsi]			;; x1 = carry1 + values1
	vmovapd	[rsi], ymm0				;; Save new value1
	vmovapd	carry1, ymm4				;; Set next block's carry
	ENDM

; This macro finishes the normalize add/sub process by adding the last two carries
; from the end back to the start of the result.
; rsi = pointer to FFT data
; rbp = pointer two-to-phi group multipliers
; rdi = biglit flags ptr
; NOTE: Assumes carry can be propagated over 4 FFT words.  Should not be a problem
; as an add/sub carry will be just 2 or 3 bits at most and multiplying by -c should
; keep the carry at about 25 bits max.
; Also note: The group multiplier will be 1.0 for the first 4 FFT words.

ynorm_op_wpn_final MACRO ttp, base2, carry1, carry2
	LOCAL	cloop, done

	ynorm_top_carry_cmn base2, rsi, carry2, 2

base2	vsubsd	carry2, carry2, YMM_BIGVAL	;; Remove BIGVAL
	vmulsd	carry2, carry2, YMM_MINUS_C	;; mul wrap around carry by -c
base2	vaddsd	carry2, carry2, YMM_BIGVAL	;; Restore BIGVAL

	sub	rdx, rdx			;; Init loop counter

cloop:
ttp	movzx	rbx, WORD PTR [rdi]		;; Load big vs. little & fudge flags
ttp	movzx	eax, bh				;; Big/lit flags 1-2
ttp	and	rbx, 01ch			;; Fudge flags 2

	vaddsd	xmm0, carry2, Q [rsi]		;; x1 = values1 + carry2
no ttp	vaddsd	xmm1, carry1, Q [rsi+32]	;; x2 = values2 + carry1
ttp	vmovsd	xmm1, Q [rsi+32]		;; values2
ttp	vmulsd	xmm1, xmm1, Q [rbp+1*YMM_GMD][rbx*8] ;; values2 *= fudged grp two-to-minus-phi
ttp	vaddsd	xmm1, carry1, xmm1		;; x2 = values2 + carry1
	new_single_rounding_interleaved ttp, base2, xmm0, carry2, xmm4, rax*8, xmm1, carry1, xmm5, rax*8+32
ttp	vmulsd	xmm1, xmm1, Q [rbp+1*YMM_GMD+YMM_GMD/2][rbx*8] ;; value2 *= fudged grp two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save new value1
	vmovsd	Q [rsi+32], xmm1		;; Save new value2

base2	vmovsd	xmm1, YMM_BIGVAL		;; Load comparison value
no base2 vxorpd	xmm1, xmm1, xmm1		;; Create comparison value
	vcmpsd	xmm0, carry2, xmm1, 0Ch		;; Test for non-zero carry
	vmovmskpd rax, xmm0			;; Extract comparison bit
	vcmpsd	xmm0, carry1, xmm1, 0Ch		;; Test for non-zero carry
	vmovmskpd rcx, xmm0			;; Extract comparison bit
	or	rax, rcx			;; Is either bit on?
	jz	short done			;; No, we are done

ttp	bump	rdi, 2				;; Advance pointers
	add	rsi, pass2blkdst		;; Next FFT data ptr
 	add	dl, 256/4			;; Special increment needed every 4th iteration
	jnc	cloop				;; Loop if not 4th iteration
	sub	rsi, pass1blkdst		;; Each pass1 block consists of 4 pass 2 blocks
	bump	rsi, 64				;; Next cache line
	jmp	cloop				;; No, repeat carry propagation loop

done:	
	ENDM

; The zero-padded FFT version.  There cannot be a wrap-around carry.
; The last carry from an add or subtract will be 2-3 bits at most, so the
; carry can be safely added into one FFT data word.  Also the group
; multiplier for the first cache line is 1.0.
ynorm_op_wpn_zpad_final MACRO ttp, base2, carry1, carry2
base2	vsubsd	carry1, carry1, YMM_BIGVAL		;; Remove BIGVAL
	vaddsd	xmm0, carry1, Q [rsi+32]		;; x1 = carry1 + values1
	vmovsd	Q [rsi+32], xmm0			;; Save new value1
	ENDM


; *************** WPN normalized add & sub macro ******************
; This macro adds and subtracts, then "normalizes" eight FFT data values.
; This involves multiplying the summed values by two-to-minus-phi, rounding
; the value to an integer while making sure the integer is smaller than the
; maximum allowable integer, generating a carry if necessary.
; Finally, the value is multiplied by two-to-phi.
; and stored.
; rsi = pointer to the first number
; rdx = pointer to the second number
; rdi = pointer to array of big vs. little flags
; rbp = pointer two-to-phi group multipliers
; ymm2,ymm3 = add carries
; ymm6,ymm7 = sub carries
; dist_to_dest1 = distance from first number to destination1
; dist_to_dest2 = distance from first number to destination2
; rax,rbx,rcx destroyed if ttp

ynorm_addsub_wpn MACRO ttp, base2, dist_to_dest1, dist_to_dest2
ttp		movzx	rbx, WORD PTR [rdi]		;; Load big vs. little & fudge flags
ttp		movzx	ecx, bl				;; Fudge flags 1
ttp		and	rcx, 0e0h
ttp		movzx	eax, bh				;; Big/lit flags 1-2
ttp		and	rbx, 01ch			;; Fudge flags 2

		vmovapd	ymm0, [rsi+0*32]		;; Load first number
		vaddpd	ymm0, ymm0, [rdx+0*32]		;; first + second number
		vmovapd	ymm1, [rsi+1*32]		;; Load first number
		vaddpd	ymm1, ymm1, [rdx+1*32]		;; first + second number
ttp		vmulpd	ymm0, ymm0, [rbp+0*YMM_GMD][rcx] ;; Mul by fudged grp two-to-minus-phi
ttp		vmulpd	ymm1, ymm1, [rbp+1*YMM_GMD][rbx*8] ;; Mul by fudged grp two-to-minus-phi
		vaddpd	ymm0, ymm0, ymm2		;; x1 = values + carry
		vaddpd	ymm1, ymm1, ymm3		;; x2 = values + carry
		rounding_interleaved ttp, base2, noexec, ymm0, ymm2, ymm4, rax*8, ymm1, ymm3, ymm5, rax*8+32
ttp		vmulpd	ymm0, ymm0, [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; value1 *= fudged grp two-to-phi
ttp		vmulpd	ymm1, ymm1, [rbp+1*YMM_GMD+YMM_GMD/2][rbx*8] ;; value2 *= fudged grp two-to-phi

		vmovapd	ymm4, [rsi+0*32]		;; Load first number
		vsubpd	ymm4, ymm4, [rdx+0*32]		;; first - second number
		vmovapd	ymm5, [rsi+1*32]		;; Load first number
		vsubpd	ymm5, ymm5, [rdx+1*32]		;; first - second number

		add	rsi, dist_to_dest1		;; Change to a destination pointer
		vmovapd	[rsi+0*32], ymm0		;; Save new value1
		vmovapd	[rsi+1*32], ymm1		;; Save new value1
		sub	rsi, dist_to_dest1		;; Change back to a pointer into the first input number

ttp		vmulpd	ymm4, ymm4, [rbp+0*YMM_GMD][rcx] ;; Mul by fudged grp two-to-minus-phi
ttp		vmulpd	ymm5, ymm5, [rbp+1*YMM_GMD][rbx*8] ;; Mul by fudged grp two-to-minus-phi
		vaddpd	ymm4, ymm4, ymm6		;; x1 = values + carry
		vaddpd	ymm5, ymm5, ymm7		;; x2 = values + carry
		rounding_interleaved ttp, base2, noexec, ymm4, ymm6, ymm0, rax*8, ymm5, ymm7, ymm1, rax*8+32
ttp		vmulpd	ymm4, ymm4, [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; value1 *= fudged grp two-to-phi
ttp		vmulpd	ymm5, ymm5, [rbp+1*YMM_GMD+YMM_GMD/2][rbx*8] ;; value2 *= fudged grp two-to-phi
		add	rsi, dist_to_dest2		;; Change to a destination pointer
		vmovapd	[rsi+0*32], ymm4		;; Save new value1
		vmovapd	[rsi+1*32], ymm5		;; Save new value1
		sub	rsi, dist_to_dest2		;; Change back to a pointer into the first input number
	ENDM

; The zero-padded FFT version
; rsi = pointer to the first number
; rdx = pointer to the second number
; rsi+rbx = pointer to destination1
; rdi = pointer to array of big vs. little flags
; rbp = pointer two-to-phi group multipliers
; ymm2,ymm3 = add carries
; ymm6,ymm7 = sub carries
; dist_to_dest2 = distance from first number to destination2
; rax,rcx destroyed if ttp

ynorm_addsub_wpn_zpad MACRO ttp, base2, dist_to_dest2
ttp		movzx	rcx, WORD PTR [rdi]		;; Load big vs. little & fudge flags
ttp		movzx	eax, ch				;; Big/lit flags 1-2
ttp		and	rcx, 0e0h			;; Fudge flags 1 & 2

		vmovapd	ymm0, [rsi+0*32]		;; Load first number
		vaddpd	ymm0, ymm0, [rdx+0*32]		;; first + second number
		vmovapd	ymm1, [rsi+1*32]		;; Load first number
		vaddpd	ymm1, ymm1, [rdx+1*32]		;; first + second number
ttp		vmulpd	ymm0, ymm0, [rbp+0*YMM_GMD][rcx] ;; Mul by fudged grp two-to-minus-phi
ttp		vmulpd	ymm1, ymm1, [rbp+0*YMM_GMD][rcx] ;; Mul by fudged grp two-to-minus-phi
		vaddpd	ymm0, ymm0, ymm2		;; x1 = values + carry
		vaddpd	ymm1, ymm1, ymm3		;; x2 = values + carry
		rounding_interleaved ttp, base2, noexec, ymm0, ymm2, ymm4, rax*8, ymm1, ymm3, ymm5, rax*8
ttp		vmulpd	ymm0, ymm0, [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; value1 *= fudged grp two-to-phi
ttp		vmulpd	ymm1, ymm1, [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; value2 *= fudged grp two-to-phi

		vmovapd	ymm4, [rsi+0*32]		;; Load first number
		vsubpd	ymm4, ymm4, [rdx+0*32]		;; first - second number
		vmovapd	ymm5, [rsi+1*32]		;; Load first number
		vsubpd	ymm5, ymm5, [rdx+1*32]		;; first - second number

		vmovapd	[rsi+rbx+0*32], ymm0		;; Save new value1
		vmovapd	[rsi+rbx+1*32], ymm1		;; Save new value1

ttp		vmulpd	ymm4, ymm4, [rbp+0*YMM_GMD][rcx] ;; Mul by fudged grp two-to-minus-phi
ttp		vmulpd	ymm5, ymm5, [rbp+0*YMM_GMD][rcx] ;; Mul by fudged grp two-to-minus-phi
		vaddpd	ymm4, ymm4, ymm6		;; x1 = values + carry
		vaddpd	ymm5, ymm5, ymm7		;; x2 = values + carry
		rounding_interleaved ttp, base2, noexec, ymm4, ymm6, ymm0, rax*8, ymm5, ymm7, ymm1, rax*8
ttp		vmulpd	ymm4, ymm4, [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; value1 *= fudged grp two-to-phi
ttp		vmulpd	ymm5, ymm5, [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; value2 *= fudged grp two-to-phi
		add	rsi, dist_to_dest2		;; Change to a destination pointer
		vmovapd	[rsi+0*32], ymm4		;; Save new value1
		vmovapd	[rsi+1*32], ymm5		;; Save new value1
		sub	rsi, dist_to_dest2		;; Change back to a pointer into the first input number
	ENDM

; *************** WPN normalized small add macro ******************
; This macro implements the smalladd with normalization feature by adding the
; provided small value to the first FFT word and propagating any carries.
; NOTE: caller can assume xmm6,xmm7 are left untouched (for better prolog/epilog handling)

ynorm_smalladd_wpn MACRO ttp, base2
	LOCAL	clmloop, cloop, done

	mov	rsi, DESTARG			;; Address of destination
	vmovsd	xmm2, DBLARG			;; Small addin value
base2	vaddsd	xmm2, xmm2, YMM_BIGVAL
ttp	mov	rbp, norm_grp_mults		;; Addr of the group multipliers
ttp	mov	rdi, norm_biglit_array		;; Addr of the big/little flags array

clmloop:
ttp	mov	edx, cache_line_multiplier	;; We must increment rdi by a funky amount every 4*clm cache lines
no ttp	sub	rdx, rdx			;; Init loop counter

cloop:
ttp	movzx	rbx, WORD PTR [rdi]		;; Load big vs. little flags
ttp	movzx	eax, bh				;; Big/lit flags 1-2
ttp	and	rbx, 0e0h			;; Fudge flags 1

	vmovsd	xmm0, Q [rsi]			;; Load value
ttp	vmulsd	xmm0, xmm0, Q [rbp+0*YMM_GMD][rbx] ;; Mul values by fudged two-to-minus-phi
	vaddsd	xmm0, xmm0, xmm2		;; x = values + carry
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
ttp	vmulsd	xmm0, xmm0, Q [rbp+0*YMM_GMD+YMM_GMD/2][rbx] ;; value = rounded value * fudged grp two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save new value

base2	vmovsd	xmm1, YMM_BIGVAL		;; Load comparison value
no base2 vxorpd	xmm1, xmm1, xmm1		;; Create comparison value
	vcmpsd	xmm0, xmm2, xmm1, 0Ch		;; Test for non-zero carry
	vmovmskpd rax, xmm0			;; Extract comparison bit
	or	rax, rax			;; Is bit on?
	jz	short done			;; No, we are done

ttp	bump	rdi, 2				;; Advance pointers
	add	rsi, pass2blkdst		;; Next FFT data ptr
 	add	dl, 256/4			;; Special increment needed every 4th iteration
	jnc	cloop				;; Loop if not 4th iteration
	sub	rsi, pass1blkdst		;; Each pass1 block consists of 4 pass 2 blocks
	bump	rsi, 64				;; Next cache line
ttp	sub	dl, 4				;; Do we need to bump rdi yet?
ttp	jnz	cloop				;; No, repeat carry propagation loop
ttp	add	rdi, normval2			;; Adjust ptr to little/big flags
ttp	jmp	clmloop				;; Yes, reload 4*clm counter and propagate
no ttp	jmp	cloop				;; No, repeat carry propagation loop

done:	
	ENDM

; *************** WPN normalized small mul macro ******************
; This macro multiplies by a small value, then "normalizes" eight FFT data values. 
; rsi = pointer to destination
; rdi = pointer to array of big vs. little flags
; rbp = pointer two-to-phi group multipliers
; ymm2,ymm3 = carries
; ymm6 = small multiplier value
; A pipelined version of this code:
;	xload	ymm0, [rsi]		;; Load second number
;	mulpd	ymm0, ymm6		;; Mul by small value
;	movzx	rcx, BYTE PTR [rdi]	;; Load big vs. little flags
;	mulpd	ymm0, [rax]		;; Mul by fudged grp two-to-minus-phi
;	addpd	ymm0, [rbp+0*32]	;; x1 = values + carry
;	xload	ymm2, YMM_LIMIT_BIGMAX[rcx*2];; Load maximum * BIGVAL - BIGVAL
;	addpd	ymm2, ymm0		;; y1 = top bits of x
;	xload	ymm6, YMM_LIMIT_BIGMAX_NEG[rcx*2];; Load -(maximum*BIGVAL-BIGVAL)
;	addpd	ymm6, ymm2		;; z1 = y1-(maximum * BIGVAL - BIGVAL)
;	subpd	ymm0, ymm6		;; rounded value = x1 - z1
;	mulpd	ymm2, YMM_LIMIT_INVERSE[rcx*2];; next carry = shifted y1
;	xload	ymm4, YMM_TTP_FUDGE[rcx*2];; fudge two-to-phi
;	mulpd	ymm0, [rax+0*32+16]	;; new value1 = val * fudged grp two-to-phi
;	xstore	[rsi], ymm0		;; Save new value1

ynorm_smallmul_wpn MACRO ttp, base2
	vmulpd	ymm0, ymm6, [rsi]		;; Mul small multiplier by value1
	vmulpd	ymm1, ymm6, [rsi+32]		;; Mul small multiplier by value2

ttp	movzx	rbx, WORD PTR [rdi]		;; Load 4 big vs. little & fudge flags
ttp	movzx	ecx, bl				;; Fudge flags 1
ttp	and	rcx, 0e0h
ttp	movzx	eax, bh				;; Big/lit flags 1-2
ttp	and	rbx, 01ch			;; Fudge flags 2

ttp	vmulpd	ymm0, ymm0, [rbp+0*YMM_GMD][rcx] ;; Mul by fudged grp two-to-minus-phi
ttp	vmulpd	ymm1, ymm1, [rbp+1*YMM_GMD][rbx*8] ;; Mul by fudged grp two-to-minus-phi
	vaddpd	ymm0, ymm0, ymm2		;; x1 = values + carry
	vaddpd	ymm1, ymm1, ymm3		;; x2 = values + carry
	rounding_interleaved ttp, base2, noexec, ymm0, ymm2, ymm4, rax*8, ymm1, ymm3, ymm5, rax*8+32
ttp	vmulpd	ymm0, ymm0, [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; value1 *= fudged grp two-to-phi
ttp	vmulpd	ymm1, ymm1, [rbp+1*YMM_GMD+YMM_GMD/2][rbx*8] ;; value2 *= fudged grp two-to-phi
	vmovapd	[rsi+0*32], ymm0		;; Save new value1
	vmovapd	[rsi+1*32], ymm1		;; Save new value2
	ENDM

; Zero-padded FFT version
ynorm_smallmul_wpn_zpad MACRO ttp, base2
	vmulpd	ymm0, ymm6, [rsi]		;; Mul small multiplier by value1
	vmulpd	ymm1, ymm6, [rsi+32]		;; Mul small multiplier by value2

ttp	movzx	rcx, WORD PTR [rdi]		;; Load 4 big vs. little & fudge flags
ttp	movzx	eax, ch				;; Big/lit flags 1-2
ttp	and	rcx, 0e0h			;; Fudge flags 1 & 2

ttp	vmulpd	ymm0, ymm0, [rbp+0*YMM_GMD][rcx] ;; Mul by fudged grp two-to-minus-phi
ttp	vmulpd	ymm1, ymm1, [rbp+0*YMM_GMD][rcx] ;; Mul by fudged grp two-to-minus-phi
	vaddpd	ymm0, ymm0, ymm2		;; x1 = values + carry
	vaddpd	ymm1, ymm1, ymm3		;; x2 = values + carry
	rounding_interleaved ttp, base2, noexec, ymm0, ymm2, ymm4, rax*8, ymm1, ymm3, ymm5, rax*8
ttp	vmulpd	ymm0, ymm0, [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; value1 *= fudged grp two-to-phi
ttp	vmulpd	ymm1, ymm1, [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; value2 *= fudged grp two-to-phi
	vmovapd	[rsi+0*32], ymm0		;; Save new value1
	vmovapd	[rsi+1*32], ymm1		;; Save new value2
	ENDM

; This macro finishes the smallmul process by adding three carry values from the
; two carry registers from the end of a pass 1 block back to the start of the
; pass 1 block.  The remaining two carry values are rotated for starting the next block.
; rsi = pointer to the FFT data values
; rbp = pointer two-to-phi multipliers
; rdi = pointer to biglit flags
; ymm2,ymm3 = carries

ynorm_smallmul_wpn_blk MACRO ttp, base2
	LOCAL	clmloop, cloop, done

	rotate_carries_interleaved base2, ymm2, ymm4, ymm3, ymm5, ymm0, ymm1 ;; Rotate carries
	vmovapd	YMM_TMP1, ymm4			;; Save next block's carries
	vmovapd	YMM_TMP2, ymm5

clmloop:
ttp	mov	edx, cache_line_multiplier	;; We must increment rdi by a funky amount every 4*clm cache lines
no ttp	sub	rdx, rdx			;; Init loop counter

cloop:
ttp	movzx	rbx, WORD PTR [rdi]		;; Load big vs. little flags
ttp	movzx	ecx, bl				;; Fudge flags 1
ttp	and	rcx, 0e0h
ttp	movzx	eax, bh				;; Big/lit flags 1-2
ttp	and	rbx, 01ch			;; Fudge flags 2

	vmovapd	ymm0, [rsi]			;; Load values1
ttp	vmulpd	ymm0, ymm0, [rbp+0*YMM_GMD][rcx] ;; Mul values1 by fudged two-to-minus-phi
	vmovapd	ymm1, [rsi+32]			;; Load values2
ttp	vmulpd	ymm1, ymm1, [rbp+1*YMM_GMD][rbx*8] ;; Mul values2 by fudged two-to-minus-phi
	vaddpd	ymm0, ymm0, ymm2		;; x1 = values1 + carry
	vaddpd	ymm1, ymm1, ymm3		;; x2 = values2 + carry
	rounding_interleaved ttp, base2, noexec, ymm0, ymm2, ymm4, rax*8, ymm1, ymm3, ymm5, rax*8+32
ttp	vmulpd	ymm0, ymm0, [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; value1 = rounded value * fudged grp two-to-phi
ttp	vmulpd	ymm1, ymm1, [rbp+1*YMM_GMD+YMM_GMD/2][rbx*8] ;; value2 = rounded value * fudged grp two-to-phi
	vmovapd	[rsi], ymm0			;; Save new value1
	vmovapd	[rsi+32], ymm1			;; Save new value2

base2	vmovapd	ymm1, YMM_BIGVAL		;; Load comparison value
no base2 vxorpd	ymm1, ymm1, ymm1		;; Create comparison value
	vcmppd	ymm0, ymm2, ymm1, 0Ch		;; Test for non-zero carries
	vmovmskpd rax, ymm0			;; Extract 4 comparison bits
	vcmppd	ymm0, ymm3, ymm1, 0Ch		;; Test for non-zero carries
	vmovmskpd rcx, ymm0			;; Extract 4 comparison bits
	or	rax, rcx			;; Are any bits on?
	jz	short done			;; No, we are done

ttp	bump	rdi, 2				;; Advance pointers
	add	rsi, pass2blkdst		;; Next FFT data ptr
 	add	dl, 256/4			;; Special increment needed every 4th iteration
	jnc	cloop				;; Loop if not 4th iteration
	sub	rsi, pass1blkdst		;; Each pass1 block consists of 4 pass 2 blocks
	bump	rsi, 64				;; Next cache line
ttp	sub	dl, 4				;; Do we need to bump rdi yet?
ttp	jnz	cloop				;; No, repeat carry propagation loop
ttp	add	rdi, normval2			;; Adjust ptr to little/big flags
ttp	jmp	clmloop				;; Yes, reload 4*clm counter and propagate
no ttp	jmp	cloop				;; No, repeat carry propagation loop

done:	vmovapd	ymm2, YMM_TMP1			;; Set next block's carry
	vmovapd	ymm3, YMM_TMP2			;; Set next block's carry
	ENDM

; The zero-padded FFT version.  There can't be a carry into the high word.
ynorm_smallmul_wpn_zpad_blk MACRO ttp, base2
	LOCAL	clmloop, cloop, done

	rotate_carries_interleaved base2, ymm2, ymm4, ymm3, ymm5, ymm0, ymm1 ;; Rotate carries
	vmovapd	YMM_TMP1, ymm4			;; Save next block's carries
	vmovapd	YMM_TMP2, ymm5

clmloop:
ttp	mov	edx, cache_line_multiplier	;; We must increment rdi by a funky amount every 4*clm cache lines
no ttp	sub	rdx, rdx			;; Init loop counter

cloop:
ttp	movzx	rcx, WORD PTR [rdi]		;; Load big vs. little flags
ttp	movzx	eax, ch				;; Big/lit flags 1-2
ttp	and	rcx, 0e0h			;; Fudge flags 1 & 2

	vmovapd	ymm0, [rsi]			;; Load values1
ttp	vmulpd	ymm0, ymm0, [rbp+0*YMM_GMD][rcx] ;; Mul values1 by fudged two-to-minus-phi
	vaddpd	ymm0, ymm0, ymm2		;; x1 = values1 + carry
	rounding ttp, base2, noexec, ymm0, ymm2, ymm4, rax*8
ttp	vmulpd	ymm0, ymm0, [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; value1 = rounded value * fudged grp two-to-phi
	vmovapd	[rsi], ymm0			;; Save new value1

base2	vmovapd	ymm1, YMM_BIGVAL		;; Load comparison value
no base2 vxorpd	ymm1, ymm1, ymm1		;; Create comparison value
	vcmppd	ymm0, ymm2, ymm1, 0Ch		;; Test for non-zero carries
	vmovmskpd rax, ymm0			;; Extract 4 comparison bits
	test	rax, rax			;; Are any bits on?
	jz	short done			;; No, we are done

ttp	bump	rdi, 2				;; Advance pointers
	add	rsi, pass2blkdst		;; Next FFT data ptr
 	add	dl, 256/4			;; Special increment needed every 4th iteration
	jnc	cloop				;; Loop if not 4th iteration
	sub	rsi, pass1blkdst		;; Each pass1 block consists of 4 pass 2 blocks
	bump	rsi, 64				;; Next cache line
ttp	sub	dl, 4				;; Do we need to bump rdi yet?
ttp	jnz	cloop				;; No, repeat carry propagation loop
ttp	add	rdi, normval2			;; Adjust ptr to little/big flags
ttp	jmp	clmloop				;; Yes, reload 4*clm counter and propagate
no ttp	jmp	cloop				;; No, repeat carry propagation loop

done:	vmovapd	ymm2, YMM_TMP1			;; Set next block's carry
	vmovapd	ymm3, YMM_TMP2			;; Set next block's carry
	ENDM

; This macro finishes the smallmul process by adding the last two carries
; from the end back to the start of the result.
; rsi = pointer to FFT data
; rbp = pointer two-to-phi group multipliers
; rdi = biglit flags ptr
; ymm2,ymm3 = carries

ynorm_smallmul_wpn_final MACRO ttp, base2
	LOCAL	clmloop, cloop, done

	ynorm_top_carry_cmn base2, rsi, xmm3, 2

base2	vsubsd	xmm3, xmm3, YMM_BIGVAL		;; Remove BIGVAL
	vmulsd	xmm3, xmm3, YMM_MINUS_C		;; mul wrap around carry by -c
base2	vaddsd	xmm3, xmm3, YMM_BIGVAL		;; Restore BIGVAL

clmloop:
ttp	mov	edx, cache_line_multiplier	;; We must increment rdi by a funky amount every 4*clm cache lines
no ttp	sub	rdx, rdx			;; Init loop counter

cloop:
ttp	movzx	rbx, WORD PTR [rdi]		;; Load big vs. little flags
ttp	movzx	ecx, bl				;; Fudge flags 1
ttp	and	rcx, 0e0h
ttp	movzx	eax, bh				;; Big/lit flags 1-2
ttp	and	rbx, 01ch			;; Fudge flags 2

	vmovsd	xmm0, Q [rsi]			;; Load value1
ttp	vmulsd	xmm0, xmm0, Q [rbp+0*YMM_GMD][rcx] ;; Mul value1 by fudged two-to-minus-phi
	vmovsd	xmm1, Q [rsi+32]		;; Load value2
ttp	vmulsd	xmm1, xmm1, Q [rbp+1*YMM_GMD][rbx*8] ;; Mul value2 by fudged two-to-minus-phi
	vaddsd	xmm0, xmm0, xmm3		;; x1 = values1 + carry2
	vaddsd	xmm1, xmm1, xmm2		;; x2 = values2 + carry1
	new_single_rounding_interleaved ttp, base2, xmm0, xmm3, xmm4, rax*8, xmm1, xmm2, xmm5, rax*8+32
ttp	vmulsd	xmm0, xmm0, Q [rbp+0*YMM_GMD+YMM_GMD/2][rcx] ;; value1 = rounded value * fudged grp two-to-phi
ttp	vmulsd	xmm1, xmm1, Q [rbp+1*YMM_GMD+YMM_GMD/2][rbx*8] ;; value2 = rounded value * fudged grp two-to-phi
	vmovsd	Q [rsi], xmm0			;; Save new value1
	vmovsd	Q [rsi+32], xmm1		;; Save new value2

base2	vmovsd	xmm1, YMM_BIGVAL		;; Load comparison value
no base2 vxorpd	xmm1, xmm1, xmm1		;; Create comparison value
	vcmpsd	xmm0, xmm3, xmm1, 0Ch		;; Test for non-zero carry
	vmovmskpd rax, xmm0			;; Extract comparison bit
	vcmpsd	xmm0, xmm2, xmm1, 0Ch		;; Test for non-zero carry
	vmovmskpd rcx, xmm0			;; Extract comparison bit
	or	rax, rcx			;; Is either bit on?
	jz	short done			;; No, we are done

ttp	bump	rdi, 2				;; Advance pointers
	add	rsi, pass2blkdst		;; Next FFT data ptr
 	add	dl, 256/4			;; Special increment needed every 4th iteration
	jnc	cloop				;; Loop if not 4th iteration
	sub	rsi, pass1blkdst		;; Each pass1 block consists of 4 pass 2 blocks
	bump	rsi, 64				;; Next cache line
ttp	sub	dl, 4				;; Do we need to bump rdi yet?
ttp	jnz	cloop				;; No, repeat carry propagation loop
ttp	add	rdi, normval2			;; Adjust ptr to little/big flags
ttp	jmp	clmloop				;; Yes, reload 4*clm counter and propagate
no ttp	jmp	cloop				;; No, repeat carry propagation loop

done:	
	ENDM


; This macro finishes the smallmul normalize process by adding the final
; two carries back into the appropriate FFT values at the start of the fft.
; rsi = pointer to FFT data
; rbp = pointer two-to-phi group multipliers
; rdi = biglit flags ptr
; ymm2,ymm3 = carries (and the ymm3 carry must be zero)

ynorm_smallmul_wpn_zpad_final MACRO ttp, base2, destptr
	LOCAL	smallk, mediumk, div_k_done

base2	vsubsd	xmm2, xmm2, YMM_BIGVAL		;; Integerize carry

	;; Copy and integerize data from 7 words above halfway point to ZPAD0-ZPAD6
	;; Clear high words as we go
	;; Then we can make an almost exact copy of the ynorm_1d_zpad_cleanup code

	mov	rsi, destptr			;; Address of FFT data
ttp	lea	rdi, YMM_FIRST_BIGLIT_VALUES	;; Address of first big/little flags

ttp	movzx	rax, BYTE PTR [rdi]		;; Load big vs. little flags
	vmovsd	xmm0, Q [rsi+32]		;; Value1
	new_single_split_lower_zpad_word ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	ZPAD0, xmm0

	add	rsi, YMM_SRC_INCR1		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+1]		;; Load big vs. little flags
	vmovsd	xmm0, Q [rsi+32]		;; Value2
	new_single_split_lower_zpad_word ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	ZPAD1, xmm0

	add	rsi, YMM_SRC_INCR2		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+2]		;; Load big vs. little flags
	vmovsd	xmm0, Q [rsi+32]		;; Value3
	new_single_split_lower_zpad_word ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	ZPAD2, xmm0

	add	rsi, YMM_SRC_INCR3		;; Next source pointer
	vmovsd	xmm0, Q [rsi+32]		;; Value4
ttp	vroundsd xmm0, xmm0, xmm0, 0		;; Round to an integer
	vaddsd	xmm0, xmm0, xmm2		;; Value4 + carry
	vmovsd	ZPAD3, xmm0

	add	rsi, YMM_SRC_INCR4		;; Next source pointer
	vmovsd	xmm0, Q [rsi+32]		;; Value5
ttp	vroundsd xmm0, xmm0, xmm0, 0		;; Round to an integer
	vmovsd	ZPAD4, xmm0
	vxorpd	xmm1, xmm1, xmm1		;; Clear highest words
	vmovsd	Q [rsi+32], xmm1		;; Clear value5

	add	rsi, YMM_SRC_INCR5		;; Next source pointer
	vmovsd	xmm0, Q [rsi+32]		;; Value6
ttp	vroundsd xmm0, xmm0, xmm0, 0		;; Round to an integer
	vmovsd	ZPAD5, xmm0
	vmovsd	Q [rsi+32], xmm1		;; Clear value6

	add	rsi, YMM_SRC_INCR6		;; Next source pointer
	vmovsd	xmm0, Q [rsi+32]		;; Value7
ttp	vroundsd xmm0, xmm0, xmm0, 0		;; Round to an integer
	vmovsd	ZPAD6, xmm0
	vmovsd	Q [rsi+32], xmm1		;; Clear value7

	;; Divide the zpad data by k.  Store the integer part in YMM_TMP
	;; and the remainder in ZPAD0.  Later we will wrap the integer part
	;; down to the bottom of the FFT data area (and multiply by -c).
	;; And we will store the remainder in the upper half of the FFT
	;; data area.

	;; Note there are three cases to handle.  K is smaller than a big word.
	;; K is between one and 2 big words in size.  And K is more than
	;; 2 big words in size.

	cmp	ZPAD_TYPE, 2			;; Are we dealing with case 1,2,or 3
	jl	smallk				;; One word case
	je	mediumk				;; Two word case

	;; This case does the divide by k where k is three words

	vmovsd	xmm0, ZPAD6			;; Load zpad word (high bits)
	vmovsd	xmm1, ZPAD5			;; Load zpad word (middle bits)
	vmovsd	xmm2, ZPAD4			;; Load zpad word (low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K6	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K6_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K6_MID		;; Mul by middle bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K6_LO		;; Mul by low bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP5, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT6		;; Shift previous zpad word
	vaddsd	xmm1, xmm1, xmm0		;; Add to create new high zpad bits
	vmovsd	xmm0, ZPAD3			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm1, ZPAD_SHIFT5		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm2
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K5	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K5_HI		;; Mul by high bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K5_MID		;; Mul by middle bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K5_LO		;; Mul by low bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP4, xmm4			;; Save word of zpad / k

	vmulsd	xmm1, xmm1, ZPAD_SHIFT5		;; Shift previous zpad word
	vaddsd	xmm2, xmm2, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD2			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm2, ZPAD_SHIFT4		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm0
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K4	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K4_HI		;; Mul by high bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K4_MID		;; Mul by middle bits of k
	vsubsd	xmm0,xmm0,xmm5			;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K4_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP3, xmm4			;; Save word of zpad / k

	vmulsd	xmm2, xmm2, ZPAD_SHIFT4		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm2		;; Add to create new high zpad bits
	vmovsd	xmm2, ZPAD1			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm0, ZPAD_SHIFT3		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm1
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K3	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K3_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K3_MID		;; Mul by middle bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K3_LO		;; Mul by low bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP2, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT3		;; Shift previous zpad word
	vaddsd	xmm1, xmm1, xmm0		;; Add to create new high zpad bits
	vmovsd	xmm0, ZPAD0			;; Load zpad word (new low bits)
	vmulsd	xmm5, xmm1, ZPAD_SHIFT2		;; Combine high and medium bits
	vaddsd	xmm5, xmm5, xmm2
	vmulsd	xmm4, xmm5, ZPAD_INVERSE_K2	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K2_HI		;; Mul by high bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K2_MID		;; Load middle bits of k
	vsubsd	xmm2, xmm2, xmm5		;; Calculate middle bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K2_LO		;; Load low bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP1, xmm4			;; Save word of zpad / k

	vmulsd	xmm1, xmm1, ZPAD_SHIFT2		;; Shift previous zpad word
	vaddsd	xmm2, xmm2, xmm1		;; Add to create new high zpad bits
	vmulsd	xmm2, xmm2, ZPAD_SHIFT1		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm2		;; Add to create new high zpad bits
	vmovsd	ZPAD0, xmm0			;; Save remainder of zpad / k

	vxorpd	xmm1, xmm1, xmm1		;; Zero words that other cases set
	vmovsd	YMM_TMP6, xmm1
	
	jmp	div_k_done

	;; This case does the divide by k where k is two words
mediumk:
	vmovsd	xmm0, ZPAD6			;; Load zpad word (high bits)
	vmovsd	xmm1, ZPAD5			;; Load zpad word (low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K6	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K6_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K6_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP6, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT6		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD4			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K5	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K5_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K5_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP5, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT5		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD3			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K4	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K4_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K4_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP4, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT4		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD2			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K3	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K3_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K3_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP3, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT3		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD1			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K2	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K2_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K2_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP2, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT2		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	xmm1, ZPAD0			;; Load zpad word (new low bits)
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD by shifted 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmulsd	xmm5, xmm4, ZPAD_K1_HI		;; Mul by high bits of k
	vsubsd	xmm0, xmm0, xmm5		;; Calculate high bits of remainder
	vmulsd	xmm5, xmm4, ZPAD_K1_LO		;; Mul by low bits of k
	vsubsd	xmm1, xmm1, xmm5		;; Calculate low bits of remainder
	vmovsd	YMM_TMP1, xmm4			;; Save word of zpad / k

	vmulsd	xmm0, xmm0, ZPAD_SHIFT1		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, xmm1		;; Add to create new high zpad bits
	vmovsd	ZPAD0, xmm0			;; Save remainder of zpad / k

	jmp	div_k_done

	;; This case does the divide by k where k is one word
	;; Assume ZPAD5 and ZPAD6 are zero.
smallk:	vmovsd	xmm0, ZPAD4			;; Load zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP5, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT4		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD3		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP4, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT3		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD2		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP3, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT2		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD1		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP2, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4

	vmulsd	xmm0, xmm0, ZPAD_SHIFT1		;; Shift previous zpad word
	vaddsd	xmm0, xmm0, ZPAD0		;; Add in zpad data
	vmulsd	xmm4, xmm0, ZPAD_INVERSE_K1	;; Mul ZPAD data by 1/k
	vroundsd xmm4, xmm4, xmm4, 0		;; Round to integer
	vmovsd	YMM_TMP1, xmm4			;; Save integer part
	vmulsd	xmm4, xmm4, ZPAD_K1_LO		;; Compute remainder
	vsubsd	xmm0, xmm0, xmm4
	vmovsd	ZPAD0, xmm0			;; Save remainder

	vxorpd	xmm1, xmm1, xmm1		;; Zero words that other cases set
	vmovsd	YMM_TMP6, xmm1

div_k_done:

	;; Now normalize the data above the halfway point.  Remember that the
	;; two-to-phi multiplier for the first value will be 1.0.

ttp	lea	rdi, YMM_FIRST_BIGLIT_VALUES	;; Address of first big/little flags
	mov	rsi, destptr			;; Address of squared number

ttp	movzx	rax, BYTE PTR [rdi]		;; First word
	vmovsd	xmm0, ZPAD0			;; Load remainder of divide by k
base2	vaddsd	xmm0, xmm0, YMM_BIGVAL
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi+32], xmm0		;; Save value1

	add	rsi, YMM_SRC_INCR1		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+1]		;; Load big vs. little flags
	new_single_rounding ttp, base2, xmm2, xmm0, xmm4, rax*8
	vmovsd	Q [rsi+32], xmm2		;; Save value2

	add	rsi, YMM_SRC_INCR2		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+2]		;; Load big vs. little flags
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi+32], xmm0		;; Save value3

	add	rsi, YMM_SRC_INCR3		;; Next source pointer
base2	vsubsd	xmm2, xmm2, YMM_BIGVAL		;; Remove integer rounding constant
	vmovsd	Q [rsi+32], xmm2		;; Save new value4

	;; Mul the integer part of (ZPAD data divided by k) by -c in
	;; preparation for adding it into the lower FFT data area.
	;; Also add in the shifted high FFT carry at this time.

	;; Now add in and normalize the bottom FFT data.  Remember that the
	;; two-to-phi multiplier for the first value will be 1.0.  We 
	;; must go 6 words deep in case k is 48-50 bits and c is 32 bits.

	mov	rsi, destptr			;; Address of squared number

ttp	movzx	rax, BYTE PTR [rdi]		;; First word
	vmovsd	xmm0, YMM_TMP1			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
base2	vaddsd	xmm0, xmm0, YMM_BIGVAL
	vaddsd	xmm0, xmm0, Q [rsi]		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi], xmm0			;; Save value1

	add	rsi, YMM_SRC_INCR1		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+1]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP2			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vaddsd	xmm0, xmm0, xmm2		;; x2 = value + carry
	vaddsd	xmm0, xmm0, Q [rsi]		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi], xmm0			;; Save value2

	add	rsi, YMM_SRC_INCR2		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+2]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP3			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vaddsd	xmm0, xmm0, xmm2		;; x3 = value + carry
	vaddsd	xmm0, xmm0, Q [rsi]		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi], xmm0			;; Save value3

	add	rsi, YMM_SRC_INCR3		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+3]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP4			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vaddsd	xmm0, xmm0, xmm2		;; x4 = value + carry
	vaddsd	xmm0, xmm0, Q [rsi]		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi], xmm0			;; Save value4

	add	rsi, YMM_SRC_INCR4		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+4]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP5			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vaddsd	xmm0, xmm0, xmm2		;; x5 = value + carry
	vaddsd	xmm0, xmm0, Q [rsi]		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi], xmm0			;; Save value5

	add	rsi, YMM_SRC_INCR5		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+5]		;; Load big vs. little flags
	vmovsd	xmm0, YMM_TMP6			;; Load integer part of divide by k
	vmulsd	xmm0, xmm0, YMM_MINUS_C		;; Mul by -c
	vaddsd	xmm0, xmm0, xmm2		;; x6 = value + carry
	vaddsd	xmm0, xmm0, Q [rsi]		;; Add in the FFT data
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi], xmm0			;; Save value6

	add	rsi, YMM_SRC_INCR6		;; Next source pointer
ttp	movzx	rax, BYTE PTR [rdi+6]		;; Load big vs. little flags
	vaddsd	xmm0, xmm2, Q [rsi]		;; x7 = value + carry
	new_single_rounding ttp, base2, xmm0, xmm2, xmm4, rax*8
	vmovsd	Q [rsi], xmm0			;; Save value7

	add	rsi, YMM_SRC_INCR7		;; Next source pointer
base2	vsubsd	xmm2, xmm2, YMM_BIGVAL		;; Remove rounding constant
	vaddsd	xmm2, xmm2, Q [rsi]		;; Add in FFT data
	vmovsd	Q [rsi], xmm2			;; Save value8
	ENDM


;;
;; Macro to copy and possibly zero 8 doubles
;;

ycopyzero MACRO
	LOCAL	nz1,nz2,nz3,nz4
	cmp	ecx, COPYZERO+3*4	;; Is this the first cache line where we copy 1 values?
	jne	short nz1		;; No, jump
	vmovapd	ymm1, YMM_TMP1		;; Yes, switch to the copy 1 values mask
nz1:	cmp	ecx, COPYZERO+2*4	;; Is this the first cache line where we copy 2 values?
	jne	short nz2		;; No, jump
	vmovapd	ymm1, YMM_TMP2		;; Yes, switch to the copy 2 values mask
nz2:	cmp	ecx, COPYZERO+1*4	;; Is this the first cache line where we copy 3 values?
	jne	short nz3		;; No, jump
	vmovapd	ymm1, YMM_TMP3		;; Yes, switch to the copy 3 values mask
nz3:	cmp	ecx, COPYZERO+0*4	;; Is this the first cache line where we copy 4 values?
	jne	short nz4		;; No, jump
	vmovapd	ymm1, YMM_TMP4		;; Yes, switch to the copy 4 values mask
nz4:	vmaskmovpd ymm0, ymm1, [rsi]	;; Conditionally load low doubles
	vmovapd	[rdi], ymm0		;; Save low doubles
	vmovapd	ymm2, [rsi+32]		;; Load high doubles
	vmovapd	[rdi+32], ymm2		;; Save high doubles
	ENDM

; Copyright 2011-2012 - Mersenne Research, Inc.  All rights reserved
; Author:  George Woltman
; Email: woltman@alum.mit.edu
;

;;
;;
;; All new macros for version 27 of gwnum.  Do a radix-5 step in an FFT.
;; The forward FFT macros multiply by the sin/cos values at the end of the macro
;; and the inverse FFTs multiply by the sin/cos values at the start of the macro.
;; We also implement the Daniel J. Bernstein (DJB) "exponent-1" idea to save sin/cos
;; memory.
;;
;;

;;
;; ************************************* five-complex-djbfft variants ******************************************
;;

;; The standard version
yr5_5cl_five_complex_djbfft_preload MACRO
	yr5_5c_djbfft_cmn_preload
	ENDM
yr5_5cl_five_complex_djbfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	yr5_5c_djbfft_cmn srcreg,srcinc,0,d1,noexec,screg,screg+64,32,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like 5cl but uses a ten-reals sin/cos data
yr5_r5cl_five_complex_djbfft_preload MACRO
	yr5_5c_djbfft_cmn_preload
	ENDM
yr5_r5cl_five_complex_djbfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	yr5_5c_djbfft_cmn srcreg,srcinc,0,d1,noexec,screg+64,screg+192,32,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

;; Like 5cl but uses rbx to index into source
yr5_f5cl_five_complex_djbfft_preload MACRO
	yr5_5c_djbfft_cmn_preload
	ENDM
yr5_f5cl_five_complex_djbfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	yr5_5c_djbfft_cmn srcreg,srcinc,rbx,d1,noexec,screg,screg+64,32,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like the 5cl version except vbroadcastsd is used to reduce sin/cos data
yr5_b5cl_five_complex_djbfft_preload MACRO
	yr5_5c_djbfft_cmn_preload
	ENDM
yr5_b5cl_five_complex_djbfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	yr5_5c_djbfft_cmn srcreg,srcinc,0,d1,exec,screg,screg+16,8,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like b5cl but extracts the sin/cos data to broadcasts from
; the ten-real/five_complex sin/cos table
yr5_rb5cl_five_complex_djbfft_preload MACRO
	yr5_5c_djbfft_cmn_preload
	ENDM
yr5_rb5cl_five_complex_djbfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	yr5_5c_djbfft_cmn srcreg,srcinc,0,d1,exec,screg+8,screg+80,40,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

;; Common code to do the 5-complex FFT.  A 5-complex FFT is:
;; r25a=r2+r5
;; r34a=r3+r4
;; i25s=i2-i5
;; i34s=i3-i4
;; outr(0) = r1 + r25a + r34a
;; t1=cos2*r25a + cos4*r34a + r1
;; t2=sin2*i25s + sin4*i34s
;; outr(1)=t1-t2
;; outr(4)=t1+t2
;; t3=cos4*r25a + cos2*r34a + r1
;; t4=sin4*i25s - sin2*i34s
;; outr(2)=t3-t4
;; outr(3)=t3+t4
;; r25s=r2-r5
;; r34s=r3-r4
;; i25a=i2+i5
;; i34a=i3+i4
;; outi(0)=i1+i25a+i34a
;; t5=cos2*i25a + cos4*i34a + i1
;; t6=sin2*r25s + sin4*r34s
;; outi(1)=t5+t6
;; outi(4)=t5-t6
;; t7=cos4*i25a + cos2*i34a + i1
;; t8=sin4*r25s - sin2*r34s
;; outi(2)=t7+t8
;; outi(3)=t7-t8
;; Where cos2 = cos 2*pi/5 = 0.309, sin2 = 0.951,
;; cos4 =-0.809, sin4 = 0.588
;; Finally, multiply 4 of the 5 results by twiddle factors.

yr5_5c_djbfft_cmn_preload MACRO
	ENDM
yr5_5c_djbfft_cmn MACRO srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd	ymm0, [srcreg+srcoff+d1]	;; r2
	vaddpd	ymm0, ymm0, [srcreg+srcoff+4*d1] ;; r25a=r2+r5
	vmovapd	ymm1, [srcreg+srcoff+2*d1]	;; r3
	vaddpd	ymm1, ymm1, [srcreg+srcoff+3*d1] ;; r34a=r3+r4
	vmovapd	ymm2, [srcreg+srcoff+d1+32]	;; i2
	vsubpd	ymm2, ymm2, [srcreg+srcoff+4*d1+32] ;; i25s=i2-i5
	vmovapd	ymm3, [srcreg+srcoff+2*d1+32]	;; i3
	vsubpd	ymm3, ymm3, [srcreg+srcoff+3*d1+32] ;; i34s=i3-i4
	vmulpd	ymm4, ymm0, YMM_P309		;; cos2*r25a
	vmulpd	ymm5, ymm1, YMM_P809		;; -cos4*r34a
	vmulpd	ymm6, ymm2, YMM_P951		;; sin2*i25s
	vmulpd	ymm7, ymm3, YMM_P588		;; sin4*i34s
	vsubpd	ymm4, ymm4, ymm5		;; cos2*r25a + cos4*r34a
	vaddpd	ymm6, ymm6, ymm7		;; t2=sin2*i25s + sin4*i34s
	vmulpd	ymm5, ymm0, YMM_P809		;; -cos4*r25a
	vaddpd	ymm0, ymm0, ymm1		;; r25a + r34a
	vmovapd	ymm7, [srcreg+srcoff]		;; r1
	vaddpd	ymm4, ymm4, ymm7		;; t1=cos2*r25a + cos4*r34a + r1
	vaddpd	ymm0, ymm0, ymm7		;; outr(0) = r1 + r25a + r34a
	vmulpd	ymm1, ymm1, YMM_P309		;; cos2*r34a
	vmulpd	ymm2, ymm2, YMM_P588		;; sin4*i25s
	vmulpd	ymm3, ymm3, YMM_P951		;; sin2*i34s
	vsubpd	ymm5, ymm1, ymm5		;; cos4*r25a+cos2*r34a
	vsubpd	ymm2, ymm2, ymm3		;; t4=sin4*i25s-sin2*i34s
	vaddpd	ymm5, ymm5, ymm7		;; t3=cos4*r25a+cos2*r34a+r1
	vsubpd	ymm7, ymm4, ymm6		;; outr(1)=t1-t2
	vaddpd	ymm4, ymm4, ymm6		;; outr(4)=t1+t2
	vsubpd	ymm6, ymm5, ymm2		;; outr(2)=t3-t4
	vaddpd	ymm5, ymm5, ymm2		;; outr(3)=t3+t4

	L1prefetchw srcreg+L1pd, L1pt

	vmovapd	YMM_TMP1, ymm7			;; Save new r2
	vmovapd	YMM_TMP2, ymm6			;; Save new r3
	vmovapd	YMM_TMP3, ymm5			;; Save new r4
	vmovapd	YMM_TMP4, ymm4			;; Save new r5

	vmovapd	ymm2, [srcreg+srcoff+d1+32]	;; i2
	vaddpd	ymm2, ymm2, [srcreg+srcoff+4*d1+32] ;; i25a=i2+i5
	vmovapd	ymm3, [srcreg+srcoff+2*d1+32]	;; i3
	vaddpd	ymm3, ymm3, [srcreg+srcoff+3*d1+32] ;; i34a=i3+i4
	vmovapd	ymm4, [srcreg+srcoff+d1]	;; r2
	vsubpd	ymm4, ymm4, [srcreg+srcoff+4*d1] ;; r25s=r2-r5
	vmovapd	ymm1, [srcreg+srcoff+2*d1]	;; r3
	vsubpd	ymm1, ymm1, [srcreg+srcoff+3*d1] ;; r34s=r3-r4

	L1prefetchw srcreg+d1+L1pd, L1pt

	vmovapd	[srcreg], ymm0			;; Save R1

	vaddpd	ymm5, ymm2, ymm3		;; i25a+i34a
	vmovapd	ymm0, YMM_P309
	vmulpd	ymm6, ymm2, ymm0		;; cos2*i25a
	vmovapd	ymm7, YMM_P809
	vmulpd	ymm2, ymm2, ymm7		;; -cos4*i25a
	vmulpd	ymm7, ymm7, ymm3		;; -cos4*i34a
	vmulpd	ymm3, ymm3, ymm0		;; cos2*i34a

	vsubpd	ymm6, ymm6, ymm7		;; cos2*i25a + cos4*i34a
	vsubpd	ymm2, ymm3, ymm2		;; cos4*i25a + cos2*i34a

	L1prefetchw srcreg+2*d1+L1pd, L1pt

	vmovapd	ymm7, YMM_P951
	vmulpd	ymm0, ymm7, ymm4		;; sin2*r25s
	vmulpd	ymm3, ymm7, ymm1		;; sin2*r34s
	vmovapd	ymm7, YMM_P588
	vmulpd	ymm4, ymm7, ymm4		;; sin4*r25s
	vmulpd	ymm1, ymm7, ymm1		;; sin4*r34s
	vmovapd	ymm7, [srcreg+srcoff+32]	;; i1
	vaddpd	ymm6, ymm6, ymm7		;; t5=cos2*i25a + cos4*i34a + i1
	vaddpd	ymm2, ymm2, ymm7		;; t7=cos4*i25a + cos2*i34a + i1
	vsubpd	ymm4, ymm4, ymm3		;; t8=sin4*r25s - sin2*r34s
	vaddpd	ymm0, ymm0, ymm1		;; t6=sin2*r25s + sin4*r34s

	L1prefetchw srcreg+3*d1+L1pd, L1pt

	vsubpd	ymm1, ymm2, ymm4		;; outi(3)=t7-t8
	vaddpd	ymm2, ymm2, ymm4		;; outi(2)=t7+t8
	vsubpd	ymm4, ymm6, ymm0		;; outi(4)=t5-t6
	vaddpd	ymm6, ymm6, ymm0		;; outi(1)=t5+t6
	vaddpd	ymm5, ymm5, ymm7		;; outi(0)=i1+i25a+i34a

no bcast vmovapd ymm3, [screg1+cosoff]		;; cosine/sine
bcast	vbroadcastsd ymm3, Q [screg1+cosoff]	;; cosine/sine
	vmulpd	ymm7, ymm3, YMM_TMP4		;; A5 = R5 * cosine/sine
	vmulpd	ymm0, ymm3, YMM_TMP1		;; A2 = R2 * cosine/sine
	vaddpd	ymm7, ymm7, ymm4		;; A5 = A5 + I5
	vsubpd	ymm0, ymm0, ymm6		;; A2 = A2 - I2
	vmulpd	ymm4, ymm4, ymm3		;; B5 = I5 * cosine/sine
	vmulpd	ymm6, ymm6, ymm3		;; B2 = I2 * cosine/sine
	vsubpd	ymm4, ymm4, YMM_TMP4		;; B5 = B5 - R5
	vaddpd	ymm6, ymm6, YMM_TMP1		;; B2 = B2 + R2
	vmovapd	[srcreg+32], ymm5		;; Save I1
no bcast vmovapd ymm3, [screg1]			;; sine
bcast	vbroadcastsd ymm3, Q [screg1]		;; sine
	vmulpd	ymm7, ymm7, ymm3		;; A5 = A5 * sine (new R5)
	vmulpd	ymm0, ymm0, ymm3		;; A2 = A2 * sine (new R2)
	vmulpd	ymm4, ymm4, ymm3		;; B5 = B5 * sine (new I5)
	vmulpd	ymm6, ymm6, ymm3		;; B2 = B2 * sine (new I2)

	L1prefetchw srcreg+4*d1+L1pd, L1pt

no bcast vmovapd ymm3, [screg2+cosoff]		;; cosine/sine
bcast	vbroadcastsd ymm3, Q [screg2+cosoff]	;; cosine/sine
	vmulpd	ymm5, ymm3, YMM_TMP2 		;; A3 = R3 * cosine/sine
	vsubpd	ymm5, ymm5, ymm2		;; A3 = A3 - I3
	vmulpd	ymm2, ymm2, ymm3		;; B3 = I3 * cosine/sine
	vmovapd	[srcreg+d1], ymm0		;; Save R2
	vmulpd	ymm0, ymm3, YMM_TMP3		;; A4 = R4 * cosine/sine
	vaddpd	ymm0, ymm0, ymm1		;; A4 = A4 + I4
	vmulpd	ymm1, ymm1, ymm3		;; B4 = I4 * cosine/sine
	vaddpd	ymm2, ymm2, YMM_TMP2		;; B3 = B3 + R3
	vsubpd	ymm1, ymm1, YMM_TMP3		;; B4 = B4 - R4
no bcast vmovapd ymm3, [screg2]			;; sine
bcast	vbroadcastsd ymm3, Q [screg2]		;; sine
	vmulpd	ymm5, ymm5, ymm3		;; A3 = A3 * sine (new R3)
	vmulpd	ymm0, ymm0, ymm3		;; A4 = A4 * sine (new R4)
	vmulpd	ymm2, ymm2, ymm3		;; B3 = B3 * sine (new I3)
	vmulpd	ymm1, ymm1, ymm3		;; B4 = B4 * sine (new I4)

	vmovapd	[srcreg+d1+32], ymm6		;; Save I2
	vmovapd	[srcreg+2*d1], ymm5		;; Save R3
	vmovapd	[srcreg+2*d1+32], ymm2		;; Save I3
	vmovapd	[srcreg+3*d1], ymm0		;; Save R4
	vmovapd	[srcreg+3*d1+32], ymm1		;; Save I4
	vmovapd	[srcreg+4*d1], ymm7		;; Save R5
	vmovapd	[srcreg+4*d1+32], ymm4		;; Save I5

	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; 64-bit version

IFDEF X86_64

yr5_5c_djbfft_cmn_preload MACRO
	vmovapd	ymm13, YMM_P309			;; cos2
	vmovapd	ymm14, YMM_P951			;; sin2
	vmovapd	ymm15, YMM_P588			;; sin4
	ENDM

yr5_5c_djbfft_cmn MACRO srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	yr5_5c_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 2,exec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 3,exec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 4,exec,exec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 5,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 5*srcinc
	bump	screg, 5*scinc
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	yr5_5c_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 2,exec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 3,exec,exec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 4,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	screg, 4*scinc
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	yr5_5c_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 2,exec,exec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 3,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	screg, 3*scinc
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	yr5_5c_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 1,exec,exec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 2,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	screg, 2*scinc
	ELSE
	yr5_5c_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 0,noexec,exec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbfft_unroll 1,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDIF
	ENDM

yr5_5c_djbfft_unroll MACRO iter,prev,this,next,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<ymm0>
y1	TEXTEQU	<ymm1>
y2	TEXTEQU	<ymm2>
y3	TEXTEQU	<ymm3>
y4	TEXTEQU	<ymm4>
y5	TEXTEQU	<ymm5>
y6	TEXTEQU	<ymm6>
y7	TEXTEQU	<ymm7>
y8	TEXTEQU	<ymm8>
y9	TEXTEQU	<ymm9>
y10	TEXTEQU	<ymm10>
y11	TEXTEQU	<ymm11>
y12	TEXTEQU	<ymm12>
	ENDIF

;; On later calls, y0-y12 contain this r2, this r3, prev R5, this r5, prev I5,
;; prev A3, prev B4, prev sine, prev I2, free, prev R2, prev R4, prev B3.
;; ymm13, ymm14 and ymm15 are preloaded constants cos2, sin2 and sin4.

this	vaddpd	y9, y0, y3				;; r25a=r2+r5			; 1-3
prev	vmulpd	y5, y5, y7				;; A3 = A3 * sine (final R3)	; 1-5
prev	vmovapd	[srcreg+(iter-1)*srcinc+4*d1], y2	;; Save R5			; 1
this	vmovapd	y2, [srcreg+iter*srcinc+srcoff+3*d1]	;; r4

prev	vmovapd	[srcreg+(iter-1)*srcinc+d1], y10	;; Save R2			; 2
this	vaddpd	y10, y1, y2				;; r34a=r3+r4			; 2-4
prev	vmulpd	y6, y6, y7				;; B4 = B4 * sine (final I4)	; 2-6
prev	vmovapd	[srcreg+(iter-1)*srcinc+4*d1+32], y4	;; Save I5			; 3
this	vmovapd	y4, [srcreg+iter*srcinc+srcoff+d1+32]	;; i2

this	vsubpd	y0, y0, y3				;; r25s=r2-r5			; 3-5
prev	vmulpd	y12, y12, y7				;; B3 = B3 * sine (final I3)	; 3-7
this	vmovapd	y7, [srcreg+iter*srcinc+srcoff+4*d1+32] ;; i5

this	vsubpd	y1, y1, y2				;; r34s=r3-r4			; 4-6
this	vmulpd	y2, ymm13, y9				;; cos2*r25a			; 4-8
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1+32], y8	;; Save I2			; 4
this	vmovapd	y8, YMM_P809				;; -cos4

this	vsubpd	y3, y4, y7				;; i25s=i2-i5			; 5-7
prev	vmovapd	[srcreg+(iter-1)*srcinc+3*d1], y11	;; Save R4			; 5
this	vmulpd	y11, y8, y10				;; -cos4*r34a			; 5-9
prev	vmovapd	[srcreg+(iter-1)*srcinc+2*d1], y5	;; Save R3			; 6
this	vmovapd	y5, [srcreg+iter*srcinc+srcoff+2*d1+32] ;; i3

this	vaddpd	y4, y4, y7				;; i25a=i2+i5			; 6-8
this	vmulpd	y8, y8, y9				;; -cos4*r25a			; 6-10
prev	vmovapd	[srcreg+(iter-1)*srcinc+3*d1+32], y6	;; Save I4			; 7
this	vmovapd	y6, [srcreg+iter*srcinc+srcoff+3*d1+32] ;; i4

this	vsubpd	y7, y5, y6				;; i34s=i3-i4			; 7-9
prev	vmovapd	[srcreg+(iter-1)*srcinc+2*d1+32], y12	;; Save I3			; 8
this	vmulpd	y12, ymm13, y10				;; cos2*r34a			; 7-11

this	vaddpd	y5, y5, y6				;; i34a=i3+i4			; 8-10
this	vmulpd	y6, ymm14, y3				;; sin2*i25s			; 8-12
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

this	vaddpd	y9, y9, y10				;; r25a + r34a			; 9-11
this	vmulpd	y3, ymm15, y3				;; sin4*i25s			; 9-13

this	vsubpd	y2, y2, y11				;; cos2*r25a + cos4*r34a	; 10-12
this	vmulpd	y11, ymm15, y7				;; sin4*i34s			; 10-14

this	vmovapd	y10, [srcreg+iter*srcinc+srcoff] ;; r1
this	vsubpd	y8, y10, y8				;; t3 = r1 + cos4*r25a		; 11-13
this	vmulpd	y7, ymm14, y7				;; sin2*i34s			; 11-15

this	vaddpd	y9, y10, y9				;; outr(0) = r1 + r25a + r34a	; 12-14
this	vmovapd	[srcreg+iter*srcinc], y9		;; Save R1			; 15
this	vmulpd	y9, ymm13, y4				;; cos2*i25a			; 12-16

this	vaddpd	y2, y2, y10				;; t1=cos2*r25a + cos4*r34a + r1 ; 13-15
this	vmovapd	y10, YMM_P809				;; -cos4
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	vaddpd	y8, y8, y12				;; t3 += cos2*r34a		; 14-16
this	vmulpd	y12, y10, y5				;; -cos4*i34a			; 13-17
this	vmulpd	y10, y10, y4				;; -cos4*i25a			; 14-18

this	vaddpd	y6, y6, y11				;; t2=sin2*i25s + sin4*i34s	; 15-17
this	vmulpd	y11, ymm13, y5				;; cos2*i34a			; 15-19

this	vsubpd	y3, y3, y7				;; t4=sin4*i25s - sin2*i34s	; 16-18
this	vmulpd	y7, ymm14, y0				;; sin2*r25s			; 16-20

this	vaddpd	y4, y4, y5				;; i25a+i34a			; 17-19
this	vmulpd	y5, ymm15, y1				;; sin4*r34s			; 17-21

this	vsubpd	y9, y9, y12				;; cos2*i25a + cos4*i34a	; 18-20
this	vmulpd	y1, ymm14, y1				;; sin2*r34s			; 18-22
this	L1prefetchw srcreg+iter*srcinc+2*d1+L1pd, L1pt

this	vmovapd	y12, [srcreg+iter*srcinc+srcoff+32]	;; I1
this	vsubpd	y10, y12, y10				;; t7=i1 + cos4*i25a		; 19-21
this	vmulpd	y0, ymm15, y0				;; sin4*r25s			; 19-23

this	vaddpd	y4, y12, y4				;; outi(0)=i1+i25a+i34a		; 20-22
this	vmovapd	[srcreg+iter*srcinc+32], y4		;; Save I1
this no bcast vmovapd y4, [screg1+iter*scinc+cosoff]	;; cosine/sine
this bcast vbroadcastsd y4, Q [screg1+iter*scinc+cosoff] ;; cosine/sine

this	vaddpd	y9, y9, y12				;; t5=cos2*i25a + cos4*i34a + i1 ; 21-23

this	vaddpd	y10, y10, y11				;; t7+=cos2*i34a		; 22-24

this	vaddpd	y7, y7, y5				;; t6=sin2*r25s + sin4*r34s	; 23-25
this	L1prefetchw srcreg+iter*srcinc+3*d1+L1pd, L1pt

this	vsubpd	y0, y0, y1				;; t8=sin4*r25s - sin2*r34s	; 24-26

this	vaddpd	y1, y2, y6				;; outr(4)=t1+t2		; 25-27

this	vsubpd	y2, y2, y6				;; outr(1)=t1-t2		; 26-28

this	vsubpd	y6, y9, y7				;; outi(4)=t5-t6		; 27-29

this	vaddpd	y9, y9, y7				;; outi(1)=t5+t6		; 28-30
this	vmulpd	y7, y1, y4				;; A5 = R5 * cosine/sine	; 28-32
this	L1prefetchw srcreg+iter*srcinc+4*d1+L1pd, L1pt

this	vaddpd	y5, y8, y3				;; outr(3)=t3+t4		; 29-31
this	vmulpd	y11, y2, y4				;; A2 = R2 * cosine/sine	; 29-33

this	vsubpd	y8, y8, y3				;; outr(2)=t3-t4		; 30-32
this	vmulpd	y3, y6, y4				;; B5 = I5 * cosine/sine	; 30-34

this	vsubpd	y12, y10, y0				;; outi(3)=t7-t8		; 31-33
this	vmulpd	y4, y9, y4				;; B2 = I2 * cosine/sine	; 31-35

this	vaddpd	y10, y10, y0				;; outi(2)=t7+t8		; 32-34
this no bcast vmovapd y0, [screg2+iter*scinc+cosoff]	;; cosine/sine
this bcast vbroadcastsd y0, Q [screg2+iter*scinc+cosoff] ;; cosine/sine

this	vaddpd	y7, y7, y6				;; A5 = A5 + I5			; 33-35
this	vmulpd	y6, y5, y0				;; A4 = R4 * cosine/sine	; 32-36
this next yloop_unrolled_one

this	vsubpd	y11, y11, y9				;; A2 = A2 - I2			; 34-36
this	vmulpd	y9, y8, y0				;; A3 = R3 * cosine/sine	; 33-37

this	vsubpd	y3, y3, y1				;; B5 = B5 - R5			; 35-37
this	vmulpd	y1, y12, y0				;; B4 = I4 * cosine/sine	; 34-38
this	vmulpd	y0, y10, y0				;; B3 = I3 * cosine/sine	; 35-39

this	vaddpd	y4, y4, y2				;; B2 = B2 + R2			; 36-38
this no bcast vmovapd y2, [screg1+iter*scinc]		;; sine
this bcast vbroadcastsd y2, Q [screg1+iter*scinc]	;; sine
this	vmulpd	y7, y7, y2				;; A5 = A5 * sine (final R5)	; 36-40

this	vaddpd	y6, y6, y12				;; A4 = A4 + I4			; 37-39
this	vmulpd	y11, y11, y2				;; A2 = A2 * sine (final R2)	; 37-41
this no bcast vmovapd y12, [screg2+iter*scinc]		;; sine
this bcast vbroadcastsd y12, Q [screg2+iter*scinc]	;; sine

this	vsubpd	y9, y9, y10				;; A3 = A3 - I3			; 38-40
this	vmulpd	y3, y3, y2				;; B5 = B5 * sine (final I5)	; 38-42
next	vmovapd	y10, [srcreg+(iter+1)*srcinc+srcoff+d1]	;; r2

this	vsubpd	y1, y1, y5				;; B4 = B4 - R4			; 39-41
this	vmulpd	y4, y4, y2				;; B2 = B2 * sine (final I2)	; 39-43
next	vmovapd	y5, [srcreg+(iter+1)*srcinc+srcoff+4*d1] ;; r5

this	vaddpd	y0, y0, y8				;; B3 = B3 + R3			; 40-42
this	vmulpd	y6, y6, y12				;; A4 = A4 * sine (final R4)	; 40-44
next	vmovapd	y2, [srcreg+(iter+1)*srcinc+srcoff+2*d1] ;; r3

;; Shuffle register assignments so that y0-y12 in the next call has this r2, this r3, prev R5,
;; this r5, prev I5, prev A3, prev B4, prev sine, prev I2, free, prev R2, prev R4, prev B3.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y10
y10	TEXTEQU	y11
y11	TEXTEQU	y6
y6	TEXTEQU	y1
y1	TEXTEQU	y2
y2	TEXTEQU	y7
y7	TEXTEQU	y12
y12	TEXTEQU	ytmp
ytmp	TEXTEQU	y3
y3	TEXTEQU	y5
y5	TEXTEQU	y9
y9	TEXTEQU	y8
y8	TEXTEQU y4
y4	TEXTEQU ytmp

	ENDM

ENDIF

;;
;; ************************************* five-complex-djbunfft variants ******************************************
;;

;; The standard version
yr5_5cl_five_complex_djbunfft_preload MACRO
	yr5_5c_djbunfft_cmn_preload
	ENDM
yr5_5cl_five_complex_djbunfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	yr5_5c_djbunfft_cmn srcreg,srcinc,d1,noexec,screg,screg+64,32,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like 5cl but uses a ten-reals sin/cos table
yr5_r5cl_five_complex_djbunfft_preload MACRO
	yr5_5c_djbunfft_cmn_preload
	ENDM
yr5_r5cl_five_complex_djbunfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	yr5_5c_djbunfft_cmn srcreg,srcinc,d1,noexec,screg+64,screg+192,32,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like the 5cl version except vbroadcastsd is used to reduce sin/cos data
yr5_b5cl_five_complex_djbunfft_preload MACRO
	yr5_5c_djbunfft_cmn_preload
	ENDM
yr5_b5cl_five_complex_djbunfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	yr5_5c_djbunfft_cmn srcreg,srcinc,d1,exec,screg,screg+16,8,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like b5cl but extracts the sin/cos data to broadcast from
; the ten-real/five_complex sin/cos table
yr5_rb5cl_five_complex_djbunfft_preload MACRO
	yr5_5c_djbunfft_cmn_preload
	ENDM
yr5_rb5cl_five_complex_djbunfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	yr5_5c_djbunfft_cmn srcreg,srcinc,d1,exec,screg+8,screg+80,40,screg,scinc,maxrpt,L1pt,L1pd
	ENDM


;; Common code to do the 5-complex inverse FFT.
;; First we apply twiddle factors to 4 of the 5 input numbers.
;; A 5-complex inverse FFT is like the forward FFT except all the sin values are negated.

yr5_5c_djbunfft_cmn_preload MACRO
	ENDM
yr5_5c_djbunfft_cmn MACRO srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd	ymm2, [srcreg+d1]		;; Load R2
no bcast vmovapd ymm0, [screg1+cosoff]		;; cosine/sine
bcast	vbroadcastsd ymm0, Q [screg1+cosoff]	;; cosine/sine
	vmulpd	ymm1, ymm2, ymm0		;; A2 = R2 * cosine/sine
	vmovapd	ymm3, [srcreg+4*d1]		;; Load R5
	vmulpd	ymm7, ymm3, ymm0		;; A5 = R5 * cosine/sine
	vmovapd	ymm4, [srcreg+d1+32]		;; I2
	vaddpd	ymm1, ymm1, ymm4		;; A2 = A2 + I2
	vmovapd	ymm6, [srcreg+4*d1+32]		;; I5
	vsubpd	ymm7, ymm7, ymm6		;; A5 = A5 - I5
	vmulpd	ymm4, ymm4, ymm0		;; B2 = I2 * cosine/sine
	vmulpd	ymm6, ymm6, ymm0		;; B5 = I5 * cosine/sine
	vsubpd	ymm4, ymm4, ymm2		;; B2 = B2 - R2
	vaddpd	ymm6, ymm6, ymm3		;; B5 = B5 + R5
no bcast vmovapd ymm0, [screg1]			;; sine
bcast	vbroadcastsd ymm0, Q [screg1]		;; sine
	vmulpd	ymm1, ymm1, ymm0		;; A2 = A2 * sine (new R2)
	vmulpd	ymm4, ymm4, ymm0		;; B2 = B2 * sine (new I2)
	vmulpd	ymm7, ymm7, ymm0		;; A5 = A5 * sine (new R5)
	vmulpd	ymm6, ymm6, ymm0		;; B5 = B5 * sine (new I5)

	vsubpd	ymm2, ymm1, ymm7		;; r25s=r2-r5
	vaddpd	ymm1, ymm1, ymm7		;; r25a=r2+r5
	vsubpd	ymm7, ymm4, ymm6		;; i25s=i2-i5
	vaddpd	ymm4, ymm4, ymm6		;; i25a=i2+i5

	vmovapd	YMM_TMP1, ymm2			;; Save r2-r5
	vmovapd	YMM_TMP2, ymm4			;; Save i2+i5

	vmovapd	ymm4, [srcreg+3*d1]		;; Load R4
no bcast vmovapd ymm0, [screg2+cosoff]		;; cosine/sine
bcast	vbroadcastsd ymm0, Q [screg2+cosoff]	;; cosine/sine
	vmulpd	ymm5, ymm4, ymm0		;; A4 = R4 * cosine/sine
	vmovapd	ymm6, [srcreg+2*d1]		;; Load R3
	vmulpd	ymm3, ymm6, ymm0		;; A3 = R3 * cosine/sine
	vmovapd	ymm2, [srcreg+3*d1+32]		;; I4
	vsubpd	ymm5, ymm5, ymm2		;; A4 = A4 - I4
	vmulpd	ymm2, ymm2, ymm0		;; B4 = I4 * cosine/sine
	vaddpd	ymm2, ymm2, ymm4		;; B4 = B4 + R4
	vmovapd	ymm4, [srcreg+2*d1+32]		;; I3
	vaddpd	ymm3, ymm3, ymm4		;; A3 = A3 + I3
	vmulpd	ymm4, ymm4, ymm0		;; B3 = I3 * cosine/sine
	vsubpd	ymm4, ymm4, ymm6		;; B3 = B3 - R3
no bcast vmovapd ymm0, [screg2]			;; sine
bcast	vbroadcastsd ymm0, Q [screg2]		;; sine
	vmulpd	ymm5, ymm5, ymm0		;; A4 = A4 * sine (new R4)
	vmulpd	ymm2, ymm2, ymm0		;; B4 = B4 * sine (new I4)
	vmulpd	ymm3, ymm3, ymm0		;; A3 = A3 * sine (new R3)
	vmulpd	ymm4, ymm4, ymm0		;; B3 = B3 * sine (new I3)

	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	ymm6, ymm3, ymm5		;; r34s=r3-r4
	vaddpd	ymm3, ymm3, ymm5		;; r34a=r3+r4
	vmovapd	YMM_TMP3, ymm6			;; Save r3-r4
	vsubpd	ymm5, ymm4, ymm2		;; i34s=i3-i4
	vaddpd	ymm4, ymm4, ymm2		;; i34a=i3+i4
	vmovapd	YMM_TMP4, ymm4			;; Save i3+i4

	L1prefetchw srcreg+d1+L1pd, L1pt

	vmulpd	ymm0, ymm1, YMM_P309		;; cos2*r25a
	vmulpd	ymm2, ymm3, YMM_P809		;; -cos4*r34a
	vmulpd	ymm4, ymm7, YMM_P951		;; sin2*i25s
	vmulpd	ymm6, ymm5, YMM_P588		;; sin4*i34s
	vsubpd	ymm0, ymm0, ymm2		;; cos2*r25a + cos4*r34a
	vaddpd	ymm4, ymm4, ymm6		;; t2=sin2*i25s + sin4*i34s
	vmulpd	ymm2, ymm1, YMM_P809		;; -cos4*r25a
	vaddpd	ymm1, ymm1, ymm3		;; r25a + r34a
	vmovapd	ymm6, [srcreg]			;; r1
	vaddpd	ymm0, ymm0, ymm6		;; t1=cos2*r25a + cos4*r34a + r1
	vaddpd	ymm1, ymm1, ymm6		;; outr(0) = r1 + r25a + r34a
	vmulpd	ymm3, ymm3, YMM_P309		;; cos2*r34a
	vmulpd	ymm7, ymm7, YMM_P588		;; sin4*i25s
	vmulpd	ymm5, ymm5, YMM_P951		;; sin2*i34s
	vsubpd	ymm2, ymm3, ymm2		;; cos4*r25a+cos2*r34a
	vsubpd	ymm7, ymm7, ymm5		;; t4=sin4*i25s-sin2*i34s
	vaddpd	ymm2, ymm2, ymm6		;; t3=cos4*r25a+cos2*r34a+r1

	L1prefetchw srcreg+2*d1+L1pd, L1pt

	vsubpd	ymm6, ymm0, ymm4		;; outr(4)=t1-t2
	vaddpd	ymm0, ymm0, ymm4		;; outr(1)=t1+t2
	vsubpd	ymm4, ymm2, ymm7		;; outr(3)=t3-t4
	vaddpd	ymm2, ymm2, ymm7		;; outr(2)=t3+t4

	vmovapd	[srcreg], ymm1			;; Save R1
	vmovapd	[srcreg+d1], ymm0		;; Save R2
	vmovapd	[srcreg+2*d1], ymm2		;; Save R3
	vmovapd	[srcreg+3*d1], ymm4		;; Save R4
	vmovapd	[srcreg+4*d1], ymm6		;; Save R5

	vmovapd	ymm0, YMM_TMP1			;; r25s=r2-r5
	vmovapd	ymm2, YMM_TMP2			;; i25a=i2+i5
	vmovapd	ymm1, YMM_TMP3			;; r34s=r3-r4
	vmovapd	ymm3, YMM_TMP4			;; i34a=i3+i4

	L1prefetchw srcreg+3*d1+L1pd, L1pt

	vaddpd	ymm5, ymm2, ymm3		;; i25a+i34a
	vmovapd	ymm4, YMM_P309
	vmulpd	ymm6, ymm2, ymm4		;; cos2*i25a
	vmovapd	ymm7, YMM_P809
	vmulpd	ymm2, ymm2, ymm7		;; -cos4*i25a
	vmulpd	ymm7, ymm3, ymm7		;; -cos4*i34a
	vmulpd	ymm3, ymm3, ymm4		;; cos2*i34a

	L1prefetchw srcreg+4*d1+L1pd, L1pt

	vmulpd	ymm4, ymm0, YMM_P951		;; sin2*r25s
	vsubpd	ymm6, ymm6, ymm7		;; cos2*i25a + cos4*i34a
	vmovapd	ymm7, YMM_P588
	vmulpd	ymm0, ymm0, ymm7		;; sin4*r25s
	vmulpd	ymm7, ymm1, ymm7		;; sin4*r34s
	vmulpd	ymm1, ymm1, YMM_P951		;; sin2*r34s
	vsubpd	ymm2, ymm3, ymm2		;; cos4*i25a + cos2*i34a
	vmovapd	ymm3, [srcreg+32]		;; I1
	vaddpd	ymm6, ymm6, ymm3		;; t5=cos2*i25a + cos4*i34a + i1
	vaddpd	ymm4, ymm4, ymm7		;; t6=sin2*r25s + sin4*r34s
	vaddpd	ymm2, ymm2, ymm3		;; t7=cos4*i25a + cos2*i34a + i1
	vsubpd	ymm0, ymm0, ymm1		;; t8=sin4*r25s - sin2*r34s

	vsubpd	ymm7, ymm6, ymm4		;; outi(1)=t5-t6
	vaddpd	ymm6, ymm6, ymm4		;; outi(4)=t5+t6
	vsubpd	ymm1, ymm2, ymm0		;; outi(2)=t7-t8
	vaddpd	ymm2, ymm2, ymm0		;; outi(3)=t7+t8
	vaddpd	ymm5, ymm5, ymm3		;; outi(0)=i1+i25a+i34a

;;	vmovapd	[srcreg], ymm4		;; Save R1
	vmovapd	[srcreg+32], ymm5	;; Save I1
;;	vmovapd	[srcreg+d1], ymm3	;; Save R2
	vmovapd	[srcreg+d1+32], ymm7	;; Save I2
;;	vmovapd	[srcreg+2*d1], ymm3	;; Save R3
	vmovapd	[srcreg+2*d1+32], ymm1	;; Save I3
;;	vmovapd	[srcreg+3*d1], ymm1	;; Save R4
	vmovapd	[srcreg+3*d1+32], ymm2	;; Save I4
;;	vmovapd	[srcreg+4*d1], ymm1	;; Save R5
	vmovapd	[srcreg+4*d1+32], ymm6	;; Save I5
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; 64-bit version

IFDEF X86_64

yr5_5c_djbunfft_cmn_preload MACRO
	vmovapd	ymm13, YMM_P309			;; cos2
	vmovapd	ymm14, YMM_P951			;; sin2
	vmovapd	ymm15, YMM_P588			;; sin4
	ENDM

yr5_5c_djbunfft_cmn MACRO srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	yr5_5c_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 3,exec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 4,exec,exec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 5,exec,noexec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 5*srcinc
	bump	screg, 5*scinc
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	yr5_5c_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 3,exec,exec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 4,exec,noexec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	screg, 4*scinc
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	yr5_5c_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 2,exec,exec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 3,exec,noexec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	screg, 3*scinc
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	yr5_5c_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 1,exec,exec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 2,exec,noexec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	screg, 2*scinc
	ELSE
	yr5_5c_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 0,noexec,exec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	yr5_5c_djbunfft_unroll 1,exec,noexec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDIF
	ENDM

yr5_5c_djbunfft_unroll MACRO iter,prev,this,next,srcreg,srcinc,d1,bcast,screg1,screg2,cosoff,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<ymm0>
y1	TEXTEQU	<ymm1>
y2	TEXTEQU	<ymm2>
y3	TEXTEQU	<ymm3>
y4	TEXTEQU	<ymm4>
y5	TEXTEQU	<ymm5>
y6	TEXTEQU	<ymm6>
y7	TEXTEQU	<ymm7>
y8	TEXTEQU	<ymm8>
y9	TEXTEQU	<ymm9>
y10	TEXTEQU	<ymm10>
y11	TEXTEQU	<ymm11>
y12	TEXTEQU	<ymm12>
	ENDIF

;; On later calls, y0-y11 contain this A2,A3,A4,A5,B2,R2,R3,R4,R5,I2,cos/sin1,cos/sin2.
;; y12 contains prev R3.  ymm13, ymm14 and ymm15 are preloaded constants cos2, sin2 and sin4.

this	vaddpd	y0, y0, y9				;; A2 = A2 + I2			; 1-3
this	vmovapd	y9, [srcreg+iter*srcinc+4*d1+32]	;; I5
this	vmulpd	y10, y9, y10				;; B5 = I5 * cosine/sine	; 1-5

this	vsubpd	y3, y3, y9				;; A5 = A5 - I5			; 2-4
this	vmovapd	y9, [srcreg+iter*srcinc+3*d1+32]	;; I4
prev	vmovapd	[srcreg+(iter-1)*srcinc+2*d1], y12	;; Save R3			; 3
this	vmulpd	y12, y9, y11				;; B4 = I4 * cosine/sine	; 2-6

this	vsubpd	y2, y2, y9				;; A4 = A4 - I4			; 3-5
this	vmovapd	y9, [srcreg+iter*srcinc+2*d1+32]	;; I3
this	vmulpd	y11, y9, y11				;; B3 = I3 * cosine/sine	; 3-7

this	vaddpd	y1, y1, y9				;; A3 = A3 + I3			; 4-6
this no bcast vmovapd y9, [screg1+iter*scinc]		;; sine
this bcast vbroadcastsd y9, Q [screg1+iter*scinc]	;; sine
this	vmulpd	y0, y0, y9				;; A2 = A2 * sine (new R2)	; 4-8

this	vsubpd	y4, y4, y5				;; B2 = B2 - R2			; 5-7
this	vmulpd	y3, y3, y9				;; A5 = A5 * sine (new R5)	; 5-9
this no bcast vmovapd y5, [screg2+iter*scinc]		;; sine
this bcast vbroadcastsd y5, Q [screg2+iter*scinc]	;; sine

this	vaddpd	y10, y10, y8				;; B5 = B5 + R5			; 6-8
this	vmulpd	y2, y2, y5				;; A4 = A4 * sine (new R4)	; 6-10
this	vmovapd	y8, YMM_P809				;; -cos4

this	vaddpd	y12, y12, y7				;; B4 = B4 + R4			; 7-9
this	vmulpd	y1, y1, y5				;; A3 = A3 * sine (new R3)	; 7-11
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

this	vsubpd	y11, y11, y6				;; B3 = B3 - R3			; 8-10
this	vmulpd	y4, y4, y9				;; B2 = B2 * sine (new I2)	; 8-12

													;; Empty ADD slot :(
this	vmulpd	y10, y10, y9				;; B5 = B5 * sine (new I5)	; 9-13

this	vaddpd	y9, y0, y3				;; r25a=r2+r5			; 10-12
this	vmulpd	y12, y12, y5				;; B4 = B4 * sine (new I4)	; 10-14
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	vsubpd	y0, y0, y3				;; r25s=r2-r5			; 11-13
this	vmulpd	y11, y11, y5				;; B3 = B3 * sine (new I3)	; 11-15

this	vaddpd	y5, y1, y2				;; r34a=r3+r4			; 12-14

this	vsubpd	y1, y1, y2				;; r34s=r3-r4			; 13-15
this	vmulpd	y2, ymm13, y9				;; cos2*r25a			; 13-17
this	L1prefetchw srcreg+iter*srcinc+2*d1+L1pd, L1pt

this	vaddpd	y3, y4, y10				;; i25a=i2+i5			; 14-16
this	vmulpd	y6, y8, y9				;; -cos4*r25a			; 14-18

this	vaddpd	y9, y9, y5				;; r25a + r34a			; 15-17

this	vaddpd	y7, y11, y12				;; i34a=i3+i4			; 16-18
this	L1prefetchw srcreg+iter*srcinc+3*d1+L1pd, L1pt

this	vsubpd	y4, y4, y10				;; i25s=i2-i5			; 17-19
this	vmulpd	y10, y8, y5				;; -cos4*r34a			; 15-19
this	vmulpd	y5, ymm13, y5				;; cos2*r34a			; 16-20

this	vsubpd	y11, y11, y12				;; i34s=i3-i4			; 18-20
this	vmovapd	y12, [srcreg+iter*srcinc]		;; r1

this	vaddpd	y9, y12, y9				;; outr(0) = r1 + r25a + r34a	; 19-21
this	vmovapd	[srcreg+iter*srcinc], y9		;; Save R1
this	vmulpd	y9, ymm13, y3				;; cos2*i25a			; 17-21

this	vsubpd	y2, y2, y10				;; cos2*r25a + cos4*r34a	; 20-22
this	vmulpd	y10, y8, y3				;; -cos4*i25a			; 18-22
this	vmulpd	y8, y8, y7				;; -cos4*i34a			; 19-23

this	vsubpd	y5, y5, y6				;; cos4*r25a + cos2*r34a	; 21-23
this	vmulpd	y6, ymm13, y7				;; cos2*i34a			; 20-24
this	L1prefetchw srcreg+iter*srcinc+4*d1+L1pd, L1pt

this	vaddpd	y3, y3, y7				;; i25a+i34a			; 22-24
this	vmulpd	y7, ymm14, y0				;; sin2*r25s			; 21-25
this	vmulpd	y0, ymm15, y0				;; sin4*r25s			; 22-26

this	vaddpd	y2, y2, y12				;; t1=cos2*r25a + cos4*r34a + r1 ; 23-25

this	vaddpd	y5, y5, y12				;; t3=cos4*r25a + cos2*r34a + r1 ; 24-26
this	vmulpd	y12, ymm15, y1				;; sin4*r34s			; 23-27
this	vmulpd	y1, ymm14, y1				;; sin2*r34s			; 24-28

this	vsubpd	y9, y9, y8				;; cos2*i25a + cos4*i34a	; 25-27
this	vmulpd	y8, ymm14, y4				;; sin2*i25s			; 25-29

this	vsubpd	y6, y6, y10				;; cos4*i25a + cos2*i34a	; 26-28
this	vmovapd	y10, [srcreg+iter*srcinc+32]		;; I1

this	vaddpd	y3, y10, y3				;; outi(0) = i1 + i25a + i34a	; 27-29
this	vmovapd	[srcreg+iter*srcinc+32], y3		;; Save I1
this	vmulpd	y3, ymm15, y11				;; sin4*i34s			; 26-30
this	vmulpd	y4, ymm15, y4				;; sin4*i25s			; 27-31

this	vaddpd	y9, y9, y10				;; t5=cos2*i25a + cos4*i34a + i1 ; 28-30
this	vmulpd	y11, ymm14, y11				;; sin2*i34s			; 28-32

this	vaddpd	y7, y7, y12				;; t6=sin2*r25s + sin4*r34s	; 29-31
next no bcast vmovapd y12, [screg1+(iter+1)*scinc+cosoff] ;; cosine/sine
next bcast vbroadcastsd y12, Q [screg1+(iter+1)*scinc+cosoff] ;; cosine/sine

this	vaddpd	y6, y6, y10				;; t7=cos4*i25a + cos2*i34a + i1 ; 30-32
next	vmovapd	y10, [srcreg+(iter+1)*srcinc+d1]	;; R2

this	vsubpd	y0, y0, y1				;; t8=sin4*r25s - sin2*r34s	; 31-33
next	vmovapd	y1, [srcreg+(iter+1)*srcinc+4*d1]	;; R5

this	vaddpd	y8, y8, y3				;; t2=sin2*i25s + sin4*i34s	; 32-34
next	vmovapd	y3, [srcreg+(iter+1)*srcinc+3*d1]	;; R4

this	vsubpd	y4, y4, y11				;; t4=sin4*i25s - sin2*i34s	; 33-35
this next yloop_unrolled_one

this	vsubpd	y11, y9, y7				;; outi(1)=t5-t6		; 34-36
this	vmovapd	[srcreg+iter*srcinc+d1+32], y11		;; Save I2			; 37
next no bcast vmovapd y11, [screg2+(iter+1)*scinc+cosoff] ;; cosine/sine
next bcast vbroadcastsd y11, Q [screg2+(iter+1)*scinc+cosoff] ;; cosine/sine

this	vaddpd	y9, y9, y7				;; outi(4)=t5+t6		; 35-37
this	vmovapd	[srcreg+iter*srcinc+4*d1+32], y9	;; Save I5			; 38
next	vmovapd	y9, [srcreg+(iter+1)*srcinc+2*d1]	;; R3

this	vsubpd	y7, y2, y8				;; outr(4)=t1-t2		; 36-38
this	vmovapd	[srcreg+iter*srcinc+4*d1], y7		;; Save R5			; 39
next	vmovapd	y7, [srcreg+(iter+1)*srcinc+d1+32]	;; I2

this	vaddpd	y2, y2, y8				;; outr(1)=t1+t2		; 37-39
this	vmovapd	[srcreg+iter*srcinc+d1], y2		;; Save R2			; 40
next	vmulpd	y2, y10, y12				;; A2 = R2 * cosine/sine	; 37-41

this	vsubpd	y8, y6, y0				;; outi(2)=t7-t8		; 38-40
this	vmovapd	[srcreg+iter*srcinc+2*d1+32], y8	;; Save I3			; 41
next	vmulpd	y8, y1, y12				;; A5 = R5 * cosine/sine	; 38-42

this	vaddpd	y6, y6, y0				;; outi(3)=t7+t8		; 39-41
this	vmovapd	[srcreg+iter*srcinc+3*d1+32], y6	;; Save I4			; 42
next	vmulpd	y6, y3, y11				;; A4 = R4 * cosine/sine	; 39-43

this	vsubpd	y0, y5, y4				;; outr(3)=t3-t4		; 40-42
this	vmovapd	[srcreg+iter*srcinc+3*d1], y0		;; Save R4			; 43
next	vmulpd	y0, y9, y11				;; A3 = R3 * cosine/sine	; 40-44

this	vaddpd	y5, y5, y4				;; outr(2)=t3+t4		; 41-43
next	vmulpd	y4, y7, y12				;; B2 = I2 * cosine/sine	; 41-45

;; Shuffle register assignments so that y0-y11 in the next call has this
;; A2,A3,A4,A5,B2,R2,R3,R4,R5,I2,cos/sin1,cos/sin2. and y12 contains prev R3.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y2
y2	TEXTEQU	y6
y6	TEXTEQU	y9
y9	TEXTEQU	y7
y7	TEXTEQU	y3
y3	TEXTEQU	y8
y8	TEXTEQU	y1
y1	TEXTEQU	ytmp
ytmp	TEXTEQU	y5
y5	TEXTEQU	y10
y10	TEXTEQU	y12
y12	TEXTEQU ytmp

	ENDM

ENDIF


;;
;; ************************************* ten-reals-fft variants ******************************************
;;

yr5_5cl_ten_reals_fft_preload MACRO
	yr5_10r_fft_cmn_preload
	ENDM
yr5_5cl_ten_reals_fft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	yr5_10r_fft_cmn srcreg,srcinc,0,d1,screg,screg+64,screg+128,screg+192,screg,scinc,,,maxrpt,L1pt,L1pd
	ENDM

; Like the 5cl version but uses two sin/cos pointers
yr5_5cl_2sc_ten_reals_fft_preload MACRO
	yr5_10r_fft_cmn_preload
	ENDM
yr5_5cl_2sc_ten_reals_fft MACRO srcreg,srcinc,d1,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	yr5_10r_fft_cmn srcreg,srcinc,0,d1,screg2,screg1,screg2+64,screg1+64,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM

; Like the 5cl version but the sin/cos data can be used by a five_complex macro at the same FFT level
yr5_5cl_csc_ten_reals_fft_preload MACRO
	yr5_10r_fft_cmn_preload
	ENDM
yr5_5cl_csc_ten_reals_fft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	yr5_10r_fft_cmn srcreg,srcinc,0,d1,screg+128,screg,screg+192,screg+64,screg,scinc,,,maxrpt,L1pt,L1pd
	ENDM

; Like the 5cl version but offsets source by rbx
yr5_f5cl_ten_reals_fft_preload MACRO
	yr5_10r_fft_cmn_preload
	ENDM
yr5_f5cl_ten_reals_fft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	yr5_10r_fft_cmn srcreg,srcinc,rbx,d1,screg,screg+64,screg+128,screg+192,screg,scinc,,,maxrpt,L1pt,L1pd
	ENDM

; Like the f5cl version but uses two sin/cos ptrs
yr5_f5cl_2sc_ten_reals_fft_preload MACRO
	yr5_10r_fft_cmn_preload
	ENDM
yr5_f5cl_2sc_ten_reals_fft MACRO srcreg,srcinc,d1,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	yr5_10r_fft_cmn srcreg,srcinc,rbx,d1,screg2,screg1,screg2+64,screg1+64,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM

;; To calculate a 10-reals FFT (in a shorthand notation):
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0000000000
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0123456789
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0246802468
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0369258147
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0482604826
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0505050505
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0628406284
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0741852963
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0864208642
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0987654321
;; Noting that w^5 = -1 and that Hermetian symmetry means we won't need
;; to calculate the last 5 rows:
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0000000000
;; r1 + r2 + r3 + r4 + r5 - r6 - r7 - r8 - r9 - r10	*  w^0123401234
;; r1 + r2 + r3 - r4 - r5 + r6 + r7 + r8 - r9 - r10	*  w^0241302413
;; r1 + r2 - r3 - r4 + r5 - r6 - r7 + r8 + r9 - r10	*  w^0314203142
;; r1 + r2 - r3 + r4 - r5 + r6 + r7 - r8 + r9 - r10	*  w^0432104321
;; Reorganize into odds and evens, that is
;; r1 + r3 + r5 + r7 + r9  * powers + (r2 + r4 + r6 + r8 + r10) * powers
;; Thus:
;; r1 + r3 + r5 + r7 + r9	*  w^00000	+ r2 + r4 + r6 + r8 + r10	*  w^00000
;; r1 + r3 + r5 - r7 - r9	*  w^02413	+ r2 + r4 - r6 - r8 - r10	*  w^13024
;; r1 + r3 - r5 + r7 - r9	*  w^04321	+ r2 - r4 + r6 + r8 - r10	*  w^21043
;; r1 - r3 + r5 - r7 + r9	*  w^01234	+ r2 - r4 - r6 + r8 - r10	*  w^34012
;; r1 - r3 - r5 + r7 + r9	*  w^03142	+ r2 + r4 + r6 - r8 - r10	*  w^42031
;; Apply the sin/cos values:
;; w^1/10 = .809 + .588i
;; w^2/10 = .309 + .951i
;; w^3/10 = -.309 + .951i
;; w^4/10 = -.809 + .588i
;; reals:
;; r1 + r3 + r5 + r7 + r9			+ r2 + r4 + r6 + r8 + r10
;; r1 + .309r3 - .809r5 - .809r7 + .309r9	+ .809r2 - .309r4 - r6 - .309r8 + .809r10
;; r1 - .809r3 + .309r5 + .309r7 - .809r9	+ .309r2 - .809r4 + r6 - .809r8 + .309r10
;; r1 - .809r3 + .309r5 + .309r7 - .809r9	+ -.309r2 + .809r4 - r6 + .809r8 - .309r10
;; r1 + .309r3 - .809r5 - .809r7 + .309r9	+ -.809r2 + .309r4 + r6 + .309r8 - .809r10
;; imaginarys:
;; 0						+ 0
;;  + .951r3 + .588r5 - .588r7 - .951r9		+ .588r2 + .951r4 - .951r8 - .588r10
;;  + .588r3 - .951r5 + .951r7 - .588r9		+ .951r2 - .588r4 + .588r8 - .951r10
;;  - .588r3 + .951r5 - .951r7 + .588r9		+ .951r2 - .588r4 + .588r8 - .951r10
;;  - .951r3 - .588r5 + .588r7 + .951r9		+ .588r2 + .951r4 - .951r8 - .588r10
;; Further simplifying:
;; reals:
;; r1 + r3 + r5 + r7 + r9		+ r2 + r4 + r6 + r8 + r10
;; r1 + .309(r3+r9) - .809(r5+r7)	+ .809(r2+r10) - .309(r4+r8) - r6
;; r1 - .809(r3+r9) + .309(r5+r7)	+ .309(r2+r10) - .809(r4+r8) + r6
;; r1 - .809(r3+r9) + .309(r5+r7)	+ -.309(r2+r10) + .809(r4+r8) - r6
;; r1 + .309(r3+r9) - .809(r5+r7)	+ -.809(r2+r10) + .309(r4+r8) + r6
;; imaginarys:
;; 0					+ 0
;;  + .951(r3-r9) + .588(r5-r7)		+ .588(r2-r10) + .951(r4-r8)
;;  + .588(r3-r9) - .951(r5-r7)		+ .951(r2-r10) - .588(r4-r8)
;;  - .588(r3-r9) + .951(r5-r7)		+ .951(r2-r10) - .588(r4-r8)
;;  - .951(r3-r9) - .588(r5-r7)		+ .588(r2-r10) + .951(r4-r8)

yr5_10r_fft_cmn_preload MACRO
	ENDM
yr5_10r_fft_cmn MACRO srcreg,srcinc,srcoff,d1,screg1,screg2,screg3,screg4,scregA,scincA,scregB,scincB,maxrpt,L1pt,L1pd
	vmovapd	ymm0, [srcreg+srcoff+2*d1]	;; R3
	vmovapd ymm1, [srcreg+srcoff+3*d1+32]	;; R9
	vsubpd	ymm7, ymm0, ymm1		;; R3-R9
	vaddpd	ymm0, ymm0, ymm1		;; R3+R9
	vmovapd	ymm3, [srcreg+srcoff+4*d1]	;; R5
	vmovapd ymm4, [srcreg+srcoff+d1+32]	;; R7
	vsubpd	ymm6, ymm3, ymm4		;; R5-R7
	vaddpd	ymm3, ymm3, ymm4		;; R5+R7
	vmovapd	ymm1, YMM_P951
	vmulpd	ymm2, ymm1, ymm7		;; new oddI2 = .951*(R3-R9)
	vmovapd	ymm5, YMM_P588
	vmulpd	ymm7, ymm5, ymm7		;; new oddI3 = .588*(R3-R9)
	vmulpd	ymm5, ymm5, ymm6		;; .588*(R5-R7)
	vmulpd	ymm1, ymm1, ymm6		;; .951*(R5-R7)
	vaddpd	ymm2, ymm2, ymm5		;; new oddI2 += .588*(R5-R7)
	vsubpd	ymm7, ymm7, ymm1		;; new oddI3 -= .951*(R5-R7)
	vmovapd	ymm1, YMM_P309
	vmulpd	ymm5, ymm1, ymm0		;; new oddR2 = .309*(R3+R9)
	vmovapd	ymm4, [srcreg+srcoff]		;; R1
	vaddpd	ymm5, ymm5, ymm4		;; new oddR2 += R1
	vmovapd	ymm6, YMM_P809
	vmovapd	YMM_TMP6, ymm2			;; Save oddI2
	vmulpd	ymm2, ymm6, ymm0		;; new -oddR3 = -.809*(R3+R9)
	vaddpd	ymm0, ymm4, ymm0		;; new oddR1 = R1+R3+R9
	vsubpd	ymm2, ymm4, ymm2		;; new oddR3 += R1
	vmulpd	ymm4, ymm6, ymm3		;; --.809*(R5+R7)
	vaddpd	ymm0, ymm0, ymm3		;; new oddR1 = R1+R3+R5+R7+R9
	vmulpd	ymm3, ymm1, ymm3		;; .309*(R5+R7)
	vsubpd	ymm5, ymm5, ymm4		;; new oddR2 += -.809*(R5+R7)
	vaddpd	ymm2, ymm2, ymm3		;; new oddR3 += .309*(R5+R7)
	vmovapd	YMM_TMP8, ymm7			;; Save oddI3
	vmovapd	[srcreg], ymm0			;; Save oddR1
	vmovapd	YMM_TMP1, ymm5			;; Save oddR2
	vmovapd	YMM_TMP3, ymm2			;; Save oddR3

	vmovapd ymm2, [srcreg+srcoff+d1]	;; R2
	vmovapd	ymm5, [srcreg+srcoff+4*d1+32]	;; R10
	vsubpd	ymm7, ymm2, ymm5		;; R2-R10
	vaddpd	ymm2, ymm2, ymm5		;; R2+R10
	vmovapd ymm4, [srcreg+srcoff+3*d1]	;; R4
	vmovapd	ymm3, [srcreg+srcoff+2*d1+32]	;; R8
	vsubpd	ymm6, ymm4, ymm3		;; R4-R8
	vaddpd	ymm4, ymm4, ymm3		;; R4+R8

	L1prefetchw srcreg+L1pd, L1pt

	vmovapd	ymm0, YMM_P951
	vmulpd	ymm1, ymm0, ymm7		;; new evenI3 = .951*(R2-R10)
	vmovapd	ymm3, YMM_P588
	vmulpd	ymm7, ymm3, ymm7		;; new evenI2 = .588*(R2-R10)
	vmulpd	ymm3, ymm3, ymm6		;; .588*(R4-R8)
	vmulpd	ymm0, ymm0, ymm6		;; .951*(R4-R8)
	vsubpd	ymm1, ymm1, ymm3		;; new evenI3 -= .588*(R4-R8)
	vaddpd	ymm7, ymm7, ymm0		;; new evenI2 += .951*(R4-R8)

	vmulpd	ymm3, ymm2, YMM_P309		;; new evenR3 = .309*(R2+R10)
	vmovapd	ymm5, [srcreg+srcoff+32]	;; R6
	vaddpd	ymm3, ymm3, ymm5		;; new evenR3 += R6
	vmovapd	ymm6, YMM_P809
	vmulpd	ymm0, ymm6, ymm2		;; new -evenR5 = -.809*(R2+R10)
	vaddpd	ymm2, ymm5, ymm2		;; new evenR1 = R6+R2+R10
	vsubpd	ymm0, ymm5, ymm0		;; new evenR5 += R6
	vmulpd	ymm6, ymm6, ymm4		;; --.809*(R4+R8)
	vaddpd	ymm2, ymm2, ymm4		;; new evenR1 = R6+R2+R4+R8+R10
	vmulpd	ymm4, ymm4, YMM_P309		;; .309*(R4+R8)
	vsubpd	ymm3, ymm3, ymm6		;; new evenR3 += -.809*(R4+R8)
	vaddpd	ymm0, ymm0, ymm4		;; new evenR5 += .309*(R4+R8)
	vmovapd	[srcreg+32], ymm2		;; Save evenR1

	L1prefetchw srcreg+d1+L1pd, L1pt

	vmovapd	ymm2, YMM_TMP1			;; oddR2
	vaddpd	ymm4, ymm2, ymm0		;; New R5 = oddR2 + evenR5
	vmovapd	ymm6, YMM_TMP6			;; oddI2
	vsubpd	ymm5, ymm7, ymm6		;; New I5 = evenI2 - oddI2
	vsubpd	ymm2, ymm2, ymm0		;; New R2 = oddR2 - evenR5
	vaddpd	ymm7, ymm7, ymm6		;; New I2 = evenI2 + oddI2

	vmovapd	ymm0, [screg4+32]		;; cosine/sine for w^4
	vmulpd	ymm6, ymm4, ymm0		;; A5 = R5 * cosine/sine
	vsubpd	ymm6, ymm6, ymm5		;; A5 = A5 - I5
	vmulpd	ymm5, ymm5, ymm0		;; B5 = I5 * cosine/sine
	vaddpd	ymm5, ymm5, ymm4		;; B5 = B5 + R5

	vmovapd	ymm0, [screg1+32]		;; cosine/sine for w^1
	vmulpd	ymm4, ymm2, ymm0		;; A2 = R2 * cosine/sine
	vsubpd	ymm4, ymm4, ymm7		;; A2 = A2 - I2
	vmulpd	ymm7, ymm7, ymm0		;; B2 = I2 * cosine/sine
	vaddpd	ymm7, ymm7, ymm2		;; B2 = B2 + R2

	L1prefetchw srcreg+2*d1+L1pd, L1pt

	vmovapd	ymm2, [screg4]
	vmulpd	ymm6, ymm6, ymm2		;; A5 = A5 * sine (final R5)
	vmulpd	ymm5, ymm5, ymm2		;; B5 = B5 * sine (final I5)
	vmovapd	ymm2, [screg1]
	vmulpd	ymm4, ymm4, ymm2		;; A2 = A2 * sine (final R2)
	vmulpd	ymm7, ymm7, ymm2		;; B2 = B2 * sine (final I2)

	vmovapd	[srcreg+d1], ymm4		;; Save final R2
	vmovapd	[srcreg+d1+32], ymm7		;; Save final I2

	vmovapd	ymm4, YMM_TMP3			;; oddR3
	vsubpd	ymm0, ymm4, ymm3		;; New R4 = oddR3 - evenR3
	vaddpd	ymm4, ymm4, ymm3		;; New R3 = oddR3 + evenR3
	vmovapd	ymm7, YMM_TMP8			;; oddI3
	vsubpd	ymm2, ymm1, ymm7		;; New I4 = evenI3 - oddI3
	vaddpd	ymm1, ymm1, ymm7		;; New I3 = evenI3 + oddI3

	L1prefetchw srcreg+3*d1+L1pd, L1pt

	vmovapd	ymm3, [screg3+32]		;; cosine/sine for w^3
	vmulpd	ymm7, ymm0, ymm3		;; A4 = R4 * cosine/sine
	vsubpd	ymm7, ymm7, ymm2		;; A4 = A4 - I4
	vmulpd	ymm2, ymm2, ymm3		;; B4 = I4 * cosine/sine
	vaddpd	ymm2, ymm2, ymm0		;; B4 = B4 + R4

	vmovapd	ymm3, [screg2+32]		;; cosine/sine for w^2
	vmulpd	ymm0, ymm4, ymm3		;; A3 = R3 * cosine/sine
	vsubpd	ymm0, ymm0, ymm1		;; A3 = A3 - I3
	vmulpd	ymm1, ymm1, ymm3		;; B3 = I3 * cosine/sine
	vaddpd	ymm1, ymm1, ymm4		;; B3 = B3 + R3

	L1prefetchw srcreg+4*d1+L1pd, L1pt

	vmovapd	ymm4, [screg3]
	vmulpd	ymm7, ymm7, ymm4		;; A4 = A4 * sine (final R4)
	vmulpd	ymm2, ymm2, ymm4		;; B4 = B4 * sine (final I4)
	vmovapd	ymm4, [screg2]
	vmulpd	ymm0, ymm0, ymm4		;; A3 = A3 * sine (final R3)
	vmulpd	ymm1, ymm1, ymm4		;; B3 = B3 * sine (final I3)

	vmovapd	[srcreg+2*d1], ymm0		;; Save R3
	vmovapd	[srcreg+2*d1+32], ymm1		;; Save I3
	vmovapd	[srcreg+3*d1], ymm7		;; Save R4
	vmovapd	[srcreg+3*d1+32], ymm2		;; Save I4
	vmovapd	[srcreg+4*d1], ymm6		;; Save R5
	vmovapd	[srcreg+4*d1+32], ymm5		;; Save I5

	bump	srcreg, srcinc
	bump	scregA, scincA
	bump	scregB, scincB
	ENDM


;;
;; ************************************* ten-reals-unfft variants ******************************************
;;

;; The standard version
yr5_5cl_ten_reals_unfft_preload MACRO
	yr5_10r_unfft_cmn_preload
	ENDM
yr5_5cl_ten_reals_unfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	yr5_10r_unfft_cmn srcreg,srcinc,d1,screg,screg+64,screg+128,screg+192,screg,scinc,,,maxrpt,L1pt,L1pd
	ENDM

;; Like 5cl except that two sin/cos ptrs are used
yr5_5cl_2sc_ten_reals_unfft_preload MACRO
	yr5_10r_unfft_cmn_preload
	ENDM
yr5_5cl_2sc_ten_reals_unfft MACRO srcreg,srcinc,d1,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	yr5_10r_unfft_cmn srcreg,srcinc,d1,screg2,screg1,screg2+64,screg1+64,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM

;; Like 5cl except that five_complex macros at the same FFT level can use the sin/cos data
yr5_5cl_csc_ten_reals_unfft_preload MACRO
	yr5_10r_unfft_cmn_preload
	ENDM
yr5_5cl_csc_ten_reals_unfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	yr5_10r_unfft_cmn srcreg,srcinc,d1,screg+128,screg,screg+192,screg+64,screg,scinc,,,maxrpt,L1pt,L1pd
	ENDM

;; To calculate a 10-reals unFFT (in a shorthand notation):
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0000000000
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0123456789
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0246802468
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0369258147
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0482604826
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0505050505
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0628406284
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0741852963
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0864208642
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0987654321
;; Noting that w^5 = -1
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0000000000
;; c1 + c2 + c3 + c4 + c5 - c6 - c7 - c8 - c9 - c10	*  w^0123401234
;; c1 + c2 + c3 - c4 - c5 + c6 + c7 + c8 - c9 - c10	*  w^0241302413
;; c1 + c2 - c3 - c4 + c5 - c6 - c7 + c8 + c9 - c10	*  w^0314203142
;; c1 + c2 - c3 + c4 - c5 + c6 + c7 - c8 + c9 - c10	*  w^0432104321
;; c1 - c2 + c3 - c4 + c5 - c6 + c7 - c8 + c9 - c10	*  w^0000000000
;; c1 - c2 + c3 - c4 + c5 + c6 - c7 + c8 - c9 + c10	*  w^0123401234
;; c1 - c2 + c3 + c4 - c5 - c6 + c7 - c8 - c9 + c10	*  w^0241302413
;; c1 - c2 - c3 + c4 + c5 + c6 - c7 - c8 + c9 + c10	*  w^0314203142
;; c1 - c2 - c3 - c4 - c5 - c6 + c7 + c8 + c9 + c10	*  w^0432104321
;; incoming is:	r1_1 = c1r + c6r
;;		c2 = c2r + c2i
;;		c3 = c3r + c3i
;;		c4 = c4r + c4i
;;		c5 = c5r + c5i
;;		r1_2 = c1r - c6r
;;		c7 = c5r - c5i	(implied)
;;		c8 = c4r - c4i	(implied)
;;		c9 = c3r - c3i	(implied)
;;		c10 = c2r - c2i	(implied)
;; And noticing the signs of the real and imaginary parts of the sin/cos values:
;; w^1/10 = .809 - .588i
;; w^2/10 = .309 - .951i
;; w^3/10 = -.309 - .951i
;; w^4/10 = -.809 - .588i
;; We get reals:
;; r1_1 + 2c2 + 2c3 + 2c4 + 2c5	*  w^00000
;; r1_2 + 2c2 + 2c3 + 2c4 + 2c5	*  w^01234
;; r1_1 + 2c2 + 2c3 - 2c4 - 2c5	*  w^02413
;; r1_2 + 2c2 - 2c3 - 2c4 + 2c5	*  w^03142
;; r1_1 + 2c2 - 2c3 + 2c4 - 2c5	*  w^04321
;; r1_2 - 2c2 + 2c3 - 2c4 + 2c5	*  w^00000
;; r1_1 - 2c2 + 2c3 - 2c4 + 2c5	*  w^01234
;; r1_2 - 2c2 + 2c3 + 2c4 - 2c5	*  w^02413
;; r1_1 - 2c2 - 2c3 + 2c4 + 2c5	*  w^03142
;; r1_2 - 2c2 - 2c3 - 2c4 - 2c5	*  w^04321
;; Now drop the multiplication by 2 (the actual r1_1 and r1_2 inputs are already doubled)
;; and expand the sin/cos multipliers:
;; r1_1 + c2r + c3r + c4r + c5r
;; r1_2 + .809c2r + .588c2i + .309c3r + .951c3i - .309c4r + .951c4i - .809c5r + .588c5i
;; r1_1 + .309c2r + .951c2i - .809c3r + .588c3i - .809c4r - .588c4i + .309c5r - .951c5i
;; r1_2 - .309c2r + .951c2i - .809c3r - .588c3i + .809c4r - .588c4i + .309c5r + .951c5i
;; r1_1 - .809c2r + .588c2i + .309c3r - .951c3i + .309c4r + .951c4i - .809c5r - .588c5i
;; r1_2 - c2r + c3r - c4r + c5r
;; r1_1 - .809c2r - .588c2i + .309c3r + .951c3i + .309c4r - .951c4i - .809c5r + .588c5i
;; r1_2 - .309c2r - .951c2i - .809c3r + .588c3i + .809c4r + .588c4i + .309c5r - .951c5i
;; r1_1 + .309c2r - .951c2i - .809c3r - .588c3i - .809c4r + .588c4i + .309c5r + .951c5i
;; r1_2 + .809c2r - .588c2i + .309c3r - .951c3i - .309c4r - .951c4i - .809c5r - .588c5i
;; Simplify:
;; r1_1 + c2r + c3r + c4r + c5r
;; r1_2 + .809(c2r-c5r) + .588(c2i+c5i) + .309(c3r-c4r) + .951(c3i+c4i)
;; r1_1 + .309(c2r+c5r) + .951(c2i-c5i) - .809(c3r+c4r) + .588(c3i-c4i)
;; r1_2 - .309(c2r-c5r) + .951(c2i+c5i) - .809(c3r-c4r) - .588(c3i+c4i)
;; r1_1 - .809(c2r+c5r) + .588(c2i-c5i) + .309(c3r+c4r) - .951(c3i-c4i)
;; r1_2 - c2r + c3r - c4r + c5r
;; r1_1 - .809(c2r+c5r) - .588(c2i-c5i) + .309(c3r+c4r) + .951(c3i-c4i)
;; r1_2 - .309(c2r-c5r) - .951(c2i+c5i) - .809(c3r-c4r) + .588(c3i+c4i)
;; r1_1 + .309(c2r+c5r) - .951(c2i-c5i) - .809(c3r+c4r) - .588(c3i-c4i)
;; r1_2 + .809(c2r-c5r) - .588(c2i+c5i) + .309(c3r-c4r) - .951(c3i+c4i)

yr5_10r_unfft_cmn_preload MACRO
	ENDM
yr5_10r_unfft_cmn MACRO srcreg,srcinc,d1,screg1,screg2,screg3,screg4,scregA,scincA,scregB,scincB,maxrpt,L1pt,L1pd
	vmovapd	ymm2, [srcreg+d1]		;; R2
	vmovapd	ymm4, [screg1+32]		;; cosine/sine
	vmulpd	ymm1, ymm2, ymm4		;; A2 = R2 * cosine/sine
	vmovapd	ymm5, [srcreg+4*d1]		;; R5
	vmovapd	ymm6, [screg4+32]		;; cosine/sine
	vmulpd	ymm0, ymm5, ymm6		;; A5 = R5 * cosine/sine
	vmovapd	ymm3, [srcreg+d1+32]		;; I2
	vaddpd	ymm1, ymm1, ymm3		;; A2 = A2 + I2
	vmovapd	ymm7, [srcreg+4*d1+32]		;; I5
	vaddpd	ymm0, ymm0, ymm7		;; A5 = A5 + I5
	vmulpd	ymm3, ymm3, ymm4		;; B2 = I2 * cosine/sine
	vmulpd	ymm7, ymm7, ymm6		;; B5 = I5 * cosine/sine
	vsubpd	ymm3, ymm3, ymm2		;; B2 = B2 - R2
	vsubpd	ymm7, ymm7, ymm5		;; B5 = B5 - R5
	vmovapd	ymm4, [screg1]			;; sine
	vmulpd	ymm1, ymm1, ymm4		;; A2 = A2 * sine (new R2)
	vmovapd	ymm6, [screg4]			;; sine
	vmulpd	ymm0, ymm0, ymm6		;; A5 = A5 * sine (new R5)
	vmulpd	ymm3, ymm3, ymm4		;; B2 = B2 * sine (new I2)
	vmulpd	ymm7, ymm7, ymm6		;; B5 = B5 * sine (new I5)
	vaddpd	ymm2, ymm0, ymm1		;; R2+R5
	vsubpd	ymm0, ymm0, ymm1		;; NEG(R2-R5) = R5-R2
	vaddpd	ymm1, ymm3, ymm7		;; I2+I5
	vsubpd	ymm3, ymm3, ymm7		;; I2-I5
	vmovapd	YMM_TMP1, ymm2			;; Temp save R2+R5
	vmovapd	YMM_TMP2, ymm3			;; Temp save I2-I5
	vmovapd	YMM_TMP3, ymm0			;; Temp save NEG(R2-R5)
	vmovapd	YMM_TMP4, ymm1			;; Temp save I2+I5

	vmovapd	ymm2, [srcreg+3*d1]		;; R4
	vmovapd	ymm4, [screg3+32]		;; cosine/sine
	vmulpd	ymm1, ymm2, ymm4		;; A4 = R4 * cosine/sine
	vmovapd	ymm5, [srcreg+2*d1]		;; R3
	vmovapd	ymm6, [screg2+32]		;; cosine/sine
	vmulpd	ymm0, ymm5, ymm6		;; A3 = R3 * cosine/sine
	vmovapd	ymm3, [srcreg+3*d1+32]		;; I4
	vaddpd	ymm1, ymm1, ymm3		;; A4 = A4 + I4
	vmovapd	ymm7, [srcreg+2*d1+32]		;; I3
	vaddpd	ymm0, ymm0, ymm7		;; A3 = A3 + I3
	vmulpd	ymm3, ymm3, ymm4		;; B4 = I4 * cosine/sine
	vmulpd	ymm7, ymm7, ymm6		;; B3 = I3 * cosine/sine
	vsubpd	ymm3, ymm3, ymm2		;; B4 = B4 - R4
	vsubpd	ymm7, ymm7, ymm5		;; B3 = B3 - R3
	vmovapd	ymm4, [screg3]			;; sine
	vmulpd	ymm1, ymm1, ymm4		;; A4 = A4 * sine (new R4)
	vmovapd	ymm6, [screg2]			;; sine
	vmulpd	ymm0, ymm0, ymm6		;; A3 = A3 * sine (new R3)
	vmulpd	ymm3, ymm3, ymm4		;; B4 = B4 * sine (new I4)
	vmulpd	ymm7, ymm7, ymm6		;; B3 = B3 * sine (new I3)
	vaddpd	ymm2, ymm0, ymm1		;; R3+R4
	vsubpd	ymm0, ymm0, ymm1		;; R3-R4
	vaddpd	ymm1, ymm7, ymm3		;; I3+I4
	vsubpd	ymm7, ymm7, ymm3		;; I3-I4
	vmovapd	YMM_TMP5, ymm0			;; Temp save R3-R4
	vmovapd	YMM_TMP6, ymm1			;; Temp save I3+I4

	L1prefetchw srcreg+L1pd, L1pt

	vmovapd	ymm1, YMM_TMP1			;; Reload R2+R5
	vmovapd	ymm4, YMM_P309
	vmulpd	ymm5, ymm4, ymm1		;; new R3 = .309*(R2+R5)
	vmovapd	ymm6, [srcreg]			;; R1_1
	vaddpd	ymm5, ymm5, ymm6		;; new R3 += R1_1
	vmovapd	ymm0, YMM_P809
	vmulpd	ymm3, ymm0, ymm1		;; new -R5 = -.809*(R2+R5)
	vaddpd	ymm1, ymm6, ymm1		;; new R1 = R1_1+R2+R5
	vsubpd	ymm3, ymm6, ymm3		;; new R5 += R1_1
	vmulpd	ymm0, ymm0, ymm2		;; --.809*(R3+R4)
	vaddpd	ymm1, ymm1, ymm2		;; final R1 = R1_1+R2+R5+R3+R4
	vmulpd	ymm2, ymm4, ymm2		;; .309*(R3+R4)
	vsubpd	ymm5, ymm5, ymm0		;; new R3 += -.809*(R3+R4)
	vaddpd	ymm3, ymm3, ymm2		;; new R5 += .309*(R3+R4)

	L1prefetchw srcreg+d1+L1pd, L1pt

	vmovapd	ymm0, YMM_TMP2			;; Reload I2-I5
	vmovapd	ymm6, YMM_P951
	vmulpd	ymm2, ymm6, ymm0		;; tmp1 = .951*(I2-I5)
	vmovapd	ymm4, YMM_P588
	vmulpd	ymm0, ymm4, ymm0		;; tmp2 = .588*(I2-I5)
	vmulpd	ymm4, ymm4, ymm7		;; .588*(I3-I4)
	vmulpd	ymm7, ymm6, ymm7		;; .951*(I3-I4)
	vaddpd	ymm2, ymm2, ymm4		;; tmp1 += .588*(I3-I4)
	vsubpd	ymm0, ymm0, ymm7		;; tmp2 -= .951*(I3-I4)

	vsubpd	ymm4, ymm5, ymm2		;; final R9 = new R3 - tmp1
	vaddpd	ymm5, ymm5, ymm2		;; final R3 = new R3 + tmp1
	vsubpd	ymm2, ymm3, ymm0		;; final R7 = new R5 - tmp2
	vaddpd	ymm3, ymm3, ymm0		;; final R5 = new R5 + tmp2

	L1prefetchw srcreg+2*d1+L1pd, L1pt

	vmovapd	[srcreg], ymm1			;; Save final R1
	vmovapd	[srcreg+2*d1], ymm5		;; Save final R3
	vmovapd	[srcreg+4*d1], ymm3		;; Save final R5

	vmovapd	ymm3, YMM_TMP5			;; Reload R3-R4
	vmovapd	ymm0, YMM_P309
	vmulpd	ymm1, ymm0, ymm3		;; new R2 = .309*(R3-R4)
	vmovapd	ymm7, [srcreg+32]		;; R1_2
	vaddpd	ymm1, ymm1, ymm7		;; new R2 += R1_2
	vmovapd	ymm5, YMM_P809
	vmulpd	ymm6, ymm5, ymm3		;; new -R4 = -.809*(R3-R4)
	vaddpd	ymm3, ymm7, ymm3		;; new R6 = R1_2+(R3-R4)
	vsubpd	ymm6, ymm7, ymm6		;; new R4 += R1_2
	vmovapd	ymm7, YMM_TMP3			;; Reload NEG(R2-R5)
	vmulpd	ymm5, ymm5, ymm7		;; --.809*NEG(R2-R5)
	vaddpd	ymm3, ymm3, ymm7		;; final R6 = R6+(R3-R4)+NEG(R2-R5)
	vmulpd	ymm0, ymm0, ymm7		;; .309*NEG(R2-R5)
	vsubpd	ymm1, ymm1, ymm5		;; new R2 += -.809*NEG(R2-R5)
	vaddpd	ymm6, ymm6, ymm0		;; new R4 += .309*NEG(R2-R5)

	L1prefetchw srcreg+3*d1+L1pd, L1pt

	vmovapd	ymm0, YMM_TMP6			;; Reload I3+I4
	vmovapd	ymm5, YMM_P588
	vmulpd	ymm7, ymm5, ymm0		;; tmp2 = .588*(I3+I4)
	vmulpd	ymm0, ymm0, YMM_P951		;; tmp1 = .951*(I3+I4)
	vmovapd	[srcreg+32], ymm3		;; Save final I1
	vmovapd	ymm3, YMM_TMP4			;; Reload I2+I5
	vmulpd	ymm5, ymm5, ymm3		;; .588*(I2+I5)
	vmulpd	ymm3, ymm3, YMM_P951		;; .951*(I2+I5)
	vaddpd	ymm0, ymm0, ymm5		;; tmp1 += .588*(I2+I5)
	vsubpd	ymm7, ymm7, ymm3		;; tmp2 -= .951*(I2+I5)

	L1prefetchw srcreg+4*d1+L1pd, L1pt

	vsubpd	ymm3, ymm1, ymm0		;; Final R10 = new R2 - tmp1
	vaddpd	ymm1, ymm1, ymm0		;; Final R2 = new R2 + tmp1
	vsubpd	ymm0, ymm6, ymm7		;; Final R4 = new R4 - tmp2
	vaddpd	ymm6, ymm6, ymm7		;; Final R8 = new R4 + tmp2

	vmovapd	[srcreg+d1], ymm1		;; Save R2
	vmovapd	[srcreg+3*d1], ymm0		;; Save R4
	vmovapd	[srcreg+d1+32], ymm2		;; Save I2
	vmovapd	[srcreg+2*d1+32], ymm6		;; Save I3
	vmovapd	[srcreg+3*d1+32], ymm4		;; Save I4
	vmovapd	[srcreg+4*d1+32], ymm3		;; Save I5
	bump	srcreg, srcinc
	bump	scregA, scincA
	bump	scregB, scincB
	ENDM


;;
;; ************************************* ten-reals-five-complex-fft variants ******************************************
;;

yr5_5cl_ten_reals_five_complex_djbfft_preload MACRO
	yr5_o10r_t5c_djbfft_mem_preload
	ENDM
yr5_5cl_ten_reals_five_complex_djbfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt
	d2=2*d1
	d3=3*d1
	d4=4*d1
	yr5_o10r_t5c_djbfft_mem [srcreg],[srcreg+d1],[srcreg+d2],[srcreg+d3],[srcreg+d4],screg,[srcreg],[srcreg+32],[srcreg+d1],[srcreg+d1+32]
;;	vmovapd	[srcreg+d1], ymm2		;; Save R2
;;	vmovapd	[srcreg+d1+32], ymm4		;; Save I2
	vmovapd	[srcreg+d2], ymm4		;; Save R3
	vmovapd	[srcreg+d2+32], ymm7		;; Save I3
	vmovapd	[srcreg+d3], ymm5		;; Save R4
	vmovapd	[srcreg+d3+32], ymm1		;; Save I4
	vmovapd	[srcreg+d4], ymm6		;; Save R5
	vmovapd	[srcreg+d4+32], ymm3		;; Save I5
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; To calculate a 10-reals FFT (in a shorthand notation):
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0000000000
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0123456789
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0246802468
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0369258147
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0482604826
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0505050505
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0628406284
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0741852963
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0864208642
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0987654321
;; Noting that w^5 = -1 and that Hermetian symmetry means we won't need
;; to calculate the last 5 rows:
;; r1 + r2 + r3 + r4 + r5 + r6 + r7 + r8 + r9 + r10	*  w^0000000000
;; r1 + r2 + r3 + r4 + r5 - r6 - r7 - r8 - r9 - r10	*  w^0123401234
;; r1 + r2 + r3 - r4 - r5 + r6 + r7 + r8 - r9 - r10	*  w^0241302413
;; r1 + r2 - r3 - r4 + r5 - r6 - r7 + r8 + r9 - r10	*  w^0314203142
;; r1 + r2 - r3 + r4 - r5 + r6 + r7 - r8 + r9 - r10	*  w^0432104321
;; Reorganize into odds and evens, that is
;; r1 + r3 + r5 + r7 + r9  * powers + (r2 + r4 + r6 + r8 + r10) * powers
;; Thus:
;; r1 + r3 + r5 + r7 + r9	*  w^00000	+ r2 + r4 + r6 + r8 + r10	*  w^00000
;; r1 + r3 + r5 - r7 - r9	*  w^02413	+ r2 + r4 - r6 - r8 - r10	*  w^13024
;; r1 + r3 - r5 + r7 - r9	*  w^04321	+ r2 - r4 + r6 + r8 - r10	*  w^21043
;; r1 - r3 + r5 - r7 + r9	*  w^01234	+ r2 - r4 - r6 + r8 - r10	*  w^34012
;; r1 - r3 - r5 + r7 + r9	*  w^03142	+ r2 + r4 + r6 - r8 - r10	*  w^42031
;; Apply the sin/cos values:
;; w^1/10 = .809 + .588i
;; w^2/10 = .309 + .951i
;; w^3/10 = -.309 + .951i
;; w^4/10 = -.809 + .588i
;; reals:
;; r1 + r3 + r5 + r7 + r9			+ r2 + r4 + r6 + r8 + r10
;; r1 + .309r3 - .809r5 - .809r7 + .309r9	+ .809r2 - .309r4 - r6 - .309r8 + .809r10
;; r1 - .809r3 + .309r5 + .309r7 - .809r9	+ .309r2 - .809r4 + r6 - .809r8 + .309r10
;; r1 - .809r3 + .309r5 + .309r7 - .809r9	+ -.309r2 + .809r4 - r6 + .809r8 - .309r10
;; r1 + .309r3 - .809r5 - .809r7 + .309r9	+ -.809r2 + .309r4 + r6 + .309r8 - .809r10
;; imaginarys:
;; 0						+ 0
;;  + .951r3 + .588r5 - .588r7 - .951r9		+ .588r2 + .951r4 - .951r8 - .588r10
;;  + .588r3 - .951r5 + .951r7 - .588r9		+ .951r2 - .588r4 + .588r8 - .951r10
;;  - .588r3 + .951r5 - .951r7 + .588r9		+ .951r2 - .588r4 + .588r8 - .951r10
;;  - .951r3 - .588r5 + .588r7 + .951r9		+ .588r2 + .951r4 - .951r8 - .588r10
;; Further simplifying:
;; reals:
;; r1 + r3 + r5 + r7 + r9		+ r2 + r4 + r6 + r8 + r10
;; r1 + .309(r3+r9) - .809(r5+r7)	+ .809(r2+r10) - .309(r4+r8) - r6
;; r1 - .809(r3+r9) + .309(r5+r7)	+ .309(r2+r10) - .809(r4+r8) + r6
;; r1 - .809(r3+r9) + .309(r5+r7)	+ -.309(r2+r10) + .809(r4+r8) - r6
;; r1 + .309(r3+r9) - .809(r5+r7)	+ -.809(r2+r10) + .309(r4+r8) + r6
;; imaginarys:
;; 0					+ 0
;;  + .951(r3-r9) + .588(r5-r7)		+ .588(r2-r10) + .951(r4-r8)
;;  + .588(r3-r9) - .951(r5-r7)		+ .951(r2-r10) - .588(r4-r8)
;;  - .588(r3-r9) + .951(r5-r7)		+ .951(r2-r10) - .588(r4-r8)
;;  - .951(r3-r9) - .588(r5-r7)		+ .588(r2-r10) + .951(r4-r8)

yr5_o10r_t5c_djbfft_mem_preload MACRO
	ENDM
yr5_o10r_t5c_djbfft_mem MACRO memr1,memr2,memr3,memr4,memr5,screg,dst1,dst2,dst3,dst4
						;; Ten reals comments		; Five complex comments
	vmovapd	ymm2, memr2		;; R2					; r2
	vmovapd	ymm3, memr3		;; R3					; r3
	vblendpd ymm0, ymm2, ymm3, 1	;; R3					; r2
	vmovapd	ymm5, memr5		;; R5					; r5
	vblendpd ymm1, ymm5, memr4[32], 1 ;; R9					; r5
	vsubpd	ymm7, ymm0, ymm1	;; R3-R9				; r25s=r2-r5
	vaddpd	ymm0, ymm0, ymm1	;; R3+R9				; r25a=r2+r5
	vblendpd ymm3, ymm3, ymm5, 1	;; R5					; r3
	vmovapd	ymm4, memr4		;; R4					; r4
	vblendpd ymm4, ymm4, memr2[32], 1 ;; R7					; r4
	vsubpd	ymm6, ymm3, ymm4	;; R5-R7				; r34s=r3-r4
	vaddpd	ymm3, ymm3, ymm4	;; R5+R7				; r34a=r3+r4
	vmovapd	ymm1, YMM_P951
	vmulpd	ymm2, ymm1, ymm7	;; new oddI2 = .951*(R3-R9)		; sin2*r25s
	vmovapd	ymm5, YMM_P588
	vmulpd	ymm7, ymm5, ymm7	;; new oddI3 = .588*(R3-R9)		; sin4*r25s
	vmulpd	ymm5, ymm5, ymm6	;; .588*(R5-R7)				; sin4*r34s
	vmulpd	ymm1, ymm1, ymm6	;; .951*(R5-R7)				; sin2*r34s
	vaddpd	ymm2, ymm2, ymm5	;; new oddI2 += .588*(R5-R7)		; t6=sin2*r25s + sin4*r34s
	vsubpd	ymm7, ymm7, ymm1	;; new oddI3 -= .951*(R5-R7)		; t8=sin4*r25s - sin2*r34s
	vmovapd	ymm1, YMM_P309
	vmulpd	ymm5, ymm1, ymm0	;; new oddR2 = .309*(R3+R9)		; cos2*r25a
	vmovapd	ymm4, memr1		;; R1					; r1
	vaddpd	ymm5, ymm5, ymm4	;; new oddR2 += R1			; cos2*r25a + r1
	vmovapd	ymm6, YMM_P809
	vmovapd	YMM_TMP6, ymm2		;; Save oddI2				; Save t6
	vmulpd	ymm2, ymm6, ymm0	;; new -oddR3 = -.809*(R3+R9)		; cos4*r25a
	vaddpd	ymm0, ymm4, ymm0	;; new oddR1 = R1+R3+R9			; r1 + r25a
	vsubpd	ymm2, ymm4, ymm2	;; new oddR3 += R1			; cos4*r25a + r1
	vmulpd	ymm4, ymm6, ymm3	;; --.809*(R5+R7)			; cos4*r34a
	vaddpd	ymm0, ymm0, ymm3	;; new oddR1 = R1+R3+R5+R7+R9		; outr(0) = r1+r25a+r34a
	vmulpd	ymm3, ymm1, ymm3	;; .309*(R5+R7)				; cos2*r34a
	vsubpd	ymm5, ymm5, ymm4	;; new oddR2 += -.809*(R5+R7)		; t1=cos2*r25a + cos4*r34a + r1
	vaddpd	ymm2, ymm2, ymm3	;; new oddR3 += .309*(R5+R7)		; t3=cos4*r25a + cos2*r34a + r1
	vmovapd	YMM_TMP8, ymm7		;; Save oddI3				; Save t8
	vmovapd	dst1, ymm0		;; Save oddR1				; Save R1
	vmovapd	YMM_TMP1, ymm5		;; Save oddR2				; Save t1
	vmovapd	YMM_TMP3, ymm2		;; Save oddR3				; Save t3

	vmovapd	ymm2, memr2[32]		;; R7					; i2
	vblendpd ymm2, ymm2, memr2, 1	;; R2					; i2
	vmovapd	ymm5, memr5[32]		;; R10					; i5
	vsubpd	ymm7, ymm2, ymm5	;; R2-R10				; i25s=i2-i5
	vaddpd	ymm2, ymm2, ymm5	;; R2+R10				; i25a=i2+i5
	vmovapd	ymm4, memr4[32]		;; R9					; i4
	vblendpd ymm4, ymm4, memr4, 1	;; R4					; i4
	vmovapd	ymm3, memr3[32]		;; R8					; i3
	vsubpd	ymm6, ymm4, ymm3	;; R4-R8				; NEGi34s=i4-i3
	vaddpd	ymm4, ymm4, ymm3	;; R4+R8				; i34a=i4+i3

	vmovapd	ymm0, YMM_P951
	vmulpd	ymm1, ymm0, ymm7	;; new evenI3 = .951*(R2-R10)		; sin2*i25s
	vmovapd	ymm3, YMM_P588
	vmulpd	ymm7, ymm3, ymm7	;; new evenI2 = .588*(R2-R10)		; sin4*i25s
	vmulpd	ymm3, ymm3, ymm6	;; .588*(R4-R8)				; sin4*NEGi34s
	vmulpd	ymm0, ymm0, ymm6	;; .951*(R4-R8)				; sin2*NEGi34s
	vsubpd	ymm1, ymm1, ymm3	;; new evenI3 -= .588*(R4-R8)		; t2=sin2*i25s - sin4*NEGi34s
	vaddpd	ymm7, ymm7, ymm0	;; new evenI2 += .951*(R4-R8)		; t4=sin4*i25s + sin2*NEGi34s

	vmulpd	ymm3, ymm2, YMM_P309	;; new evenR3 = .309*(R2+R10)		; cos2*i25a
	vmovapd	ymm5, memr1[32]		;; R6					; i1
	vaddpd	ymm3, ymm3, ymm5	;; new evenR3 += R6			; cos2*i25a + i1
	vmovapd	ymm6, YMM_P809
	vmulpd	ymm0, ymm6, ymm2	;; new -evenR5 = -.809*(R2+R10)		; cos4*i25a
	vaddpd	ymm2, ymm5, ymm2	;; new evenR1 = R6+R2+R10		; i1 + i25a
	vsubpd	ymm0, ymm5, ymm0	;; new evenR5 += R6			; cos4*i25a+ i1
	vmulpd	ymm6, ymm6, ymm4	;; --.809*(R4+R8)			; cos4*i34a
	vaddpd	ymm2, ymm2, ymm4	;; new evenR1 = R6+R2+R4+R8+R10		; outi(0)=i1+i25a+i34a
	vmulpd	ymm4, ymm4, YMM_P309	;; .309*(R4+R8)				; cos2*i34a
	vsubpd	ymm3, ymm3, ymm6	;; new evenR3 += -.809*(R4+R8)		; t5=cos2*i25a + cos4*i34a + i1
	vaddpd	ymm0, ymm0, ymm4	;; new evenR5 += .309*(R4+R8)		; t7=cos4*i25a + cos2*i34a + i1
	vmovapd	dst2, ymm2		;; Save evenR1				; Save I1

	vblendpd ymm5, ymm3, ymm7, 1	;; evenI2				; t5
	vblendpd ymm4, ymm7, ymm3, 1	;; evenR3				; t4
	vblendpd ymm2, ymm1, ymm0, 1	;; evenR5				; t2
	vblendpd ymm7, ymm0, ymm1, 1	;; evenI3				; t7

	vmovapd	ymm1, YMM_TMP1		;; oddR2				; t1
	vaddpd	ymm0, ymm1, ymm2	;; New R5 = oddR2 + evenR5		; outr(4)=t1+t2
	vmovapd	ymm6, YMM_TMP6		;; oddI2				; t6
	vsubpd	ymm3, ymm5, ymm6	;; New I5 = evenI2 - oddI2		; outi(4)=t5-t6
	vsubpd	ymm1, ymm1, ymm2	;; New R2 = oddR2 - evenR5		; outr(1)=t1-t2
	vaddpd	ymm5, ymm5, ymm6	;; New I2 = evenI2 + oddI2		; outi(1)=t5+t6

	vmovapd	ymm2, [screg+192+32]			;; cosine/sine for w^4
	vmulpd	ymm6, ymm0, ymm2			;; A5 = R5 * cosine/sine
	vsubpd	ymm6, ymm6, ymm3			;; A5 = A5 - I5
	vmulpd	ymm3, ymm3, ymm2			;; B5 = I5 * cosine/sine
	vaddpd	ymm3, ymm3, ymm0			;; B5 = B5 + R5

	vmovapd	ymm2, [screg+32]			;; cosine/sine for w^1
	vmulpd	ymm0, ymm1, ymm2			;; A2 = R2 * cosine/sine
	vsubpd	ymm0, ymm0, ymm5			;; A2 = A2 - I2
	vmulpd	ymm5, ymm5, ymm2			;; B2 = I2 * cosine/sine
	vaddpd	ymm5, ymm5, ymm1			;; B2 = B2 + R2

	vmovapd	ymm1, [screg+192]
	vmulpd	ymm6, ymm6, ymm1			;; A5 = A5 * sine (final R5)
	vmulpd	ymm3, ymm3, ymm1			;; B5 = B5 * sine (final I5)
	vmovapd	ymm1, [screg]
	vmulpd	ymm0, ymm0, ymm1			;; A2 = A2 * sine (final R2)
	vmulpd	ymm5, ymm5, ymm1			;; B2 = B2 * sine (final I2)

	vmovapd	dst3, ymm0				;; Save final R2
	vmovapd	dst4, ymm5				;; Save final I2

	vmovapd	ymm0, YMM_TMP3		;; oddR3				; t3
	vsubpd	ymm2, ymm0, ymm4	;; New R4 = oddR3 - evenR3		; outr(2)=t3-t4
	vaddpd	ymm0, ymm0, ymm4	;; New R3 = oddR3 + evenR3		; outr(3)=t3+t4
	vmovapd	ymm5, YMM_TMP8		;; oddI3				; t8
	vsubpd	ymm1, ymm7, ymm5	;; New I4 = evenI3 - oddI3		; outi(3)=t7-t8
	vaddpd	ymm7, ymm7, ymm5	;; New I3 = evenI3 + oddI3		; outi(2)=t7+t8

	vblendpd ymm4, ymm0, ymm2, 1			;; R4
	vblendpd ymm2, ymm2, ymm0, 1			;; R3

	vmovapd	ymm0, [screg+128+32]			;; cosine/sine for w^3
	vmulpd	ymm5, ymm4, ymm0			;; A4 = R4 * cosine/sine
	vsubpd	ymm5, ymm5, ymm1			;; A4 = A4 - I4
	vmulpd	ymm1, ymm1, ymm0			;; B4 = I4 * cosine/sine
	vaddpd	ymm1, ymm1, ymm4			;; B4 = B4 + R4

	vmovapd	ymm0, [screg+64+32]			;; cosine/sine for w^2
	vmulpd	ymm4, ymm2, ymm0			;; A3 = R3 * cosine/sine
	vsubpd	ymm4, ymm4, ymm7			;; A3 = A3 - I3
	vmulpd	ymm7, ymm7, ymm0			;; B3 = I3 * cosine/sine
	vaddpd	ymm7, ymm7, ymm2			;; B3 = B3 + R3

	vmovapd	ymm2, [screg+128]
	vmulpd	ymm5, ymm5, ymm2			;; A4 = A4 * sine (final R4)
	vmulpd	ymm1, ymm1, ymm2			;; B4 = B4 * sine (final I4)
	vmovapd	ymm2, [screg+64]
	vmulpd	ymm4, ymm4, ymm2			;; A3 = A3 * sine (final R3)
	vmulpd	ymm7, ymm7, ymm2			;; B3 = B3 * sine (final I3)
	ENDM

;;
;;
;; ************************************* ten-reals-five-complex-unfft variants ******************************************
;;

;; Macro to do a ten_reals_unfft and three five_complex_djbunfft in pass 2.
;; The ten-reals operation is done in the lower double of the YMM
;; register.  The five complex operation is done in the upper values
;; of a YMM register.

yr5_5cl_ten_reals_five_complex_djbunfft_preload MACRO
	yr5_o10r_t5c_djbunfft_mem_preload
	ENDM
yr5_5cl_ten_reals_five_complex_djbunfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt
	d2 = 2*d1
	d3 = 3*d1
	d4 = 4*d1
	yr5_o10r_t5c_djbunfft_mem [srcreg],[srcreg+d1],[srcreg+d2],[srcreg+d3],[srcreg+d4],screg,[srcreg],[srcreg+d2],[srcreg+d4],[srcreg+32]
;	vmovapd	[srcreg], ymm4		;; Save R1
	vmovapd	[srcreg+d1], ymm2	;; Save R2
;	vmovapd	[srcreg+d2], ymm3	;; Save R3
	vmovapd	[srcreg+d3], ymm7	;; Save R4
;	vmovapd	[srcreg+d4], ymm0	;; Save R5
;	vmovapd	[srcreg+32], ymm2	;; Save I1
	vmovapd	[srcreg+d1+32], ymm0	;; Save I2
	vmovapd	[srcreg+d2+32], ymm1	;; Save I3
	vmovapd	[srcreg+d3+32], ymm3	;; Save I4
	vmovapd	[srcreg+d4+32], ymm6	;; Save I5
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; To calculate a 10-reals unFFT (in a shorthand notation):
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0000000000
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0123456789
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0246802468
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0369258147
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0482604826
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0505050505
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0628406284
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0741852963
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0864208642
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0987654321
;; Noting that w^5 = -1
;; c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10	*  w^0000000000
;; c1 + c2 + c3 + c4 + c5 - c6 - c7 - c8 - c9 - c10	*  w^0123401234
;; c1 + c2 + c3 - c4 - c5 + c6 + c7 + c8 - c9 - c10	*  w^0241302413
;; c1 + c2 - c3 - c4 + c5 - c6 - c7 + c8 + c9 - c10	*  w^0314203142
;; c1 + c2 - c3 + c4 - c5 + c6 + c7 - c8 + c9 - c10	*  w^0432104321
;; c1 - c2 + c3 - c4 + c5 - c6 + c7 - c8 + c9 - c10	*  w^0000000000
;; c1 - c2 + c3 - c4 + c5 + c6 - c7 + c8 - c9 + c10	*  w^0123401234
;; c1 - c2 + c3 + c4 - c5 - c6 + c7 - c8 - c9 + c10	*  w^0241302413
;; c1 - c2 - c3 + c4 + c5 + c6 - c7 - c8 + c9 + c10	*  w^0314203142
;; c1 - c2 - c3 - c4 - c5 - c6 + c7 + c8 + c9 + c10	*  w^0432104321
;; incoming is:	r1_1 = c1r + c6r
;;		c2 = c2r + c2i
;;		c3 = c3r + c3i
;;		c4 = c4r + c4i
;;		c5 = c5r + c5i
;;		r1_2 = c1r - c6r
;;		c7 = c5r - c5i	(implied)
;;		c8 = c4r - c4i	(implied)
;;		c9 = c3r - c3i	(implied)
;;		c10 = c2r - c2i	(implied)
;; And noticing the signs of the real and imaginary parts of the sin/cos values:
;; w^1/10 = .809 - .588i
;; w^2/10 = .309 - .951i
;; w^3/10 = -.309 - .951i
;; w^4/10 = -.809 - .588i
;; We get reals:
;; r1_1 + 2c2 + 2c3 + 2c4 + 2c5	*  w^00000
;; r1_2 + 2c2 + 2c3 + 2c4 + 2c5	*  w^01234
;; r1_1 + 2c2 + 2c3 - 2c4 - 2c5	*  w^02413
;; r1_2 + 2c2 - 2c3 - 2c4 + 2c5	*  w^03142
;; r1_1 + 2c2 - 2c3 + 2c4 - 2c5	*  w^04321
;; r1_2 - 2c2 + 2c3 - 2c4 + 2c5	*  w^00000
;; r1_1 - 2c2 + 2c3 - 2c4 + 2c5	*  w^01234
;; r1_2 - 2c2 + 2c3 + 2c4 - 2c5	*  w^02413
;; r1_1 - 2c2 - 2c3 + 2c4 + 2c5	*  w^03142
;; r1_2 - 2c2 - 2c3 - 2c4 - 2c5	*  w^04321
;; Now drop the multiplication by 2 (the actual r1_1 and r1_2 inputs are already doubled)
;; and expand the sin/cos multipliers:
;; r1_1 + c2r + c3r + c4r + c5r
;; r1_2 + .809c2r + .588c2i + .309c3r + .951c3i - .309c4r + .951c4i - .809c5r + .588c5i
;; r1_1 + .309c2r + .951c2i - .809c3r + .588c3i - .809c4r - .588c4i + .309c5r - .951c5i
;; r1_2 - .309c2r + .951c2i - .809c3r - .588c3i + .809c4r - .588c4i + .309c5r + .951c5i
;; r1_1 - .809c2r + .588c2i + .309c3r - .951c3i + .309c4r + .951c4i - .809c5r - .588c5i
;; r1_2 - c2r + c3r - c4r + c5r
;; r1_1 - .809c2r - .588c2i + .309c3r + .951c3i + .309c4r - .951c4i - .809c5r + .588c5i
;; r1_2 - .309c2r - .951c2i - .809c3r + .588c3i + .809c4r + .588c4i + .309c5r - .951c5i
;; r1_1 + .309c2r - .951c2i - .809c3r - .588c3i - .809c4r + .588c4i + .309c5r + .951c5i
;; r1_2 + .809c2r - .588c2i + .309c3r - .951c3i - .309c4r - .951c4i - .809c5r - .588c5i
;; Simplify:
;; r1_1 + c2r + c3r + c4r + c5r
;; r1_2 + .809(c2r-c5r) + .588(c2i+c5i) + .309(c3r-c4r) + .951(c3i+c4i)
;; r1_1 + .309(c2r+c5r) + .951(c2i-c5i) - .809(c3r+c4r) + .588(c3i-c4i)
;; r1_2 - .309(c2r-c5r) + .951(c2i+c5i) - .809(c3r-c4r) - .588(c3i+c4i)
;; r1_1 - .809(c2r+c5r) + .588(c2i-c5i) + .309(c3r+c4r) - .951(c3i-c4i)
;; r1_2 - c2r + c3r - c4r + c5r
;; r1_1 - .809(c2r+c5r) - .588(c2i-c5i) + .309(c3r+c4r) + .951(c3i-c4i)
;; r1_2 - .309(c2r-c5r) - .951(c2i+c5i) - .809(c3r-c4r) + .588(c3i+c4i)
;; r1_1 + .309(c2r+c5r) - .951(c2i-c5i) - .809(c3r+c4r) - .588(c3i-c4i)
;; r1_2 + .809(c2r-c5r) - .588(c2i+c5i) + .309(c3r-c4r) - .951(c3i+c4i)

yr5_o10r_t5c_djbunfft_mem_preload MACRO
	ENDM
yr5_o10r_t5c_djbunfft_mem MACRO memr1,memr2,memr3,memr4,memr5,screg,dstr1,dstr3,dstr5,dsti1
					;; Ten reals comments		;; Five complex comments
	vmovapd	ymm2, memr2				;; R2
	vmovapd	ymm4, [screg+32]			;; cosine/sine
	vmulpd	ymm1, ymm2, ymm4			;; A2 = R2 * cosine/sine
	vmovapd	ymm5, memr5				;; R5
	vmovapd	ymm6, [screg+192+32]			;; cosine/sine
	vmulpd	ymm0, ymm5, ymm6			;; A5 = R5 * cosine/sine
	vmovapd	ymm3, memr2[32]				;; I2
	vaddpd	ymm1, ymm1, ymm3			;; A2 = A2 + I2
	vmovapd	ymm7, memr5[32]				;; I5
	vaddpd	ymm0, ymm0, ymm7			;; A5 = A5 + I5
	vmulpd	ymm3, ymm3, ymm4			;; B2 = I2 * cosine/sine
	vmulpd	ymm7, ymm7, ymm6			;; B5 = I5 * cosine/sine
	vsubpd	ymm3, ymm3, ymm2			;; B2 = B2 - R2
	vsubpd	ymm7, ymm7, ymm5			;; B5 = B5 - R5
	vmovapd	ymm4, [screg]				;; sine
	vmulpd	ymm1, ymm1, ymm4			;; A2 = A2 * sine (new R2)
	vmovapd	ymm6, [screg+192]			;; sine
	vmulpd	ymm0, ymm0, ymm6			;; A5 = A5 * sine (new R5)
	vmulpd	ymm3, ymm3, ymm4			;; B2 = B2 * sine (new I2)
	vmulpd	ymm7, ymm7, ymm6			;; B5 = B5 * sine (new I5)
	vaddpd	ymm2, ymm0, ymm1	;; R2+R5			; r25a=r2+r5
	vsubpd	ymm0, ymm0, ymm1	;; NEG(R2-R5) = R5-R2		; NEGr25s=r5-r2
	vaddpd	ymm1, ymm3, ymm7	;; I2+I5			; i25a=i2+i5
	vsubpd	ymm3, ymm3, ymm7	;; I2-I5			; i25s=i2-i5
	vblendpd ymm4, ymm1, ymm0, 1	;; NEG(R2-R5)			; i25a
	vblendpd ymm0, ymm0, ymm1, 1	;; I2+I5			; NEGr25s
	vmovapd	YMM_TMP1, ymm2		;; Temp save R2+R5		; Temp save r25a
	vmovapd	YMM_TMP2, ymm3		;; Temp save I2-I5		; Temp save i25s
	vmovapd	YMM_TMP3, ymm4		;; Temp save NEG(R2-R5)		; Temp save i25a
	vmovapd	YMM_TMP4, ymm0		;; Temp save I2+I5		; Temp save NEGr25s

	vmovapd	ymm2, memr4				;; R4
	vmovapd	ymm4, [screg+128+32]			;; cosine/sine
	vmulpd	ymm1, ymm2, ymm4			;; A4 = R4 * cosine/sine
	vmovapd	ymm5, memr3				;; R3
	vmovapd	ymm6, [screg+64+32]			;; cosine/sine
	vmulpd	ymm0, ymm5, ymm6			;; A3 = R3 * cosine/sine
	vmovapd	ymm3, memr4[32]				;; I4
	vaddpd	ymm1, ymm1, ymm3			;; A4 = A4 + I4
	vmovapd	ymm7, memr3[32]				;; I3
	vaddpd	ymm0, ymm0, ymm7			;; A3 = A3 + I3
	vmulpd	ymm3, ymm3, ymm4			;; B4 = I4 * cosine/sine
	vmulpd	ymm7, ymm7, ymm6			;; B3 = I3 * cosine/sine
	vsubpd	ymm3, ymm3, ymm2			;; B4 = B4 - R4
	vsubpd	ymm7, ymm7, ymm5			;; B3 = B3 - R3
	vmovapd	ymm4, [screg+128]			;; sine
	vmulpd	ymm1, ymm1, ymm4			;; A4 = A4 * sine (new R4)
	vmovapd	ymm6, [screg+64]			;; sine
	vmulpd	ymm0, ymm0, ymm6			;; A3 = A3 * sine (new R3)
	vmulpd	ymm3, ymm3, ymm4			;; B4 = B4 * sine (new I4)
	vmulpd	ymm7, ymm7, ymm6			;; B3 = B3 * sine (new I3)
	vaddpd	ymm2, ymm0, ymm1	;; R3+R4			; r34a=r3+r4
	vsubpd	ymm0, ymm0, ymm1	;; R3-R4			; r34s=r3-r4
	vaddpd	ymm1, ymm7, ymm3	;; I3+I4			; i34a=i3+i4
	vsubpd	ymm7, ymm7, ymm3	;; I3-I4			; i34s=i3-i4
	vblendpd ymm3, ymm1, ymm0, 1	;; R3-R4			; i34a
	vblendpd ymm0, ymm0, ymm1, 1	;; I3+I4			; r34s
	vmovapd	YMM_TMP5, ymm3		;; Temp save R3-R4		; Temp save i34a
	vmovapd	YMM_TMP6, ymm0		;; Temp save I3+I4		; Temp save r34s

	vmovapd	ymm1, YMM_TMP1		;; Reload R2+R5			; Reload save r25a
	vmovapd	ymm4, YMM_P309
	vmulpd	ymm5, ymm4, ymm1	;; new R3 = .309*(R2+R5)	; cos2*r25a
	vmovapd	ymm6, memr1		;; R1_1				; r1
	vaddpd	ymm5, ymm5, ymm6	;; new R3 += R1_1		; cos2*r25a + r1
	vmovapd	ymm0, YMM_P809
	vmulpd	ymm3, ymm0, ymm1	;; new -R5 = -.809*(R2+R5)	; cos4*r25a
	vaddpd	ymm1, ymm6, ymm1	;; new R1 = R1_1+R2+R5		; r1 + r25a
	vsubpd	ymm3, ymm6, ymm3	;; new R5 += R1_1		; cos4*r25a + r1
	vmulpd	ymm0, ymm0, ymm2	;; --.809*(R3+R4)		; cos4*r34a
	vaddpd	ymm1, ymm1, ymm2	;; final R1 = R1_1+R2+R5+R3+R4	; outr(0) = r1 + r25a + r34a
	vmulpd	ymm2, ymm4, ymm2	;; .309*(R3+R4)			; cos2*r34a
	vsubpd	ymm5, ymm5, ymm0	;; new R3 += -.809*(R3+R4)	; t1=cos2*r25a + cos4*r34a + r1
	vaddpd	ymm3, ymm3, ymm2	;; new R5 += .309*(R3+R4)	; t3=cos4*r25a + cos2*r34a + r1

	vmovapd	ymm0, YMM_TMP2		;; Reload I2-I5			; Reload i25s
	vmovapd	ymm6, YMM_P951
	vmulpd	ymm2, ymm6, ymm0	;; tmp1 = .951*(I2-I5)		; sin2*i25s
	vmovapd	ymm4, YMM_P588
	vmulpd	ymm0, ymm4, ymm0	;; tmp2 = .588*(I2-I5)		; sin4*i25s
	vmulpd	ymm4, ymm4, ymm7	;; .588*(I3-I4)			; sin4*i34s
	vmulpd	ymm7, ymm6, ymm7	;; .951*(I3-I4)			; sin2*i34s
	vaddpd	ymm2, ymm2, ymm4	;; tmp1 += .588*(I3-I4)		; t2=sin2*i25s + sin4*i34s
	vsubpd	ymm0, ymm0, ymm7	;; tmp2 -= .951*(I3-I4)		; t4=sin4*i25s - sin2*i34s

	vsubpd	ymm4, ymm5, ymm2	;; final R9 = new R3 - tmp1	; outr(4)=t1-t2
	vaddpd	ymm5, ymm5, ymm2	;; final R3 = new R3 + tmp1	; outr(1)=t1+t2
	vsubpd	ymm2, ymm3, ymm0	;; final R7 = new R5 - tmp2	; outr(3)=t3-t4
	vaddpd	ymm3, ymm3, ymm0	;; final R5 = new R5 + tmp2	; outr(2)=t3+t4
	vmovapd	dstr1, ymm1				;; Save final R1
	vblendpd ymm1, ymm3, ymm5, 1			;; final R3
	vmovapd	dstr3, ymm1				;; Save final R3
	vblendpd ymm1, ymm4, ymm3, 1			;; final R5
	vmovapd	dstr5, ymm1				;; Save final R5
	vblendpd ymm5, ymm5, ymm4, 1	;; final R9 (I4)		; final R2

	vmovapd	ymm3, YMM_TMP5		;; Reload R3-R4			; Reload i34a
	vmovapd	ymm0, YMM_P309
	vmulpd	ymm1, ymm0, ymm3	;; new R2 = .309*(R3-R4)	; cos2*i34a
	vmovapd	ymm7, memr1[32]		;; R1_2				; i1
	vaddpd	ymm1, ymm1, ymm7	;; new R2 += R1_2		; cos2*i34a + i1
	vmovapd	ymm4, YMM_P809
	vmulpd	ymm6, ymm4, ymm3	;; new -R4 = -.809*(R3-R4)	; cos4*i34a
	vaddpd	ymm3, ymm7, ymm3	;; new R6 = R1_2+(R3-R4)	; i1 + i34a
	vsubpd	ymm6, ymm7, ymm6	;; new R4 += R1_2		; cos4*i34a + i1
	vmovapd	ymm7, YMM_TMP3		;; Reload NEG(R2-R5)		; Reload i25a
	vmulpd	ymm4, ymm4, ymm7	;; --.809*NEG(R2-R5)		; cos4*i25a
	vaddpd	ymm3, ymm3, ymm7	;; final R6 = R6+(R3-R4)+NEG(R2-R5) ; outi(0)=i1 + i34a + i25a
	vmulpd	ymm0, ymm0, ymm7	;; .309*NEG(R2-R5)		; cos2*i25a
	vsubpd	ymm1, ymm1, ymm4	;; new R2 += -.809*NEG(R2-R5)	; t7=cos2*i34a + i1 + cos4*i25a
	vaddpd	ymm6, ymm6, ymm0	;; new R4 += .309*NEG(R2-R5)	; t5=cos4*i34a + i1 + cos2*i25a

	vmovapd	ymm0, YMM_TMP6		;; Reload I3+I4			; Reload r34s
	vmovapd	ymm4, YMM_P588
	vmulpd	ymm7, ymm4, ymm0	;; tmp2 = .588*(I3+I4)		; sin4*r34s
	vmulpd	ymm0, ymm0, YMM_P951	;; tmp1 = .951*(I3+I4)		; sin2*r34s
	vmovapd	dsti1, ymm3				;; Save final I1
	vmovapd	ymm3, YMM_TMP4		;; Reload I2+I5			; Reload NEGr25s
	vmulpd	ymm4, ymm4, ymm3	;; .588*(I2+I5)			; sin4*NEGr25s
	vmulpd	ymm3, ymm3, YMM_P951	;; .951*(I2+I5)			; sin2*NEGr25s
	vaddpd	ymm0, ymm0, ymm4	;; tmp1 += .588*(I2+I5)		; t8=sin2*r34s + sin4*NEGr25s
	vsubpd	ymm7, ymm7, ymm3	;; tmp2 -= .951*(I2+I5)		; t6=sin4*r34s - sin2*NEGr25s

	vsubpd	ymm3, ymm1, ymm0	;; Final R10 = new R2 - tmp1	; outi(3)=t7-t8
	vaddpd	ymm1, ymm1, ymm0	;; Final R2 = new R2 + tmp1	; outi(2)=t7+t8
	vsubpd	ymm0, ymm6, ymm7	;; Final R4 = new R4 - tmp2	; outi(1)=t5-t6
	vaddpd	ymm6, ymm6, ymm7	;; Final R8 = new R4 + tmp2	; outi(4)=t5+t6

	vblendpd ymm7, ymm2, ymm0, 1	;; final R4			; final R4
	vblendpd ymm0, ymm0, ymm2, 1	;; final R7 (I2)		; final I2
	vblendpd ymm2, ymm5, ymm1, 1	;; final R2			; final R2
	vblendpd ymm1, ymm1, ymm6, 1	;; final R8 (I3)		; final I3
	vblendpd ymm6, ymm6, ymm3, 1	;; final R10 (I5)		; final I5
	vblendpd ymm3, ymm3, ymm5, 1	;; final R9 (I4)		; final I4
	ENDM


;;
;; ************************************* 20-reals-fft variants ******************************************
;;

;; This should in theory be faster than an 8-real step 1 followed by a 5-complex (or 10-real) step 2.
;; To see this, count the adds and muls to process 80 reals as either
;;    1)  10 * eight-reals, 2 ten-reals and 6 five-complex, or
;;    2)  4 * twenty-reals, 1/2 * (one eight-reals and 9 four-complex)

;; These macros operate on twenty reals doing 4.32 levels of the FFT.  The output is
;; 2 reals and 9 complex numbers.

;; To calculate a 20-reals FFT, we calculate 20 complex values in a brute force way (using a shorthand notation):
;; r1 + r2 + ... + r20	*  w^0000000000...
;; r1 + r2 + ... + r20	*  w^0123456789A...
;; r1 + r2 + ... + r20	*  w^02468ACE....
;;    ...
;; r1 + r2 + ... + r20	*  w^...A987654321
;; Note that Hermetian symmetry means we won't need to calculate the last 10 complex values.
;;
;; The sin/cos values (w = 20th root of unity) are:
;; w^1 = .951 + .309i
;; w^2 = .809 + .588i
;; w^3 = .588 + .809i
;; w^4 = .309 + .951i
;; w^5 = 0 + 1i
;; w^6 = -.309 + .951i
;; w^7 = -.588 + .809i
;; w^8 = -.809 + .588i
;; w^9 = -.951 + .309i
;; w^10 = -1
;;
;; Applying the sin/cos values above (and noting that combining r2 and r20, r3 and r19, etc. will simplify calculations):
;; reals:
;; r1     +(r2+r20)     +(r3+r19)     +(r4+r18)     +(r5+r17) + (r6+r16)     +(r7+r15)     +(r8+r14)     +(r9+r13)     +(r10+r12) + r11
;; r1 +.951(r2+r20) +.809(r3+r19) +.588(r4+r18) +.309(r5+r17)            -.309(r7+r15) -.588(r8+r14) -.809(r9+r13) -.951(r10+r12) - r11
;; r1 +.809(r2+r20) +.309(r3+r19) -.309(r4+r18) -.809(r5+r17) - (r6+r16) -.809(r7+r15) -.309(r8+r14) +.309(r9+r13) +.809(r10+r12) + r11
;; r1 +.588(r2+r20) -.309(r3+r19) -.951(r4+r18) -.809(r5+r17)            +.809(r7+r15) +.951(r8+r14) +.309(r9+r13) -.588(r10+r12) - r11
;; r1 +.309(r2+r20) -.809(r3+r19) -.809(r4+r18) +.309(r5+r17) + (r6+r16) +.309(r7+r15) -.809(r8+r14) -.809(r9+r13) +.309(r10+r12) + r11
;; r1                   -(r3+r19)                   +(r5+r17)                -(r7+r15)                   +(r9+r13)                - r11
;; r1 -.309(r2+r20) -.809(r3+r19) +.809(r4+r18) +.309(r5+r17) - (r6+r16) +.309(r7+r15) +.809(r8+r14) -.809(r9+r13) -.309(r10+r12) + r11
;; r1 -.588(r2+r20) -.309(r3+r19) +.951(r4+r18) -.809(r5+r17)            +.809(r7+r15) -.951(r8+r14) +.309(r9+r13) +.588(r10+r12) - r11
;; r1 -.809(r2+r20) +.309(r3+r19) +.309(r4+r18) -.809(r5+r17) + (r6+r16) -.809(r7+r15) +.309(r8+r14) +.309(r9+r13) -.809(r10+r12) + r11
;; r1 -.951(r2+r20) +.809(r3+r19) -.588(r4+r18) +.309(r5+r17)            -.309(r7+r15) +.588(r8+r14) -.809(r9+r13) +.951(r10+r12) - r11
;; r1     -(r2+r20)     +(r3+r19)     -(r4+r18)     +(r5+r17) - (r6+r16)     +(r7+r15)     -(r8+r14)     +(r9+r13)     -(r10+r12) + r11
;;
;; imaginarys:
;; 0
;; +.309(r2-r20) +.588(r3-r19) +.809(r4-r18) +.951(r5-r17) + (r6-r16) +.951(r7-r15) +.809(r8-r14) +.588(r9-r13) +.309(r10-r12)
;; +.588(r2-r20) +.951(r3-r19) +.951(r4-r18) +.588(r5-r17)            -.588(r7-r15) -.951(r8-r14) -.951(r9-r13) -.588(r10-r12)
;; +.809(r2-r20) +.951(r3-r19) +.309(r4-r18) -.588(r5-r17) - (r6-r16) -.588(r7-r15) +.309(r8-r14) +.951(r9-r13) +.809(r10-r12)
;; +.951(r2-r20) +.588(r3-r19) -.588(r4-r18) -.951(r5-r17)            +.951(r7-r15) +.588(r8-r14) -.588(r9-r13) -.951(r10-r12)
;;      (r2-r20)                   -(r4-r18)               + (r6-r16)                   -(r8-r14)                   +(r10-r12)
;; +.951(r2-r20) -.588(r3-r19) -.588(r4-r18) +.951(r5-r17)            -.951(r7-r15) +.588(r8-r14) +.588(r9-r13) -.951(r10-r12)
;; +.809(r2-r20) -.951(r3-r19) +.309(r4-r18) +.588(r5-r17) - (r6-r16) +.588(r7-r15) +.309(r8-r14) -.951(r9-r13) +.809(r10-r12)
;; +.588(r2-r20) -.951(r3-r19) +.951(r4-r18) -.588(r5-r17)            +.588(r7-r15) -.951(r8-r14) +.951(r9-r13) -.588(r10-r12)
;; +.309(r2-r20) -.588(r3-r19) +.809(r4-r18) -.951(r5-r17) + (r6-r16) -.951(r7-r15) +.809(r8-r14) -.588(r9-r13) +.309(r10-r12)
;; 0
;;
;; There are many more symmetries we can take advantage of.   For example, the (r2+/-r20) column
;; always has the same multiplier as the (r10+/-r12) column.  This is true for all the "even" columns.
;; Also the computations for the 2nd row are very similar to the computations for the 10th row,
;; the 3rd row are similar to the 9th, etc.  Finally, note that for the odd columns, there are
;; only two multipliers to apply and can be combined with every fourth column.
;;
;; Lastly, output would normally be 9 complex and 2 reals but the users of this routine
;; expect us to "back up" the 2 reals by one level.  That is:
;;	real #1A:  r1 + r3+r19 + r5+r17 + ...
;;	real #1B:  r2+r20 + r4+r18 + ...

yr5_10cl_20_reals_fft_preload MACRO
	ENDM

yr5_10cl_20_reals_fft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd

	;; Do the odd columns for the real results

	vmovapd	ymm0, [srcreg+2*d2]		;; r5
	vaddpd	ymm0, ymm0, [srcreg+3*d2+32]	;; r5+r17
	vmovapd	ymm3, YMM_P309
	vmulpd	ymm1, ymm3, ymm0		;; .309(r5+r17)
	vmovapd	ymm7, YMM_P809
	vmulpd	ymm4, ymm7, ymm0		;; .809(r5+r17)
	vmovapd	ymm2, [srcreg]			;; r1
	vaddpd	ymm0, ymm2, ymm0		;; r1+(r5+r17)
	vaddpd	ymm1, ymm2, ymm1		;; r1+.309(r5+r17)
	vsubpd	ymm2, ymm2, ymm4		;; r1-.809(r5+r17)

	vmovapd	ymm4, [srcreg+4*d2]		;; r9
	vaddpd	ymm4, ymm4, [srcreg+d2+32]	;; r9+r13
	vmulpd	ymm5, ymm7, ymm4		;; .809(r9+r13)
	vaddpd	ymm0, ymm0, ymm4		;; r1+(r5+r17)+(r9+r13)
	vmulpd	ymm4, ymm3, ymm4		;; .309(r9+r13)
	vsubpd	ymm1, ymm1, ymm5		;; r1+.309(r5+r17)-.809(r9+r13)
	vaddpd	ymm2, ymm2, ymm4		;; r1-.809(r5+r17)+.309(r9+r13)

	vmovapd	ymm5, [srcreg+d2]		;; r3
	vaddpd	ymm5, ymm5, [srcreg+4*d2+32]	;; r3+r19
	vmulpd	ymm6, ymm7, ymm5		;; .809(r3+r19)
	vmulpd	ymm7, ymm3, ymm5		;; .309(r3+r19)
	vmovapd	ymm4, [srcreg+32]		;; r11
	vaddpd	ymm5, ymm5, ymm4		;; (r3+r19)+r11
	vsubpd	ymm6, ymm6, ymm4		;; .809(r3+r19)-r11
	vaddpd	ymm7, ymm7, ymm4		;; .309(r3+r19)+r11

	vmovapd	ymm4, [srcreg+3*d2]		;; r7
	vaddpd	ymm4, ymm4, [srcreg+2*d2+32]	;; r7+r15
	vmulpd	ymm3, ymm3, ymm4		;; .309(r7+r15)
	vaddpd	ymm5, ymm5, ymm4		;; (r3+r19)+(r7+r15)+r11
	vmulpd	ymm4, ymm4, YMM_P809		;; .809(r7+r15)
	vsubpd	ymm6, ymm6, ymm3		;; .809(r3+r19)-.309(r7+r15)-r11
	vsubpd	ymm7, ymm7, ymm4		;; .309(r3+r19)-.809(r7+r15)+r11

	vsubpd	ymm3, ymm0, ymm5		;; Real odd-cols row #6 (final real #6)
	vaddpd	ymm0, ymm0, ymm5		;; Real odd-cols row #1 (final real #1A)

	vsubpd	ymm4, ymm1, ymm6		;; Real odd-cols row #5
	vaddpd	ymm1, ymm1, ymm6		;; Real odd-cols row #2

	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	ymm5, ymm2, ymm7		;; Real odd-cols row #4
	vaddpd	ymm2, ymm2, ymm7		;; Real odd-cols row #3

	vmovapd	YMM_TMPS[4*32],ymm3		;; Real #6
	vmovapd	[srcreg], ymm0			;; Final real #1A
	vmovapd	YMM_TMPS[3*32], ymm4		;; Real odd-cols row #5
	vmovapd	YMM_TMPS[0*32], ymm1		;; Real odd-cols row #2
	vmovapd	YMM_TMPS[2*32], ymm5		;; Real odd-cols row #4
	vmovapd	YMM_TMPS[1*32], ymm2		;; Real odd-cols row #3

	;; Do the even columns for the real results

	vmovapd	ymm7, [srcreg+d1]		;; r2
	vaddpd	ymm7, ymm7, [srcreg+4*d2+d1+32]	;; r2+r20
	vmovapd	ymm0, [srcreg+4*d2+d1]		;; r10
	vaddpd	ymm0, ymm0, [srcreg+d1+32]	;; r10+r12
	vsubpd	ymm1, ymm7, ymm0		;; (r2+r20)-(r10+r12)
	vaddpd	ymm7, ymm7, ymm0		;; (r2+r20)+(r10+r12)

	vmovapd	ymm2, YMM_P951
	vmulpd	ymm6, ymm2, ymm1		;; .951((r2+r20)-(r10+r12))
	vmovapd	ymm5, YMM_P588
	vmulpd	ymm1, ymm5, ymm1		;; .588((r2+r20)-(r10+r12))

	vmovapd	ymm4, [srcreg+d2+d1]		;; r4
	vaddpd	ymm4, ymm4, [srcreg+3*d2+d1+32]	;; r4+r18
	vmovapd	ymm3, [srcreg+3*d2+d1]		;; r8
	vaddpd	ymm3, ymm3, [srcreg+d2+d1+32]	;; r8+r14
	vsubpd	ymm0, ymm4, ymm3		;; (r4+r18)-(r8+r14)
	vaddpd	ymm4, ymm4, ymm3		;; (r4+r18)+(r8+r14)

	L1prefetchw srcreg+d1+L1pd, L1pt

	vmulpd	ymm5, ymm5, ymm0		;; .588((r4+r18)-(r8+r14))
	vaddpd	ymm6, ymm6, ymm5		;; .951((r2+r20)-(r10+r12))+.588((r4+r18)-(r8+r14))
	vmulpd	ymm2, ymm2, ymm0		;; .951((r4+r18)-(r8+r14))
	vsubpd	ymm1, ymm1, ymm2		;; .588((r2+r20)-(r10+r12))-.951((r4+r18)-(r8+r14))

	vmovapd	ymm2, YMM_P809
	vmulpd	ymm0, ymm2, ymm7		;; .809((r2+r20)+(r10+r12))
	vmovapd	ymm3, YMM_P309
	vmulpd	ymm5, ymm3, ymm7		;; .309((r2+r20)+(r10+r12))

	vmovapd	YMM_TMPS[5*32], ymm6		;; Save real even-cols row #2
	vmovapd	YMM_TMPS[6*32], ymm1		;; Save real even-cols row #4

	vaddpd	ymm7, ymm7, ymm4		;; ((r2+r20)+(r10+r12))+((r4+r18)+(r8+r14))
	vmulpd	ymm1, ymm3, ymm4		;; .309((r4+r18)+(r8+r14))
	vsubpd	ymm0, ymm0, ymm1		;; .809((r2+r20)+(r10+r12))-.309((r4+r18)+(r8+r14))
	vmulpd	ymm1, ymm2, ymm4		;; .809((r4+r18)+(r8+r14))
	vsubpd	ymm5, ymm5, ymm1		;; .309((r2+r20)+(r10+r12))-.809((r4+r18)+(r8+r14))

	vmovapd	ymm1, [srcreg+2*d2+d1]		;; r6
	vmovapd	ymm6, [srcreg+2*d2+d1+32]	;; r16
	vsubpd	ymm4, ymm1, ymm6		;; r6-r16
	vaddpd	ymm1, ymm1, ymm6		;; r6+r16
	vaddpd	ymm7, ymm7, ymm1		;; ((r2+r20)+(r10+r12))+((r4+r18)+(r8+r14))+(r6+r16)
	vsubpd	ymm0, ymm0, ymm1		;; .809((r2+r20)+(r10+r12))-.309((r4+r18)+(r8+r14))-(r6+r16)
	vaddpd	ymm5, ymm5, ymm1		;; .309((r2+r20)+(r10+r12))-.809((r4+r18)+(r8+r14))+(r6+r16)

	vmovapd	[srcreg+32], ymm7		;; Save final real #1B (real even-cols row #1)
	vmovapd	YMM_TMPS[7*32], ymm0		;; Save real even-cols row #3
	vmovapd	YMM_TMPS[8*32], ymm5		;; Save real even-cols row #5

	;; Do the even columns for the imaginary results

	vmovapd	ymm1, [srcreg+d1]		;; r2
	vsubpd	ymm1, ymm1, [srcreg+4*d2+d1+32]	;; r2-r20
	vmovapd	ymm6, [srcreg+4*d2+d1]		;; r10
	vsubpd	ymm6, ymm6, [srcreg+d1+32]	;; r10-r12
	vaddpd	ymm7, ymm1, ymm6		;; (r2-r20)+(r10-r12)
	vsubpd	ymm1, ymm1, ymm6		;; (r2-r20)-(r10-r12)

	vmulpd	ymm0, ymm3, ymm7		;; .309((r2-r20)+(r10-r12))
	vmulpd	ymm5, ymm2, ymm7		;; .809((r2-r20)+(r10-r12))

	L1prefetchw srcreg+d2+L1pd, L1pt

	vaddpd	ymm7, ymm7, ymm4		;; ((r2-r20)+(r10-r12))+(r6-r16)
	vaddpd	ymm0, ymm0, ymm4		;; .309((r2-r20)+(r10-r12))+(r6-r16)
	vsubpd	ymm5, ymm5, ymm4		;; .809((r2-r20)+(r10-r12))-(r6-r16)

	vmovapd	ymm4, [srcreg+d2+d1]		;; r4
	vsubpd	ymm4, ymm4, [srcreg+3*d2+d1+32]	;; r4-r18
	vmovapd	ymm6, [srcreg+3*d2+d1]		;; r8
	vsubpd	ymm6, ymm6, [srcreg+d2+d1+32]	;; r8-r14

	vaddpd	ymm3, ymm4, ymm6		;; (r4-r18)+(r8-r14)
	vsubpd	ymm4, ymm4, ymm6		;; (r4-r18)-(r8-r14)

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm7, ymm7, ymm3		;; ((r2-r20)+(r10-r12))-((r4-r18)+(r8-r14))+(r6-r16)
	vmulpd	ymm2, ymm2, ymm3		;; .809((r4-r18)+(r8-r14))
	vaddpd	ymm0, ymm0, ymm2		;; .309((r2-r20)+(r10-r12))+.809((r4-r18)+(r8-r14))+(r6-r16)
	vmulpd	ymm3, ymm3, YMM_P309		;; .309((r4-r18)+(r8-r14))
	vaddpd	ymm5, ymm5, ymm3		;; .809((r2-r20)+(r10-r12))+.309((r4-r18)+(r8-r14))-(r6-r16)

	vmovapd	YMM_TMPS[9*32], ymm7		;; Save imag row #6
	vmovapd	YMM_TMPS[10*32], ymm0		;; Save imag even-cols row #2
	vmovapd	YMM_TMPS[11*32], ymm5		;; Save imag even-cols row #4

	vmovapd	ymm2, YMM_P588
	vmulpd	ymm0, ymm2, ymm1		;; .588((r2-r20)-(r10-r12))
	vmovapd	ymm3, YMM_P951
	vmulpd	ymm5, ymm3, ymm1		;; .951((r2-r20)-(r10-r12))

	L1prefetchw srcreg+2*d2+L1pd, L1pt

	vmulpd	ymm6, ymm3, ymm4		;; .951((r4-r18)-(r8-r14))
	vaddpd	ymm0, ymm0, ymm6		;; .588((r2-r20)-(r10-r12))+.951((r4-r18)-(r8-r14))
	vmulpd	ymm4, ymm2, ymm4		;; .588((r4-r18)-(r8-r14))
	vsubpd	ymm5, ymm5, ymm4		;; .951((r2-r20)-(r10-r12))-.588((r4-r18)-(r8-r14))

	vmovapd	YMM_TMPS[12*32], ymm0		;; Save imag even-cols row #3
	vmovapd	YMM_TMPS[13*32], ymm5		;; Save imag even-cols row #5

	;; Do the odd columns for the imag results

	vmovapd	ymm1, [srcreg+2*d2]		;; r5
	vsubpd	ymm1, ymm1, [srcreg+3*d2+32]	;; r5-r17
	vmulpd	ymm6, ymm3, ymm1		;; .951(r5-r17)
	vmulpd	ymm7, ymm2, ymm1		;; .588(r5-r17)

	vmovapd	ymm4, [srcreg+4*d2]		;; r9
	vsubpd	ymm4, ymm4, [srcreg+d2+32]	;; r9-r13
	vmulpd	ymm0, ymm2, ymm4		;; .588(r9-r13)
	vmulpd	ymm4, ymm3, ymm4		;; .951(r9-r13)
	vaddpd	ymm6, ymm6, ymm0		;; .951(r5-r17)+.588(r9-r13)
	vsubpd	ymm7, ymm7, ymm4		;; .588(r5-r17)-.951(r9-r13)

	vmovapd	ymm5, [srcreg+d2]		;; r3
	vsubpd	ymm5, ymm5, [srcreg+4*d2+32]	;; r3-r19
	vmulpd	ymm1, ymm2, ymm5		;; .588(r3-r19)
	vmulpd	ymm5, ymm3, ymm5		;; .951(r3-r19)

	vmovapd	ymm0, [srcreg+3*d2]		;; r7
	vsubpd	ymm0, ymm0, [srcreg+2*d2+32]	;; r7-r15
	vmulpd	ymm4, ymm3, ymm0		;; .951(r7-r15)
	vmulpd	ymm0, ymm2, ymm0		;; .588(r7-r15)
	vaddpd	ymm1, ymm1, ymm4		;; .588(r3-r19)+.951(r7-r15)
	vsubpd	ymm5, ymm5, ymm0		;; .951(r3-r19)-.588(r7-r15)

	vaddpd	ymm2, ymm1, ymm6		;; Imag odd-cols row #2
	vsubpd	ymm1, ymm1, ymm6		;; Imag odd-cols row #5

	L1prefetchw srcreg+2*d2+d1+L1pd, L1pt

	vaddpd	ymm3, ymm5, ymm7		;; Imag odd-cols row #3
	vsubpd	ymm5, ymm5, ymm7		;; Imag odd-cols row #4

	vmovapd	YMM_TMPS[17*32], ymm1		;; Imag odd-cols row #5
	vmovapd	YMM_TMPS[15*32], ymm3		;; Imag odd-cols row #3
	vmovapd	YMM_TMPS[16*32], ymm5		;; Imag odd-cols row #4

	;; Now combine the even and odd columns then do the post-multiply by twiddle factors.

	vmovapd	ymm1, YMM_TMPS[0*32]		;; Real odd-cols row #2
	vmovapd	ymm4, YMM_TMPS[5*32]		;; Real even-cols row #2
	vsubpd	ymm0, ymm1, ymm4		;; Real #10
	vaddpd	ymm1, ymm1, ymm4		;; Real #2
	vmovapd	ymm4, YMM_TMPS[10*32]		;; Imag even-cols row #2
	vsubpd	ymm3, ymm4, ymm2		;; Imag #10
	vaddpd	ymm2, ymm4, ymm2		;; Imag #2

	vmovapd	ymm5, [screg+8*64+32]		;; cosine/sine
	vmulpd	ymm6, ymm0, ymm5		;; A10 = R10 * cosine/sine
	vsubpd	ymm6, ymm6, ymm3		;; A10 = A10 - I10
	vmulpd	ymm3, ymm3, ymm5		;; B10 = I10 * cosine/sine
	vmovapd	ymm4, [screg+32]		;; cosine/sine
	vmulpd	ymm7, ymm1, ymm4		;; A2 = R2 * cosine/sine
	vsubpd	ymm7, ymm7, ymm2		;; A2 = A2 - I2
	vmulpd	ymm2, ymm2, ymm4		;; B2 = I2 * cosine/sine
	vaddpd	ymm3, ymm3, ymm0		;; B10 = B10 + R10
	vaddpd	ymm2, ymm2, ymm1		;; B2 = B2 + R2
	vmulpd	ymm6, ymm6, [screg+8*64]	;; A10 = A10 * sine (final R10)
	vmulpd	ymm3, ymm3, [screg+8*64]	;; B10 = B10 * sine (final I10)
	vmulpd	ymm7, ymm7, [screg]		;; A2 = A2 * sine (final R2)
	vmulpd	ymm2, ymm2, [screg]		;; B2 = B2 * sine (final I2)
	vmovapd	[srcreg+4*d2+d1], ymm6		;; Save final R10
	vmovapd	[srcreg+4*d2+d1+32], ymm3	;; Save final I10
	vmovapd	[srcreg+d1], ymm7		;; Save final R2
	vmovapd	[srcreg+d1+32], ymm2		;; Save final I2

	vmovapd	ymm1, YMM_TMPS[1*32]		;; Real odd-cols row #3
	vmovapd	ymm2, YMM_TMPS[7*32]		;; Real even-cols row #3
	vsubpd	ymm0, ymm1, ymm2		;; Real #9
	vaddpd	ymm1, ymm1, ymm2		;; Real #3
	vmovapd	ymm3, YMM_TMPS[12*32]		;; Imag even-cols row #3
	vmovapd	ymm4, YMM_TMPS[15*32]		;; Imag odd-cols row #3
	vsubpd	ymm2, ymm3, ymm4		;; Imag #9
	vaddpd	ymm3, ymm3, ymm4		;; Imag #3

	vmovapd	ymm5, [screg+7*64+32]		;; cosine/sine
	vmulpd	ymm6, ymm0, ymm5		;; A9 = R9 * cosine/sine
	vsubpd	ymm6, ymm6, ymm2		;; A9 = A9 - I9
	vmulpd	ymm2, ymm2, ymm5		;; B9 = I9 * cosine/sine
	vmovapd	ymm4, [screg+64+32]		;; cosine/sine
	vmulpd	ymm7, ymm1, ymm4		;; A3 = R3 * cosine/sine
	vsubpd	ymm7, ymm7, ymm3		;; A3 = A3 - I3
	vmulpd	ymm3, ymm3, ymm4		;; B3 = I3 * cosine/sine
	vaddpd	ymm2, ymm2, ymm0		;; B9 = B9 + R9
	vaddpd	ymm3, ymm3, ymm1		;; B3 = B3 + R3
	L1prefetchw srcreg+3*d2+L1pd, L1pt
	vmulpd	ymm6, ymm6, [screg+7*64]	;; A9 = A9 * sine (final R9)
	vmulpd	ymm2, ymm2, [screg+7*64]	;; B9 = B9 * sine (final I9)
	vmulpd	ymm7, ymm7, [screg+64]		;; A3 = A3 * sine (final R3)
	vmulpd	ymm3, ymm3, [screg+64]		;; B3 = B3 * sine (final I3)
	vmovapd	[srcreg+4*d2], ymm6		;; Save final R9
	vmovapd	[srcreg+4*d2+32], ymm2		;; Save final I9
	vmovapd	[srcreg+d2], ymm7		;; Save final R3
	vmovapd	[srcreg+d2+32], ymm3		;; Save final I3

	vmovapd	ymm1, YMM_TMPS[2*32]		;; Real odd-cols row #4
	vmovapd	ymm2, YMM_TMPS[6*32]		;; Real even-cols row #4
	vsubpd	ymm0, ymm1, ymm2		;; Real #8
	vaddpd	ymm1, ymm1, ymm2		;; Real #4
	vmovapd	ymm3, YMM_TMPS[11*32]		;; Imag even-cols row #4
	vmovapd	ymm4, YMM_TMPS[16*32]		;; Imag odd-cols row #4
	vsubpd	ymm2, ymm3, ymm4		;; Imag #8
	vaddpd	ymm3, ymm3, ymm4		;; Imag #4

	vmovapd	ymm5, [screg+6*64+32]		;; cosine/sine
	vmulpd	ymm6, ymm0, ymm5		;; A8 = R8 * cosine/sine
	vsubpd	ymm6, ymm6, ymm2		;; A8 = A8 - I8
	vmulpd	ymm2, ymm2, ymm5		;; B8 = I8 * cosine/sine
	vmovapd	ymm4, [screg+2*64+32]		;; cosine/sine
	vmulpd	ymm7, ymm1, ymm4		;; A4 = R4 * cosine/sine
	vsubpd	ymm7, ymm7, ymm3		;; A4 = A4 - I4
	vmulpd	ymm3, ymm3, ymm4		;; B4 = I4 * cosine/sine
	vaddpd	ymm2, ymm2, ymm0		;; B8 = B8 + R8
	vaddpd	ymm3, ymm3, ymm1		;; B4 = B4 + R4
	L1prefetchw srcreg+3*d2+d1+L1pd, L1pt
	vmulpd	ymm6, ymm6, [screg+6*64]	;; A8 = A8 * sine (final R8)
	vmulpd	ymm2, ymm2, [screg+6*64]	;; B8 = B8 * sine (final I8)
	vmulpd	ymm7, ymm7, [screg+2*64]	;; A4 = A4 * sine (final R4)
	vmulpd	ymm3, ymm3, [screg+2*64]	;; B4 = B4 * sine (final I4)
	vmovapd	[srcreg+3*d2+d1], ymm6		;; Save final R8
	vmovapd	[srcreg+3*d2+d1+32], ymm2	;; Save final I8
	vmovapd	[srcreg+d2+d1], ymm7		;; Save final R4
	vmovapd	[srcreg+d2+d1+32], ymm3		;; Save final I4

	vmovapd	ymm1, YMM_TMPS[3*32]		;; Real odd-cols row #5
	vmovapd	ymm2, YMM_TMPS[8*32]		;; Real even-cols row #5
	vsubpd	ymm0, ymm1, ymm2		;; Real #7
	vaddpd	ymm1, ymm1, ymm2		;; Real #5
	vmovapd	ymm3, YMM_TMPS[13*32]		;; Imag even-cols row #5
	vmovapd	ymm4, YMM_TMPS[17*32]		;; Imag odd-cols row #5
	vsubpd	ymm2, ymm3, ymm4		;; Imag #7
	vaddpd	ymm3, ymm3, ymm4		;; Imag #5

	vmovapd	ymm5, [screg+5*64+32]		;; cosine/sine
	vmulpd	ymm6, ymm0, ymm5		;; A7 = R7 * cosine/sine
	vsubpd	ymm6, ymm6, ymm2		;; A7 = A7 - I7
	vmulpd	ymm2, ymm2, ymm5		;; B7 = I7 * cosine/sine
	vmovapd	ymm4, [screg+3*64+32]		;; cosine/sine
	vmulpd	ymm7, ymm1, ymm4		;; A5 = R5 * cosine/sine
	vsubpd	ymm7, ymm7, ymm3		;; A5 = A5 - I5
	vmulpd	ymm3, ymm3, ymm4		;; B5 = I5 * cosine/sine
	vaddpd	ymm2, ymm2, ymm0		;; B7 = B7 + R7
	vaddpd	ymm3, ymm3, ymm1		;; B5 = B5 + R5
	L1prefetchw srcreg+4*d2+L1pd, L1pt
	vmulpd	ymm6, ymm6, [screg+5*64]	;; A7 = A7 * sine (final R7)
	vmulpd	ymm2, ymm2, [screg+5*64]	;; B7 = B7 * sine (final I7)
	vmulpd	ymm7, ymm7, [screg+3*64]	;; A5 = A5 * sine (final R5)
	vmulpd	ymm3, ymm3, [screg+3*64]	;; B5 = B5 * sine (final I5)
	vmovapd	[srcreg+3*d2], ymm6		;; Save final R7
	vmovapd	[srcreg+3*d2+32], ymm2		;; Save final I7
	vmovapd	[srcreg+2*d2], ymm7		;; Save final R5
	vmovapd	[srcreg+2*d2+32], ymm3		;; Save final I5

	vmovapd	ymm1, YMM_TMPS[4*32]		;; Real #6
	vmovapd	ymm2, YMM_TMPS[9*32]		;; Imag #6
	vmovapd	ymm5, [screg+4*64+32]		;; cosine/sine
	vmulpd	ymm0, ymm1, ymm5		;; A6 = R6 * cosine/sine
	vsubpd	ymm0, ymm0, ymm2		;; A6 = A6 - I6
	L1prefetchw srcreg+4*d2+d1+L1pd, L1pt
	vmulpd	ymm2, ymm2, ymm5		;; B6 = I6 * cosine/sine
	vaddpd	ymm2, ymm2, ymm1		;; B6 = B6 + R6
	vmulpd	ymm0, ymm0, [screg+4*64]	;; A6 = A6 * sine (final R6)
	vmulpd	ymm2, ymm2, [screg+4*64]	;; B6 = B6 * sine (final I6)
	vmovapd	[srcreg+2*d2+d1], ymm0		;; Save final R6
	vmovapd	[srcreg+2*d2+d1+32], ymm2	;; Save final I6

	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; 64-bit version

IFDEF X86_64

yr5_10cl_20_reals_fft_preload MACRO
	vmovapd	ymm12, YMM_P309
	vmovapd	ymm13, YMM_P809
	vmovapd	ymm14, YMM_P588
	vmovapd	ymm15, YMM_P951
	ENDM

yr5_10cl_20_reals_fft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd	ymm0, [srcreg+2*d2]		;; r5
	vaddpd	ymm0, ymm0, [srcreg+3*d2+32]	;; r5+r17						; 1-3

	vmovapd	ymm1, [srcreg+d2]		;; r3
	vaddpd	ymm1, ymm1, [srcreg+4*d2+32]	;; r3+r19						; 2-4

	vmovapd	ymm2, [srcreg+4*d2]		;; r9
	vaddpd	ymm2, ymm2, [srcreg+d2+32]	;; r9+r13						; 3-5

	vmovapd	ymm3, [srcreg+3*d2]		;; r7
	vaddpd	ymm3, ymm3, [srcreg+2*d2+32]	;; r7+r15						; 4-6
	vmulpd	ymm4, ymm12, ymm0		;; .309(r5+r17)						;	4-8

	vmovapd	ymm5, [srcreg]			;; r1
	vaddpd	ymm6, ymm5, ymm0		;; r1+(r5+r17)						; 5-7
	vmulpd	ymm0, ymm13, ymm0		;; .809(r5+r17)						;	5-9

	vmovapd	ymm7, [srcreg+32]		;; r11
	vaddpd	ymm8, ymm1, ymm7		;; (r3+r19)+r11						; 6-8
	vmulpd	ymm9, ymm13, ymm1		;; .809(r3+r19)						;	6-10

	vaddpd	ymm6, ymm6, ymm2		;; r1+(r5+r17)+(r9+r13)					; 7-9
	vmulpd	ymm1, ymm12, ymm1		;; .309(r3+r19)						;	7-11

	vaddpd	ymm8, ymm8, ymm3		;; (r3+r19)+(r7+r15)+r11				; 8-10
	vmulpd	ymm10, ymm13, ymm2		;; .809(r9+r13)						;	8-12

	L1prefetchw srcreg+L1pd, L1pt

	vaddpd	ymm4, ymm5, ymm4		;; r1+.309(r5+r17)					; 9-11
	vmulpd	ymm2, ymm12, ymm2		;; .309(r9+r13)						;	9-13

	vsubpd	ymm5, ymm5, ymm0		;; r1-.809(r5+r17)					; 10-12
	vmulpd	ymm0, ymm12, ymm3		;; .309(r7+r15)						;	10-14
	vmovapd	ymm11, [srcreg+d1]		;; r2

	vsubpd	ymm9, ymm9, ymm7		;; .809(r3+r19)-r11					; 11-13
	vmulpd	ymm3, ymm13, ymm3		;; .809(r7+r15)						;	11-15

	vaddpd	ymm1, ymm1, ymm7		;; .309(r3+r19)+r11					; 12-14
	vmovapd	ymm7, [srcreg+4*d2+d1]		;; r10

	vsubpd	ymm4, ymm4, ymm10		;; r1+.309(r5+r17)-.809(r9+r13)				; 13-15
	vmovapd	ymm10, [srcreg+d2+d1]		;; r4

	vaddpd	ymm5, ymm5, ymm2		;; r1-.809(r5+r17)+.309(r9+r13)				; 14-16
	vmovapd	ymm2, [srcreg+3*d2+d1]		;; r8

	vsubpd	ymm9, ymm9, ymm0		;; .809(r3+r19)-.309(r7+r15)-r11			; 15-17
	vmovapd	ymm0, [srcreg+2*d2+d1]		;; r6

	vsubpd	ymm1, ymm1, ymm3		;; .309(r3+r19)-.809(r7+r15)+r11			; 16-18

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm3, ymm6, ymm8		;; Real odd-cols row #6 (final real #6)			; 17-19
	vaddpd	ymm6, ymm6, ymm8		;; Real odd-cols row #1 (final real #1A) 		; 18-20

	L1prefetchw srcreg+d2+L1pd, L1pt

	vsubpd	ymm8, ymm4, ymm9		;; Real odd-cols row #5					; 19-21
	vaddpd	ymm4, ymm4, ymm9		;; Real odd-cols row #2					; 20-22

	vsubpd	ymm11, ymm11, [srcreg+4*d2+d1+32] ;; r2-r20						; 21-23
	vmovapd	[srcreg], ymm6			;; Final real #1A					; 21

	vsubpd	ymm7, ymm7, [srcreg+d1+32]	;; r10-r12						; 22-24
	vmovapd	YMM_TMPS[3*32], ymm8		;; Real odd-cols row #5					; 22

	vsubpd	ymm10, ymm10, [srcreg+3*d2+d1+32] ;; r4-r18						; 23-25
	vmovapd	YMM_TMPS[0*32], ymm4		;; Real odd-cols row #2					; 23

	vsubpd	ymm2, ymm2, [srcreg+d2+d1+32]	;; r8-r14						; 24-26

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vaddpd	ymm4, ymm11, ymm7		;; (r2-r20)+(r10-r12)					; 25-27
	vsubpd	ymm11, ymm11, ymm7		;; (r2-r20)-(r10-r12)					; 26-28
	vaddpd	ymm7, ymm10, ymm2		;; (r4-r18)+(r8-r14)					; 27-29
	vsubpd	ymm10, ymm10, ymm2		;; (r4-r18)-(r8-r14)					; 28-30

	vsubpd	ymm0, ymm0, [srcreg+2*d2+d1+32]	;; r6-r16						; 29-31

	vmulpd	ymm2, ymm12, ymm4		;; .309((r2-r20)+(r10-r12))				;	28-32
	vmulpd	ymm8, ymm13, ymm4		;; .809((r2-r20)+(r10-r12))				;	29-33

	vsubpd	ymm4, ymm4, ymm7		;; ((r2-r20)+(r10-r12))-((r4-r18)+(r8-r14)) 		; 30-32

	vmulpd	ymm6, ymm13, ymm7		;; .809((r4-r18)+(r8-r14))				;	30-34
	vmulpd	ymm7, ymm12, ymm7		;; .309((r4-r18)+(r8-r14))				;	31-35

	L1prefetchw srcreg+2*d2+L1pd, L1pt

	vsubpd	ymm9, ymm5, ymm1		;; Real odd-cols row #4					; 31-33
	vaddpd	ymm5, ymm5, ymm1		;; Real odd-cols row #3					; 32-34

	vmulpd	ymm1, ymm14, ymm11		;; .588((r2-r20)-(r10-r12))				;	32-36
	vmulpd	ymm11, ymm15, ymm11		;; .951((r2-r20)-(r10-r12))				;	33-37

	vaddpd	ymm4, ymm4, ymm0		;; ((r2-r20)+(r10-r12))-((r4-r18)+(r8-r14))+(r6-r16) 	; 33-35			(I6)
	vmovapd	YMM_TMPS[2*32], ymm9		;; Real odd-cols row #4					; 34

	vmovapd	ymm9, [srcreg+2*d2]		;; r5
	vsubpd	ymm9, ymm9, [srcreg+3*d2+32]	;; r5-r17						; 34-36
	vmovapd	YMM_TMPS[1*32], ymm5		;; Real odd-cols row #3					; 35

	vmulpd	ymm5, ymm15, ymm10		;; .951((r4-r18)-(r8-r14))				;	34-38
	vmulpd	ymm10, ymm14, ymm10		;; .588((r4-r18)-(r8-r14))				;	35-39

	vaddpd	ymm2, ymm2, ymm6		;; .309((r2-r20)+(r10-r12))+.809((r4-r18)+(r8-r14)) 	; 35-37
	vmovapd	ymm6, [srcreg+4*d2]		;; r9

	vaddpd	ymm8, ymm8, ymm7		;; .809((r2-r20)+(r10-r12))+.309((r4-r18)+(r8-r14)) 	; 36-38

	vsubpd	ymm6, ymm6, [srcreg+d2+32]	;; r9-r13						; 37-39
	vmovapd	ymm7, [screg+4*64+32]		;; cosine/sine

	vaddpd	ymm2, ymm2, ymm0		;; .309((r2-r20)+(r10-r12))+.809((r4-r18)+(r8-r14))+(r6-r16) ; 38-40
	vsubpd	ymm8, ymm8, ymm0		;; .809((r2-r20)+(r10-r12))+.309((r4-r18)+(r8-r14))-(r6-r16) ; 39-41

	vmulpd	ymm0, ymm3, ymm7		;; A6 = R6 * cosine/sine				;	36-40
	vmulpd	ymm7, ymm4, ymm7		;; B6 = I6 * cosine/sine				;	37-41

	vmovapd	YMM_TMPS[10*32], ymm2		;; Save imag even-cols row #2				; 41
	vmovapd	ymm2, [srcreg+d2]		;; r3
	vsubpd	ymm2, ymm2, [srcreg+4*d2+32]	;; r3-r19						; 40-42

	L1prefetchw srcreg+2*d2+d1+L1pd, L1pt

	vsubpd	ymm0, ymm0, ymm4		;; A6 = A6 - I6						; 41-43
	vmovapd	ymm4, [srcreg+3*d2]		;; r7

	vaddpd	ymm7, ymm7, ymm3		;; B6 = B6 + R6						; 42-44
	vmovapd	YMM_TMPS[11*32], ymm8		;; Save imag even-cols row #4				; 42

	vmulpd	ymm3, ymm15, ymm9		;; .951(r5-r17)						;	41-45
	vmulpd	ymm9, ymm14, ymm9		;; .588(r5-r17)						;	42-46

	vsubpd	ymm4, ymm4, [srcreg+2*d2+32]	;; r7-r15						; 43-45

	vmulpd	ymm8, ymm14, ymm6		;; .588(r9-r13)						;	43-47
	vmulpd	ymm6, ymm15, ymm6		;; .951(r9-r13)						;	44-48

	vaddpd	ymm1, ymm1, ymm5		;; .588((r2-r20)-(r10-r12))+.951((r4-r18)-(r8-r14))	; 44-46
	vmovapd ymm5, [screg+4*64]		;; sine

	vsubpd	ymm11, ymm11, ymm10		;; .951((r2-r20)-(r10-r12))-.588((r4-r18)-(r8-r14))	; 45-47
	vmovapd	ymm10, [srcreg+d1]		;; r2

	vmulpd	ymm0, ymm0, ymm5		;; A6 = A6 * sine (final R6)				;	45-49
	vmulpd	ymm7, ymm7, ymm5		;; B6 = B6 * sine (final I6)				;	46-50

	vaddpd	ymm10, ymm10, [srcreg+4*d2+d1+32] ;; r2+r20						; 46-48

	vmovapd	ymm5, [srcreg+4*d2+d1]		;; r10
	vaddpd	ymm5, ymm5, [srcreg+d1+32]	;; r10+r12						; 47-49

	vmovapd	YMM_TMPS[4*32], ymm1		;; Save imag even-cols row #3				; 47
	vmulpd	ymm1, ymm14, ymm2		;; .588(r3-r19)						;	47-51

	vmovapd	YMM_TMPS[5*32], ymm11		;; Save imag even-cols row #5				; 48
	vmovapd	ymm11, [srcreg+2*d2+d1]		;; r6
	vaddpd	ymm11, ymm11, [srcreg+2*d2+d1+32] ;; r6+r16						; 48-50
	vmulpd	ymm2, ymm15, ymm2		;; .951(r3-r19)						;	48-52

	vmovapd	[srcreg+2*d2+d1], ymm0		;; Save final R6					; 50
	vmovapd	ymm0, [srcreg+d2+d1]		;; r4
	vaddpd	ymm0, ymm0, [srcreg+3*d2+d1+32] ;; r4+r18						; 49-51

	vmovapd	[srcreg+2*d2+d1+32], ymm7	;; Save final I6					; 51
	vmovapd	ymm7, [srcreg+3*d2+d1]		;; r8
	vaddpd	ymm7, ymm7, [srcreg+d2+d1+32]	;; r8+r14						; 50-52

	vaddpd	ymm3, ymm3, ymm8		;; .951(r5-r17)+.588(r9-r13)				; 51-53
	vmulpd	ymm8, ymm15, ymm4		;; .951(r7-r15)						;	49-53
	vmulpd	ymm4, ymm14, ymm4		;; .588(r7-r15)						;	50-54

	vsubpd	ymm9, ymm9, ymm6		;; .588(r5-r17)-.951(r9-r13)				; 52-54

	L1prefetchw srcreg+3*d2+L1pd, L1pt

	vaddpd	ymm6, ymm10, ymm5		;; (r2+r20)+(r10+r12)					; 53-55
	vsubpd	ymm10, ymm10, ymm5		;; (r2+r20)-(r10+r12)					; 54-56
	vaddpd	ymm5, ymm0, ymm7		;; (r4+r18)+(r8+r14)					; 55-57
	vsubpd	ymm0, ymm0, ymm7		;; (r4+r18)-(r8+r14)					; 56-58

	vmulpd	ymm7, ymm13, ymm6		;; .809((r2+r20)+(r10+r12))				;	56-60

	vaddpd	ymm1, ymm1, ymm8		;; .588(r3-r19)+.951(r7-r15)				; 57-59
	vmulpd	ymm8, ymm12, ymm6		;; .309((r2+r20)+(r10+r12))				;	57-61

	L1prefetchw srcreg+3*d2+d1+L1pd, L1pt

	vsubpd	ymm2, ymm2, ymm4		;; .951(r3-r19)-.588(r7-r15)				; 58-60

	vaddpd	ymm6, ymm6, ymm5		;; ((r2+r20)+(r10+r12))+((r4+r18)+(r8+r14)) 		; 59-61

	vaddpd	ymm4, ymm1, ymm3		;; Imag odd-cols row #2					; 60-62
	vsubpd	ymm1, ymm1, ymm3		;; Imag odd-cols row #5					; 61-63
	vmulpd	ymm3, ymm12, ymm5		;; .309((r4+r18)+(r8+r14))				;	58-62
	vmulpd	ymm5, ymm13, ymm5		;; .809((r4+r18)+(r8+r14))				;	59-63

	vmovapd	YMM_TMPS[7*32], ymm1		;; Imag odd-cols row #5					; 64
	vaddpd	ymm1, ymm2, ymm9		;; Imag odd-cols row #3					; 62-64
	vsubpd	ymm2, ymm2, ymm9		;; Imag odd-cols row #4					; 63-65

	vmulpd	ymm9, ymm15, ymm10		;; .951((r2+r20)-(r10+r12))				;	60-64
	vmulpd	ymm10, ymm14, ymm10		;; .588((r2+r20)-(r10+r12))				;	61-65

	vmovapd	YMM_TMPS[9*32], ymm2		;; Imag odd-cols row #4					; 66
	vmulpd	ymm2, ymm14, ymm0		;; .588((r4+r18)-(r8+r14))				;	62-66
	vmulpd	ymm0, ymm15, ymm0		;; .951((r4+r18)-(r8+r14))				;	63-67

	vsubpd	ymm7, ymm7, ymm3		;; .809((r2+r20)+(r10+r12))-.309((r4+r18)+(r8+r14)) 	; 64-66
	vmovapd	ymm3, YMM_TMPS[1*32]		;; Real odd-cols row #3	

	vsubpd	ymm8, ymm8, ymm5		;; .309((r2+r20)+(r10+r12))-.809((r4+r18)+(r8+r14)) 	; 65-67
	vmovapd	ymm5, YMM_TMPS[4*32]		;; Imag even-cols row #3

	vaddpd	ymm6, ymm6, ymm11		;; ((r2+r20)+(r10+r12))+((r4+r18)+(r8+r14))+(r6+r16) 	; 66-68

	vsubpd	ymm7, ymm7, ymm11		;; .809((r2+r20)+(r10+r12))-.309((r4+r18)+(r8+r14))-(r6+r16) ; 67-69

	vaddpd	ymm8, ymm8, ymm11		;; .309((r2+r20)+(r10+r12))-.809((r4+r18)+(r8+r14))+(r6+r16) ; 68-70
	vmovapd	ymm11, [screg+7*64+32]		;; cosine/sine

	vaddpd	ymm9, ymm9, ymm2		;; .951((r2+r20)-(r10+r12))+.588((r4+r18)-(r8+r14)) 	; 69-71
	vmovapd	ymm2, [screg+64+32]		;; cosine/sine

	vsubpd	ymm10, ymm10, ymm0		;; .588((r2+r20)-(r10+r12))-.951((r4+r18)-(r8+r14)) 	; 70-72
	vmovapd	[srcreg+32], ymm6		;; Save final real #1B (real even-cols row #1)		; 69

	;; Now combine the even and odd columns then do the post-multiply by twiddle factors.

	vsubpd	ymm6, ymm3, ymm7		;; Real #9						; 71-73
	vmovapd	YMM_TMPS[8*32], ymm8		;; Save real even-cols row #5				; 71

	vsubpd	ymm8, ymm5, ymm1		;; Imag #9						; 72-74
	vmovapd	ymm0, YMM_TMPS[10*32]		;; Imag even-cols row #2

	vaddpd	ymm3, ymm3, ymm7		;; Real #3						; 73-75
	vmovapd	ymm7, YMM_TMPS[0*32]		;; Real odd-cols row #2
	vmovapd	YMM_TMPS[6*32], ymm10		;; Save real even-cols row #4				; 73

	vaddpd	ymm5, ymm5, ymm1		;; Imag #3						; 74-76

	vmulpd	ymm1, ymm6, ymm11		;; A9 = R9 * cosine/sine				;	74-78
	vmulpd	ymm11, ymm8, ymm11		;; B9 = I9 * cosine/sine				;	75-79

	vsubpd	ymm10, ymm7, ymm9		;; Real #10						; 75-77
	vaddpd	ymm7, ymm7, ymm9		;; Real #2						; 76-78

	L1prefetchw srcreg+4*d2+L1pd, L1pt

	vsubpd	ymm9, ymm0, ymm4		;; Imag #10						; 77-79
	vaddpd	ymm0, ymm0, ymm4		;; Imag #2						; 78-80

	vmulpd	ymm4, ymm3, ymm2		;; A3 = R3 * cosine/sine				;	76-80
	vmulpd	ymm2, ymm5, ymm2		;; B3 = I3 * cosine/sine				;	77-81

	vsubpd	ymm1, ymm1, ymm8		;; A9 = A9 - I9						; 79-81
	vmovapd	ymm8, [screg+8*64+32]		;; cosine/sine
	vaddpd	ymm11, ymm11, ymm6		;; B9 = B9 + R9						; 80-82
	vmulpd	ymm6, ymm10, ymm8		;; A10 = R10 * cosine/sine				;	78-82

	vsubpd	ymm4, ymm4, ymm5		;; A3 = A3 - I3						; 81-83
	vmovapd	ymm5, [screg+32]		;; cosine/sine
	vaddpd	ymm2, ymm2, ymm3		;; B3 = B3 + R3						; 82-84
	vmulpd	ymm3, ymm7, ymm5		;; A2 = R2 * cosine/sine				;	79-83

	L1prefetchw srcreg+4*d2+d1+L1pd, L1pt

	vmulpd	ymm8, ymm9, ymm8		;; B10 = I10 * cosine/sine				;	80-84
	vmulpd	ymm5, ymm0, ymm5		;; B2 = I2 * cosine/sine				;	81-85

	vsubpd	ymm6, ymm6, ymm9		;; A10 = A10 - I10					; 83-85
	vmovapd	ymm9, [screg+7*64]		;; sine
	vmulpd	ymm1, ymm1, ymm9		;; A9 = A9 * sine (final R9)				;	82-86
	vmulpd	ymm11, ymm11, ymm9		;; B9 = B9 * sine (final I9)				;	83-87
	vmovapd	ymm9, [screg+64]		;; sine

	vsubpd	ymm3, ymm3, ymm0		;; A2 = A2 - I2						; 84-86
	vmovapd ymm0, [screg+8*64]		;; sine

	vaddpd	ymm8, ymm8, ymm10		;; B10 = B10 + R10					; 85-87
	vmovapd ymm10, [screg]			;; sine

	vmulpd	ymm4, ymm4, ymm9		;; A3 = A3 * sine (final R3)				;	84-88
	vmulpd	ymm2, ymm2, ymm9		;; B3 = B3 * sine (final I3)				;	85-89
	vmovapd	ymm9, YMM_TMPS[2*32]		;; Real odd-cols row #4

	vaddpd	ymm5, ymm5, ymm7		;; B2 = B2 + R2						; 86-88
	vmulpd	ymm6, ymm6, ymm0		;; A10 = A10 * sine (final R10)				;	86-90
	vmovapd	ymm7, YMM_TMPS[6*32]		;; Real even-cols row #4

	vmulpd	ymm3, ymm3, ymm10		;; A2 = A2 * sine (final R2)				;	87-91
	vmovapd	[srcreg+4*d2], ymm1		;; Save final R9					; 87
	vsubpd	ymm1, ymm9, ymm7		;; Real #8						; 87-89

	vaddpd	ymm9, ymm9, ymm7		;; Real #4						; 88-90
	vmovapd	ymm7, YMM_TMPS[11*32]		;; Imag even-cols row #4
	vmulpd	ymm8, ymm8, ymm0		;; B10 = B10 * sine (final I10)				;	88-92
	vmovapd	ymm0, YMM_TMPS[9*32]		;; Imag odd-cols row #4
	vmovapd	[srcreg+4*d2+32], ymm11		;; Save final I9					; 88

	vsubpd	ymm11, ymm7, ymm0		;; Imag #8						; 89-91
	vmulpd	ymm5, ymm5, ymm10		;; B2 = B2 * sine (final I2)				;	89-93
	vmovapd	ymm10, [screg+6*64+32]		;; cosine/sine
	vmovapd	[srcreg+d2], ymm4		;; Save final R3					; 89
	vmovapd	ymm4, YMM_TMPS[3*32]		;; Real odd-cols row #5

	vaddpd	ymm7, ymm7, ymm0		;; Imag #4						; 90-92
	vmulpd	ymm0, ymm1, ymm10		;; A8 = R8 * cosine/sine				;	90-94
	vmovapd	[srcreg+d2+32], ymm2		;; Save final I3					; 90
	vmovapd	ymm2, YMM_TMPS[8*32]		;; Real even-cols row #5

	vmovapd	[srcreg+4*d2+d1], ymm6		;; Save final R10					; 91
	vsubpd	ymm6, ymm4, ymm2		;; Real #7						; 91-93

	vaddpd	ymm4, ymm4, ymm2		;; Real #5						; 92-94
	vmovapd	ymm2, YMM_TMPS[5*32]		;; Imag even-cols row #5
	vmovapd	[srcreg+d1], ymm3		;; Save final R2					; 92
	vmovapd	ymm3, YMM_TMPS[7*32]		;; Imag odd-cols row #5

	vmovapd	[srcreg+4*d2+d1+32], ymm8	;; Save final I10					; 93
	vsubpd	ymm8, ymm2, ymm3		;; Imag #7						; 93-95

	vaddpd	ymm2, ymm2, ymm3		;; Imag #5						; 94-96
	vmovapd	ymm3, [screg+2*64+32]		;; cosine/sine
	vmovapd	[srcreg+d1+32], ymm5		;; Save final I2					; 94
	vmulpd	ymm5, ymm9, ymm3		;; A4 = R4 * cosine/sine				;	91-95
	vmulpd	ymm10, ymm11, ymm10		;; B8 = I8 * cosine/sine				;	92-96

	vmulpd	ymm3, ymm7, ymm3		;; B4 = I4 * cosine/sine				;	93-97

	vsubpd	ymm0, ymm0, ymm11		;; A8 = A8 - I8						; 95-97
	vmovapd	ymm11, [screg+5*64+32]		;; cosine/sine
	vsubpd	ymm5, ymm5, ymm7		;; A4 = A4 - I4						; 96-98
	vmulpd	ymm7, ymm6, ymm11		;; A7 = R7 * cosine/sine				;	94-98

	vaddpd	ymm10, ymm10, ymm1		;; B8 = B8 + R8						; 97-99
	vmovapd	ymm1, [screg+3*64+32]		;; cosine/sine
	vaddpd	ymm3, ymm3, ymm9		;; B4 = B4 + R4						; 98-100
	vmulpd	ymm9, ymm4, ymm1		;; A5 = R5 * cosine/sine				;	95-99
	vmulpd	ymm11, ymm8, ymm11		;; B7 = I7 * cosine/sine				;	96-100
	vmulpd	ymm1, ymm2, ymm1		;; B5 = I5 * cosine/sine				;	97-101

	vsubpd	ymm7, ymm7, ymm8		;; A7 = A7 - I7						; 99-101
	vmovapd	ymm8, [screg+6*64]		;; sine
	vmulpd	ymm0, ymm0, ymm8		;; A8 = A8 * sine (final R8)				;	98-102

	vsubpd	ymm9, ymm9, ymm2		;; A5 = A5 - I5						; 100-102
	vmovapd	ymm2, [screg+2*64]		;; sine
	vmulpd	ymm5, ymm5, ymm2		;; A4 = A4 * sine (final R4)				;	99-103
	vmulpd	ymm10, ymm10, ymm8		;; B8 = B8 * sine (final I8)				;	100-104
	vmovapd	ymm8, [screg+5*64]		;; sine
	vmulpd	ymm3, ymm3, ymm2		;; B4 = B4 * sine (final I4)				;	101-105
	vmovapd	ymm2, [screg+3*64]		;; sine

	vaddpd	ymm11, ymm11, ymm6		;; B7 = B7 + R7						; 101-103
	vaddpd	ymm1, ymm1, ymm4		;; B5 = B5 + R5						; 102-104

	vmulpd	ymm7, ymm7, ymm8		;; A7 = A7 * sine (final R7)				;	102-106
	vmulpd	ymm9, ymm9, ymm2		;; A5 = A5 * sine (final R5)				;	103-107
	vmulpd	ymm11, ymm11, ymm8		;; B7 = B7 * sine (final I7)				;	104-108
	vmulpd	ymm1, ymm1, ymm2		;; B5 = B5 * sine (final I5)				;	105-109

	vmovapd	[srcreg+3*d2+d1], ymm0		;; Save final R8					; 103
	vmovapd	[srcreg+d2+d1], ymm5		;; Save final R4					; 104
	vmovapd	[srcreg+3*d2+d1+32], ymm10	;; Save final I8					; 105
	vmovapd	[srcreg+d2+d1+32], ymm3		;; Save final I4					; 106

	vmovapd	[srcreg+3*d2], ymm7		;; Save final R7					; 107
	vmovapd	[srcreg+2*d2], ymm9		;; Save final R5					; 108
	vmovapd	[srcreg+3*d2+32], ymm11		;; Save final I7					; 109
	vmovapd	[srcreg+2*d2+32], ymm1		;; Save final I5					; 110

	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

ENDIF

;;
;; ************************************* 20-reals-last-unfft variants ******************************************
;;

;; These macros produce 20 reals after doing 4.32 levels of the inverse FFT applying
;; the sin/cos multipliers beforehand.  The input is 2 real and 9 complex numbers.

;; To calculate a 20-reals inverse FFT, we calculate 20 real values from 20 complex inputs in a brute force way.
;; First we note that the 20 complex values are computed from the 9 complex and 2 real inputs using Hermetian symmetry, thusly:
;; c1 = r1A + 0*i
;; c2 = r2 + i2*i
;; ...
;; c10 = r10 + i10*i
;; c11 = r1B + 0*i
;; c12 = r10 - i10*i
;; ...
;; c20 = r2 - i2*i 
;;
;; The brute force calculations are:
;;
;; c1 + c2 + ... + c20	*  w^-0000000000...
;; c1 + c2 + ... + c20	*  w^-0123456789A...
;; c1 + c2 + ... + c20	*  w^-02468ACE....
;;    ...
;; c1 + c2 + ... + c20	*  w^-...A987654321
;;
;; The sin/cos values (w = 20th root of unity) are:
;; w^-1 = .951 - .309i
;; w^-2 = .809 - .588i
;; w^-3 = .588 - .809i
;; w^-4 = .309 - .951i
;; w^-5 = 0 - 1i
;; w^-6 = -.309 - .951i
;; w^-7 = -.588 - .809i
;; w^-8 = -.809 - .588i
;; w^-9 = -.951 - .309i
;; w^-10 = -1
;;
;; Applying the sin/cos values above, taking advantage of symmetry, and ignoring a lot of multiplies by 2:
;; r1     +(r2+r10)     +(r3+r9)     +(r4+r8)     +(r5+r7) + r6 + r11
;; r1 +.951(r2-r10) +.809(r3-r9) +.588(r4-r8) +.309(r5-r7)      - r11 +.309(i2+i10) +.588*(i3+i9) +.809(i4+i8) +.951(i5+i7) + i6
;; r1 +.809(r2+r10) +.309(r3+r9) -.309(r4+r8) -.809(r5+r7) - r6 + r11 +.588(i2-i10) +.951*(i3-i9) +.951(i4-i8) +.588(i5-i7)
;; r1 +.588(r2-r10) -.309(r3-r9) -.951(r4-r8) -.809(r5-r7)      - r11 +.809(i2+i10) +.951*(i3+i9) +.309(i4+i8) -.588(i5+i7) - i6
;; r1 +.309(r2+r10) -.809(r3+r9) -.809(r4+r8) +.309(r5+r7) + r6 + r11 +.951(i2-i10) +.588*(i3-i9) -.588(i4-i8) -.951(i5-i7)
;; r1                   -(r3-r9)                  +(r5-r7)      - r11     +(i2+i10)                   -(i4+i8)              + i6
;; r1 -.309(r2+r10) -.809(r3+r9) +.809(r4+r8) +.309(r5+r7) - r6 + r11 +.951(i2-i10) -.588*(i3-i9) -.588(i4-i8) +.951(i5-i7)
;; r1 -.588(r2-r10) -.309(r3-r9) +.951(r4-r8) -.809(r5-r7)      - r11 +.809(i2+i10) -.951*(i3+i9) +.309(i4+i8) +.588(i5+i7) - i6
;; r1 -.809(r2+r10) +.309(r3+r9) +.309(r4+r8) -.809(r5+r7) + r6 + r11 +.588(i2-i10) -.951*(i3-i9) +.951(i4-i8) -.588(i5-i7)
;; r1 -.951(r2-r10) +.809(r3-r9) -.588(r4-r8) +.309(r5-r7)      - r11 +.309(i2+i10) -.588*(i3+i9) +.809(i4+i8) -.951(i5+i7) + i6
;; r1     -(r2+r10)     +(r3+r9)     -(r4+r8)     +(r5+r7) - r6 + r11
;; r1 -.951(r2-r10) +.809(r3-r9) -.588(r4-r8) +.309(r5-r7)      - r11 -.309(i2+i10) +.588*(i3+i9) -.809(i4+i8) +.951(i5+i7)
;; ... r13 thru r20 are the same as r8 through r1 but with the sign of the imaginary component changed.
;;
;; Also remember that due to the funny way we do things reals input r1A = r1+r11 and r1B = r1-11

yr5_10cl_20_reals_unfft_preload MACRO
	ENDM

yr5_10cl_20_reals_unfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd

;; Apply the 9 twiddle factors.  Also do the first +/- on the reals so that
;; we have a perfect balance of adds and multiplies.

	vmovapd	ymm0, [screg+32]		;; cosine/sine
	vmovapd	ymm1, [srcreg+d1]		;; R2
	vmulpd	ymm2, ymm1, ymm0		;; A2 = R2 * cosine/sine
	vmovapd	ymm4, [screg+8*64+32]		;; cosine/sine
	vmovapd	ymm5, [srcreg+4*d2+d1]		;; R10
	vmulpd	ymm6, ymm5, ymm4		;; A10 = R10 * cosine/sine
	vmovapd	ymm3, [srcreg+d1+32]		;; I2
	vaddpd	ymm2, ymm2, ymm3		;; A2 = A2 + I2
	vmulpd	ymm3, ymm3, ymm0		;; B2 = I2 * cosine/sine
	vmovapd	ymm7, [srcreg+4*d2+d1+32]	;; I10
	vaddpd	ymm6, ymm6, ymm7		;; A10 = A10 + I10
	vmulpd	ymm7, ymm7, ymm4		;; B10 = I10 * cosine/sine
	vsubpd	ymm3, ymm3, ymm1		;; B2 = B2 - R2
	vmovapd	ymm1, [screg]			;; sine
	vmulpd	ymm2, ymm2, ymm1		;; R2 = A2 * sine
	vsubpd	ymm7, ymm7, ymm5		;; B10 = B10 - R10
	vmovapd	ymm5, [screg+8*64]		;; sine
	vmulpd	ymm6, ymm6, ymm5		;; R10 = A10 * sine
	vmulpd	ymm3, ymm3, ymm1		;; I2 = B2 * sine
	vmulpd	ymm7, ymm7, ymm5		;; I10 = B10 * sine
	L1prefetchw srcreg+L1pd, L1pt
	vaddpd	ymm1, ymm2, ymm6		;; R2+R10
	vsubpd	ymm2, ymm2, ymm6		;; R2-R10
	vaddpd	ymm5, ymm3, ymm7		;; I2+I10
	vsubpd	ymm3, ymm3, ymm7		;; I2-I10
	vmovapd	YMM_TMPS[16*32], ymm1		;; Save R2+R10
	vmovapd	YMM_TMPS[0*32], ymm2		;; Save R2-R10
	vmovapd	YMM_TMPS[17*32], ymm5		;; Save I2+I10
	vmovapd	YMM_TMPS[1*32], ymm3		;; Save I2-I10

	vmovapd	ymm0, [screg+64+32]		;; cosine/sine
	vmovapd	ymm1, [srcreg+d2]		;; R3
	vmulpd	ymm2, ymm1, ymm0		;; A3 = R3 * cosine/sine
	vmovapd	ymm4, [screg+7*64+32]		;; cosine/sine
	vmovapd	ymm5, [srcreg+4*d2]		;; R9
	vmulpd	ymm6, ymm5, ymm4		;; A9 = R9 * cosine/sine
	vmovapd	ymm3, [srcreg+d2+32]		;; I3
	vaddpd	ymm2, ymm2, ymm3		;; A3 = A3 + I3
	vmulpd	ymm3, ymm3, ymm0		;; B3 = I3 * cosine/sine
	vmovapd	ymm7, [srcreg+4*d2+32]		;; I9
	vaddpd	ymm6, ymm6, ymm7		;; A9 = A9 + I9
	vmulpd	ymm7, ymm7, ymm4		;; B9 = I9 * cosine/sine
	vsubpd	ymm3, ymm3, ymm1		;; B3 = B3 - R3
	vmovapd	ymm1, [screg+64]		;; sine
	vmulpd	ymm2, ymm2, ymm1		;; R3 = A3 * sine
	vsubpd	ymm7, ymm7, ymm5		;; B9 = B9 - R9
	vmovapd	ymm5, [screg+7*64]		;; sine
	vmulpd	ymm6, ymm6, ymm5		;; R9 = A9 * sine
	vmulpd	ymm3, ymm3, ymm1		;; I3 = B3 * sine
	vmulpd	ymm7, ymm7, ymm5		;; I9 = B9 * sine
	L1prefetchw srcreg+d1+L1pd, L1pt
	vaddpd	ymm1, ymm2, ymm6		;; R3+R9
	vsubpd	ymm2, ymm2, ymm6		;; R3-R9
	vaddpd	ymm5, ymm3, ymm7		;; I3+I9
	vsubpd	ymm3, ymm3, ymm7		;; I3-I9
	vmovapd	YMM_TMPS[14*32], ymm1		;; Save R3+R9
	vmovapd	YMM_TMPS[2*32], ymm2		;; Save R3-R9
	vmovapd	YMM_TMPS[15*32], ymm5		;; Save I3+I9
	vmovapd	YMM_TMPS[3*32], ymm3		;; Save I3-I9

	vmovapd	ymm0, [screg+2*64+32]		;; cosine/sine
	vmovapd	ymm1, [srcreg+d2+d1]		;; R4
	vmulpd	ymm2, ymm1, ymm0		;; A4 = R4 * cosine/sine
	vmovapd	ymm4, [screg+6*64+32]		;; cosine/sine
	vmovapd	ymm5, [srcreg+3*d2+d1]		;; R8
	vmulpd	ymm6, ymm5, ymm4		;; A8 = R8 * cosine/sine
	vmovapd	ymm3, [srcreg+d2+d1+32]		;; I4
	vaddpd	ymm2, ymm2, ymm3		;; A4 = A4 + I4
	vmulpd	ymm3, ymm3, ymm0		;; B4 = I4 * cosine/sine
	vmovapd	ymm7, [srcreg+3*d2+d1+32]	;; I8
	vaddpd	ymm6, ymm6, ymm7		;; A8 = A8 + I8
	vmulpd	ymm7, ymm7, ymm4		;; B8 = I8 * cosine/sine
	vsubpd	ymm3, ymm3, ymm1		;; B4 = B4 - R4
	vmovapd	ymm1, [screg+2*64]		;; sine
	vmulpd	ymm2, ymm2, ymm1		;; R4 = A4 * sine
	vsubpd	ymm7, ymm7, ymm5		;; B8 = B8 - R8
	vmovapd	ymm5, [screg+6*64]		;; sine
	vmulpd	ymm6, ymm6, ymm5		;; R8 = A8 * sine
	vmulpd	ymm3, ymm3, ymm1		;; I4 = B4 * sine
	vmulpd	ymm7, ymm7, ymm5		;; I8 = B8 * sine
	L1prefetchw srcreg+d2+L1pd, L1pt
	vaddpd	ymm1, ymm2, ymm6		;; R4+R8
	vsubpd	ymm2, ymm2, ymm6		;; R4-R8
	vaddpd	ymm5, ymm3, ymm7		;; I4+I8
	vsubpd	ymm3, ymm3, ymm7		;; I4-I8
	vmovapd	YMM_TMPS[12*32], ymm1		;; Save R4+R8
	vmovapd	YMM_TMPS[4*32], ymm2		;; Save R4-R8
	vmovapd	YMM_TMPS[13*32], ymm5		;; Save I4+I8
	vmovapd	YMM_TMPS[5*32], ymm3		;; Save I4-I8

	vmovapd	ymm0, [screg+3*64+32]		;; cosine/sine
	vmovapd	ymm1, [srcreg+2*d2]		;; R5
	vmulpd	ymm2, ymm1, ymm0		;; A5 = R5 * cosine/sine
	vmovapd	ymm4, [screg+5*64+32]		;; cosine/sine
	vmovapd	ymm5, [srcreg+3*d2]		;; R7
	vmulpd	ymm6, ymm5, ymm4		;; A7 = R7 * cosine/sine
	vmovapd	ymm3, [srcreg+2*d2+32]		;; I5
	vaddpd	ymm2, ymm2, ymm3		;; A5 = A5 + I5
	vmulpd	ymm3, ymm3, ymm0		;; B5 = I5 * cosine/sine
	vmovapd	ymm7, [srcreg+3*d2+32]		;; I7
	vaddpd	ymm6, ymm6, ymm7		;; A7 = A7 + I7
	vmulpd	ymm7, ymm7, ymm4		;; B7 = I7 * cosine/sine
	vsubpd	ymm3, ymm3, ymm1		;; B5 = B5 - R5
	vmovapd	ymm1, [screg+3*64]		;; sine
	vmulpd	ymm2, ymm2, ymm1		;; R5 = A5 * sine
	vsubpd	ymm7, ymm7, ymm5		;; B7 = B7 - R7
	vmovapd	ymm5, [screg+5*64]		;; sine
	vmulpd	ymm6, ymm6, ymm5		;; R7 = A7 * sine
	vmulpd	ymm3, ymm3, ymm1		;; I5 = B5 * sine
	vmulpd	ymm7, ymm7, ymm5		;; I7 = B7 * sine
	L1prefetchw srcreg+d2+d1+L1pd, L1pt
	vaddpd	ymm1, ymm2, ymm6		;; R5+R7
	vsubpd	ymm2, ymm2, ymm6		;; R5-R7
	vaddpd	ymm5, ymm3, ymm7		;; I5+I7
	vsubpd	ymm3, ymm3, ymm7		;; I5-I7
	vmovapd	YMM_TMPS[10*32], ymm1		;; Save R5+R7
	vmovapd	YMM_TMPS[6*32], ymm2		;; Save R5-R7
	vmovapd	YMM_TMPS[11*32], ymm5		;; Save I5+I7
	vmovapd	YMM_TMPS[7*32], ymm3		;; Save I5-I7

	vmovapd	ymm0, [screg+4*64+32]		;; cosine/sine
	vmovapd	ymm1, [srcreg+2*d2+d1]		;; R6
	vmulpd	ymm2, ymm1, ymm0		;; A6 = R6 * cosine/sine
	vmovapd	ymm3, [srcreg+2*d2+d1+32]	;; I6
	vaddpd	ymm2, ymm2, ymm3		;; A6 = A6 + I6
	vmulpd	ymm3, ymm3, ymm0		;; B6 = I6 * cosine/sine
	vsubpd	ymm3, ymm3, ymm1		;; B6 = B6 - R6
	vmovapd	ymm1, [screg+4*64]		;; sine
	vmulpd	ymm2, ymm2, ymm1		;; R6 = A6 * sine
	vmulpd	ymm3, ymm3, ymm1		;; I6 = B6 * sine
	vmovapd	YMM_TMPS[8*32], ymm2		;; Save R6
	vmovapd	YMM_TMPS[9*32], ymm3		;; Save I6

	;; Calculate even columns derived from real inputs (even rows)

	vmovapd	ymm0, YMM_TMPS[0*32]		;; r2-r10
	vmovapd	ymm6, YMM_P951
	vmulpd	ymm4, ymm6, ymm0		;; .951(r2-r10)
	vmovapd	ymm7, YMM_P588
	vmulpd	ymm5, ymm7, ymm0		;; .588(r2-r10)

	vmovapd	ymm0, YMM_TMPS[4*32]		;; r4-r8
	vmulpd	ymm7, ymm7, ymm0		;; .588(r4-r8)
	vaddpd	ymm4, ymm4, ymm7		;; .951(r2-r10)+.588(r4-r8)
	vmulpd	ymm6, ymm6, ymm0		;; .951(r4-r8)
	vsubpd	ymm5, ymm5, ymm6		;; .588(r2-r10)-.951(r4-r8)

	;; Calculate odd columns derived from real inputs (even rows)

	vmovapd	ymm0, YMM_TMPS[2*32]		;; r3-r9
	vmovapd	ymm2, YMM_P809
	vmulpd	ymm6, ymm2, ymm0		;; .809(r3-r9)
	vmovapd	ymm7, [srcreg+32]		;; r1-r11
	vaddpd	ymm6, ymm7, ymm6		;; r1+.809(r3-r9)-r11
	vmovapd	ymm3, YMM_P309
	vmulpd	ymm1, ymm3, ymm0		;; .309(r3-r9)
	vsubpd	ymm0, ymm0, ymm7		;; -(r1-(r3-r9)-r11)
	vsubpd	ymm7, ymm7, ymm1		;; r1-.309(r3-r9)-r11

	vmovapd	ymm1, YMM_TMPS[6*32]		;; r5-r7
	vsubpd	ymm0, ymm1, ymm0		;; r1-(r3-r9)+(r5-r7)-r11
	vmovapd	[srcreg+2*d2+d1], ymm0		;; Save odd-real-cols row #6 (also is real-cols row #6)
	vmulpd	ymm0, ymm3, ymm1		;; .309(r5-r7)
	vaddpd	ymm6, ymm6, ymm0		;; r1+.809(r3-r9)+.309(r5-r7)-r11
	vmulpd	ymm1, ymm2, ymm1		;; .809(r5-r7)
	vsubpd	ymm7, ymm7, ymm1		;; r1-.309(r3-r9)-.809(r5-r7)-r11

	L1prefetchw srcreg+2*d2+L1pd, L1pt

	;; Combine even and odd columns (even rows)

	vaddpd	ymm0, ymm6, ymm4		;; real-cols row #2 (odd#2 + even#2)
	vsubpd	ymm6, ymm6, ymm4		;; real-cols row #10 (odd#2 - even#2)
	vaddpd	ymm1, ymm7, ymm5		;; real-cols row #4 (odd#4 + even#4)
	vsubpd	ymm7, ymm7, ymm5		;; real-cols row #8 (odd#4 - even#4)

	vmovapd	YMM_TMPS[0*32], ymm0		;; Save real-cols row #2
	vmovapd	YMM_TMPS[2*32], ymm6		;; Save real-cols row #10
	vmovapd	YMM_TMPS[4*32], ymm1		;; Save real-cols row #4
	vmovapd	YMM_TMPS[6*32], ymm7		;; Save real-cols row #8

	;; Calculate even columns derived from real inputs (odd rows)

	vmovapd	ymm0, YMM_TMPS[16*32]		;; r2+r10
	vmulpd	ymm6, ymm2, ymm0		;; .809(r2+r10)
	vmulpd	ymm7, ymm3, ymm0		;; .309(r2+r10)

	vmovapd	ymm1, YMM_TMPS[12*32]		;; r4+r8
	vaddpd	ymm0, ymm0, ymm1		;; (r2+r10)+(r4+r8)
	vmulpd	ymm4, ymm3, ymm1		;; .309(r4+r8)
	vsubpd	ymm6, ymm6, ymm4		;; .809(r2+r10)-.309(r4+r8)
	vmulpd	ymm1, ymm2, ymm1		;; .809(r4+r8)
	vsubpd	ymm7, ymm7, ymm1		;; .309(r2+r10)-.809(r4+r8)

	L1prefetchw srcreg+2*d2+d1+L1pd, L1pt

	vmovapd	ymm4, YMM_TMPS[8*32]		;; r6
	vaddpd	ymm0, ymm0, ymm4		;; (r2+r10)+(r4+r8)+r6
	vsubpd	ymm6, ymm6, ymm4		;; .809(r2+r10)-.309(r4+r8)-r6
	vaddpd	ymm7, ymm7, ymm4		;; .309(r2+r10)-.809(r4+r8)+r6

	;; Calculate odd columns derived from real inputs (odd rows)
	;; From above, even-real-cols row #1,3,5 are in ymm0,ymm6,ymm7

	vmovapd	ymm1, YMM_TMPS[14*32]		;; r3+r9
	vmulpd	ymm4, ymm3, ymm1		;; .309(r3+r9)
	vmulpd	ymm5, ymm2, ymm1		;; .809(r3+r9)
	vmovapd	ymm3, [srcreg]			;; r1+r11
	vaddpd	ymm1, ymm3, ymm1		;; r1+(r3+r9)+r11
	vaddpd	ymm4, ymm3, ymm4		;; r1+.309(r3+r9)+r11
	vsubpd	ymm5, ymm3, ymm5		;; r1-.809(r3+r9)+r11

	L1prefetchw srcreg+3*d2+L1pd, L1pt

	vmovapd	ymm3, YMM_TMPS[10*32]		;; r5+r7
	vaddpd	ymm1, ymm1, ymm3		;; r1+(r3+r9)+(r5+r7)+r11
	vmulpd	ymm2, ymm2, ymm3		;; .809(r5+r7)
	vsubpd	ymm4, ymm4, ymm2		;; r1+.309(r3+r9)-.809(r5+r7)+r11
	vmulpd	ymm3, ymm3, YMM_P309		;; .309(r5+r7)
	vaddpd	ymm5, ymm5, ymm3		;; r1-.809(r3+r9)+.309(r5+r7)+r11

	;; Combine even and odd columns (odd rows)

	vaddpd	ymm3, ymm1, ymm0		;; real-cols row #1 (and final R1)
	vsubpd	ymm1, ymm1, ymm0		;; real-cols row #11 (and final R11)
	vmovapd	[srcreg], ymm3			;; Save final R1
	vmovapd	[srcreg+32], ymm1		;; Save final R11

	vaddpd	ymm0, ymm4, ymm6		;; real-cols row #3
	vsubpd	ymm4, ymm4, ymm6		;; real-cols row #9
	vmovapd	YMM_TMPS[8*32], ymm0		;; Save real-cols row #3
	vmovapd	YMM_TMPS[10*32], ymm4		;; Save real-cols row #9

	vaddpd	ymm0, ymm5, ymm7		;; real-cols row #5
	vsubpd	ymm5, ymm5, ymm7 		;; real-cols row #7
	vmovapd	YMM_TMPS[12*32], ymm0		;; Save real-cols row #5
	vmovapd	YMM_TMPS[14*32], ymm5		;; Save real-cols row #7

	;; Calculate even columns derived from imaginary inputs (even rows)

	vmovapd	ymm0, YMM_TMPS[17*32]		;; i2+i10
	vmovapd	ymm2, YMM_P309
	vmulpd	ymm6, ymm2, ymm0		;; .309(i2+i10)
	vmovapd	ymm3, YMM_P809
	vmulpd	ymm7, ymm3, ymm0		;; .809(i2+i10)

	vmovapd	ymm1, YMM_TMPS[13*32]		;; i4+i8
	vsubpd	ymm0, ymm0, ymm1		;; (i2+i10)-(i4+i8)
	vmulpd	ymm4, ymm3, ymm1		;; .809(i4+i8)
	vaddpd	ymm6, ymm6, ymm4		;; .309(i2+i10)+.809(i4+i8)
	vmulpd	ymm1, ymm2, ymm1		;; .309(i4+i8)
	vaddpd	ymm7, ymm7, ymm1		;; .809(i2+i10)+.309(i4+i8)

	L1prefetchw srcreg+3*d2+d1+L1pd, L1pt

	vmovapd	ymm4, YMM_TMPS[9*32]		;; i6
	vaddpd	ymm0, ymm0, ymm4		;; (i2+i10)-(i4+i8)+i6
	vaddpd	ymm6, ymm6, ymm4		;; .309(i2+i10)+.809(i4+i8)+i6
	vsubpd	ymm7, ymm7, ymm4		;; .809(i2+i10)+.309(i4+i8)-i6

	;; Combine real and imaginary data for row #6

	vmovapd	ymm4, [srcreg+2*d2+d1]		;; Load real-cols row #6
	vaddpd	ymm1, ymm4, ymm0		;; final R6
	vsubpd	ymm4, ymm4, ymm0		;; final R16
	vmovapd	[srcreg+2*d2+d1], ymm1		;; Save R6
	vmovapd	[srcreg+2*d2+d1+32], ymm4	;; Save R16

	;; Calculate odd columns derived from imaginary inputs (even rows)
	;; From above, even-imag-cols row #2,4 are in ymm6, ymm7

	vmovapd	ymm4, YMM_TMPS[15*32]		;; i3+i9
	vmovapd	ymm1, YMM_P588
	vmulpd	ymm3, ymm1, ymm4		;; .588(i3+i9)
	vmovapd	ymm2, YMM_P951
	vmulpd	ymm4, ymm2, ymm4		;; .951(i3+i9)

	vmovapd	ymm0, YMM_TMPS[11*32]		;; i5+i7
	vmulpd	ymm5, ymm2, ymm0		;; .951(i5+i7)
	vaddpd	ymm3, ymm3, ymm5		;; .588(i3+i9)+.951(i5+i7)
	vmulpd	ymm0, ymm1, ymm0		;; .588(i5+i7)
	vsubpd	ymm4, ymm4, ymm0		;; .951(i3+i9)-.588(i5+i7)

	;; Combine even and odd columns, then real and imag data (even rows)

	L1prefetchw srcreg+4*d2+L1pd, L1pt

	vsubpd	ymm0, ymm6, ymm3		;; imag-cols row #10 (even#2 - odd#2)
	vaddpd	ymm6, ymm6, ymm3		;; imag-cols row #2 (even#2 + odd#2)
	vsubpd	ymm5, ymm7, ymm4		;; imag-cols row #8 (even#4 - odd#4)
	vaddpd	ymm7, ymm7, ymm4		;; imag-cols row #4 (even#4 + odd#4)

	vmovapd	ymm3, YMM_TMPS[2*32]		;; Load real-cols row #10
	vaddpd	ymm4, ymm3, ymm0		;; final R10
	vsubpd	ymm3, ymm3, ymm0		;; final R12
	vmovapd	[srcreg+4*d2+d1], ymm4		;; Save R10
	vmovapd	[srcreg+d1+32], ymm3		;; Save R12

	vmovapd	ymm0, YMM_TMPS[0*32]		;; Load real-cols row #2
	vaddpd	ymm4, ymm0, ymm6		;; final R2
	vsubpd	ymm0, ymm0, ymm6		;; final R20
	vmovapd	[srcreg+d1], ymm4		;; Save R2
	vmovapd	[srcreg+4*d2+d1+32], ymm0	;; Save R20

	vmovapd	ymm0, YMM_TMPS[6*32]		;; Load real-cols row #8
	vaddpd	ymm4, ymm0, ymm5		;; final R8
	vsubpd	ymm0, ymm0, ymm5		;; final R14
	vmovapd	[srcreg+3*d2+d1], ymm4		;; Save R8
	vmovapd	[srcreg+d2+d1+32], ymm0		;; Save R14

	vmovapd	ymm0, YMM_TMPS[4*32]		;; Load real-cols row #4
	vaddpd	ymm4, ymm0, ymm7		;; final R4
	vsubpd	ymm0, ymm0, ymm7		;; final R18
	vmovapd	[srcreg+d2+d1], ymm4		;; Save R4
	vmovapd	[srcreg+3*d2+d1+32], ymm0	;; Save R18

	;; Calculate even columns derived from imaginary inputs (odd rows)

	vmovapd	ymm7, YMM_TMPS[1*32]		;; i2-i10
	vmulpd	ymm6, ymm1, ymm7		;; .588(i2-i10)
	vmulpd	ymm7, ymm2, ymm7		;; .951(i2-i10)

	vmovapd	ymm4, YMM_TMPS[5*32]		;; i4-i8
	vmulpd	ymm3, ymm2, ymm4		;; .951(i4-i8)
	vaddpd	ymm6, ymm6, ymm3		;; .588(i2-i10)+.951(i4-i8)
	vmulpd	ymm4, ymm1, ymm4		;; .588(i4-i8)
	vsubpd	ymm7, ymm7, ymm4		;; .951(i2-i10)-.588(i4-i8)

	;; Calculate odd columns derived from imaginary inputs (odd rows)
	;; From above, even-imag-cols row #3,5 are in ymm6,ymm7

	vmovapd	ymm4, YMM_TMPS[3*32]		;; i3-i9
	vmulpd	ymm3, ymm2, ymm4		;; .951(i3-i9)
	vmulpd	ymm4, ymm1, ymm4		;; .588(i3-i9)

	vmovapd	ymm0, YMM_TMPS[7*32]		;; i5-i7
	vmulpd	ymm1, ymm1, ymm0		;; .588(i5-i7)
	vaddpd	ymm3, ymm3, ymm1		;; .951(i3-i9)+.588(i5-i7)
	vmulpd	ymm0, ymm2, ymm0		;; .951(i5-i7)
	vsubpd	ymm4, ymm4, ymm0		;; .588(i3-i9)-.951(i5-i7)

	;; Combine even and odd columns, then real and imag data (odd rows)

	L1prefetchw srcreg+4*d2+d1+L1pd, L1pt

	vaddpd	ymm0, ymm6, ymm3		;; imag-cols row #3 (even#3 + odd#3)
	vsubpd	ymm6, ymm6, ymm3		;; imag-cols row #9 (even#3 - odd#3)
	vaddpd	ymm1, ymm7, ymm4		;; imag-cols row #5 (even#5 + odd#5)
	vsubpd	ymm7, ymm7, ymm4		;; imag-cols row #7 (even#5 - odd#5)

	vmovapd	ymm2, YMM_TMPS[8*32]		;; Load real-cols row #3
	vaddpd	ymm3, ymm2, ymm0		;; final R3
	vsubpd	ymm2, ymm2, ymm0		;; final R19
	vmovapd	ymm0, YMM_TMPS[10*32]		;; Load real-cols row #9
	vaddpd	ymm4, ymm0, ymm6		;; final R9
	vsubpd	ymm0, ymm0, ymm6		;; final R13
	vmovapd	[srcreg+d2], ymm3		;; Save R3
	vmovapd	[srcreg+4*d2+32], ymm2		;; Save R19
	vmovapd	[srcreg+4*d2], ymm4		;; Save R9
	vmovapd	[srcreg+d2+32], ymm0		;; Save R13

	vmovapd	ymm5, YMM_TMPS[12*32]		;; Load real-cols row #5
	vaddpd	ymm3, ymm5, ymm1		;; final R5
	vsubpd	ymm5, ymm5, ymm1		;; final R17
	vmovapd	ymm6, YMM_TMPS[14*32]		;; Load real-cols row #7
	vaddpd	ymm1, ymm6, ymm7		;; final R7
	vsubpd	ymm6, ymm6, ymm7		;; final R15
	vmovapd	[srcreg+2*d2], ymm3		;; Save R5
	vmovapd	[srcreg+3*d2+32], ymm5		;; Save R17
	vmovapd	[srcreg+3*d2], ymm1		;; Save R7
	vmovapd	[srcreg+2*d2+32], ymm6		;; Save R15

	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; 64-bit version

IFDEF X86_64

yr5_10cl_20_reals_unfft_preload MACRO
	vmovapd	ymm12, YMM_P309
	vmovapd	ymm13, YMM_P809
	vmovapd	ymm14, YMM_P588
	vmovapd	ymm15, YMM_P951
	ENDM

yr5_10cl_20_reals_unfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd

	vmovapd	ymm0, [screg+32]		;; cosine/sine
	vmovapd	ymm1, [srcreg+d1]		;; R2
	vmulpd	ymm2, ymm1, ymm0		;; A2 = R2 * cosine/sine				;	1-5

	vmovapd	ymm4, [screg+8*64+32]		;; cosine/sine
	vmovapd	ymm5, [srcreg+4*d2+d1]		;; R10
	vmulpd	ymm6, ymm5, ymm4		;; A10 = R10 * cosine/sine				;	2-6

	vmovapd	ymm3, [srcreg+d1+32]		;; I2
	vmulpd	ymm0, ymm3, ymm0		;; B2 = I2 * cosine/sine				;	3-7

	vmovapd	ymm7, [srcreg+4*d2+d1+32]	;; I10
	vmulpd	ymm4, ymm7, ymm4		;; B10 = I10 * cosine/sine				;	4-8

	vmovapd	ymm8, [screg+64+32]		;; cosine/sine
	vmovapd	ymm9, [srcreg+d2]		;; R3
	vmulpd	ymm10, ymm9, ymm8		;; A3 = R3 * cosine/sine				;	5-9
	vmovapd	ymm11, [screg+7*64+32]		;; cosine/sine

	vaddpd	ymm2, ymm2, ymm3		;; A2 = A2 + I2						; 6-8
	vmovapd	ymm3, [srcreg+4*d2]		;; R9
	vaddpd	ymm6, ymm6, ymm7		;; A10 = A10 + I10					; 7-9
	vmulpd	ymm7, ymm3, ymm11		;; A9 = R9 * cosine/sine				;	6-10

	vsubpd	ymm0, ymm0, ymm1		;; B2 = B2 - R2						; 8-10
	vmovapd	ymm1, [srcreg+d2+32]		;; I3
	vmulpd	ymm8, ymm1, ymm8		;; B3 = I3 * cosine/sine				;	7-11

	vsubpd	ymm4, ymm4, ymm5		;; B10 = B10 - R10					; 9-11
	vmovapd	ymm5, [srcreg+4*d2+32]		;; I9
	vmulpd	ymm11, ymm5, ymm11		;; B9 = I9 * cosine/sine				;	8-12

	vaddpd	ymm10, ymm10, ymm1		;; A3 = A3 + I3						; 10-12
	vmovapd	ymm1, [screg]			;; sine
	vmulpd	ymm2, ymm2, ymm1		;; R2 = A2 * sine					;	9-13

	vaddpd	ymm7, ymm7, ymm5		;; A9 = A9 + I9						; 11-13
	vmovapd	ymm5, [screg+8*64]		;; sine
	vmulpd	ymm6, ymm6, ymm5		;; R10 = A10 * sine					;	10-14

	vmulpd	ymm0, ymm0, ymm1		;; I2 = B2 * sine					;	11-15
	vmovapd	ymm1, [screg+64]		;; sine

	vsubpd	ymm8, ymm8, ymm9		;; B3 = B3 - R3						; 12-14
	vmulpd	ymm4, ymm4, ymm5		;; I10 = B10 * sine					;	12-16
	vmovapd	ymm9, [screg+7*64]		;; sine

	vsubpd	ymm11, ymm11, ymm3		;; B9 = B9 - R9						; 13-15
	vmulpd	ymm10, ymm10, ymm1		;; R3 = A3 * sine					;	13-17
	vmovapd	ymm5, [screg+2*64+32]		;; cosine/sine

	vmulpd	ymm7, ymm7, ymm9		;; R9 = A9 * sine					;	14-18

	vaddpd	ymm3, ymm2, ymm6		;; R2+R10						; 15-17
	vmulpd	ymm8, ymm8, ymm1		;; I3 = B3 * sine					;	15-19
	vmovapd	ymm1, [srcreg+d2+d1]		;; R4

	vsubpd	ymm2, ymm2, ymm6		;; R2-R10						; 16-18
	vmulpd	ymm11, ymm11, ymm9		;; I9 = B9 * sine					;	16-20
	vmovapd	ymm6, [screg+6*64+32]		;; cosine/sine

	vaddpd	ymm9, ymm0, ymm4		;; I2+I10						; 17-19

	vsubpd	ymm0, ymm0, ymm4		;; I2-I10						; 18-20
	vmulpd	ymm4, ymm1, ymm5		;; A4 = R4 * cosine/sine				;	18-22
	vmovapd	YMM_TMPS[8*32], ymm3		;; Save R2+R10						; 18
	vmovapd	ymm3, [srcreg+3*d2+d1]		;; R8

	vmovapd	YMM_TMPS[0*32], ymm2		;; Save R2-R10						; 19
	vaddpd	ymm2, ymm10, ymm7		;; R3+R9						; 19-21
	vmovapd	YMM_TMPS[9*32], ymm9		;; Save I2+I10						; 20
	vmulpd	ymm9, ymm3, ymm6		;; A8 = R8 * cosine/sine				;	19-23

	vsubpd	ymm10, ymm10, ymm7		;; R3-R9						; 20-22
	vmovapd	ymm7, [srcreg+d2+d1+32]		;; I4
	vmulpd	ymm5, ymm7, ymm5		;; B4 = I4 * cosine/sine				;	20-24

	vmovapd	YMM_TMPS[1*32], ymm0		;; Save I2-I10						; 21
	vaddpd	ymm0, ymm8, ymm11		;; I3+I9						; 21-23
	vmovapd	YMM_TMPS[4*32], ymm2		;; Save R3+R9						; 22
	vmovapd	ymm2, [srcreg+3*d2+d1+32]	;; I8
	vmulpd	ymm6, ymm2, ymm6		;; B8 = I8 * cosine/sine				;	21-25

	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	ymm8, ymm8, ymm11		;; I3-I9						; 22-24
	vmovapd	ymm11, [screg+3*64+32]		;; cosine/sine
	vmovapd	YMM_TMPS[2*32], ymm10		;; Save R3-R9						; 23
	vmovapd	ymm10, [srcreg+2*d2]		;; R5
	vmovapd	YMM_TMPS[6*32], ymm0		;; Save I3+I9						; 24
	vmulpd	ymm0, ymm10, ymm11		;; A5 = R5 * cosine/sine				;	22-26

	vaddpd	ymm4, ymm4, ymm7		;; A4 = A4 + I4						; 23-25
	vmovapd	ymm7, [screg+5*64+32]		;; cosine/sine
	vaddpd	ymm9, ymm9, ymm2		;; A8 = A8 + I8						; 24-26
	vmovapd	ymm2, [srcreg+3*d2]		;; R7
	vmovapd	YMM_TMPS[3*32], ymm8		;; Save I3-I9						; 25
	vmulpd	ymm8, ymm2, ymm7		;; A7 = R7 * cosine/sine				;	23-27

	vsubpd	ymm5, ymm5, ymm1		;; B4 = B4 - R4						; 25-27
	vmovapd	ymm1, [srcreg+2*d2+32]		;; I5
	vmulpd	ymm11, ymm1, ymm11		;; B5 = I5 * cosine/sine				;	24-28

	vsubpd	ymm6, ymm6, ymm3		;; B8 = B8 - R8						; 26-28
	vmovapd	ymm3, [srcreg+3*d2+32]		;; I7
	vmulpd	ymm7, ymm3, ymm7		;; B7 = I7 * cosine/sine				;	25-29

	vaddpd	ymm0, ymm0, ymm1		;; A5 = A5 + I5						; 27-29
	vmovapd	ymm1, [screg+2*64]		;; sine
	vmulpd	ymm4, ymm4, ymm1		;; R4 = A4 * sine					;	26-30

	vaddpd	ymm8, ymm8, ymm3		;; A7 = A7 + I7						; 28-30
	vmovapd	ymm3, [screg+6*64]		;; sine
	vmulpd	ymm9, ymm9, ymm3		;; R8 = A8 * sine					;	27-31

	vmulpd	ymm5, ymm5, ymm1		;; I4 = B4 * sine					;	28-32
	vmovapd	ymm1, [screg+3*64]		;; sine

	vsubpd	ymm11, ymm11, ymm10		;; B5 = B5 - R5						; 29-31
	vmulpd	ymm6, ymm6, ymm3		;; I8 = B8 * sine					;	29-33
	vmovapd	ymm10, [screg+5*64]		;; sine

	vsubpd	ymm7, ymm7, ymm2		;; B7 = B7 - R7						; 30-32
	vmulpd	ymm0, ymm0, ymm1		;; R5 = A5 * sine					;	30-34
	vmovapd	ymm3, [screg+4*64+32]		;; cosine/sine

	vmulpd	ymm8, ymm8, ymm10		;; R7 = A7 * sine					;	31-35

	vaddpd	ymm2, ymm4, ymm9		;; R4+R8						; 32-34
	vmulpd	ymm11, ymm11, ymm1		;; I5 = B5 * sine					;	32-36
	vmovapd	ymm1, [srcreg+2*d2+d1]		;; R6

	vsubpd	ymm4, ymm4, ymm9		;; R4-R8						; 33-35
	vmulpd	ymm7, ymm7, ymm10		;; I7 = B7 * sine					;	33-37

	L1prefetchw srcreg+d1+L1pd, L1pt

	vaddpd	ymm10, ymm5, ymm6		;; I4+I8						; 34-36
	vmulpd	ymm9, ymm1, ymm3		;; A6 = R6 * cosine/sine				;	34-38

	vsubpd	ymm5, ymm5, ymm6		;; I4-I8						; 35-37
	vmovapd	ymm6, [srcreg+2*d2+d1+32]	;; I6
	vmulpd	ymm3, ymm6, ymm3		;; B6 = I6 * cosine/sine				;	35-39
	vmovapd	YMM_TMPS[12*32], ymm2		;; Save R4+R8						; 35

	vaddpd	ymm2, ymm0, ymm8		;; R5+R7						; 36-38
	vmovapd	YMM_TMPS[13*32], ymm10		;; Save I4+I8						; 37
	vmovapd	ymm10, YMM_TMPS[0*32]		;; r2-r10
	vmovapd	YMM_TMPS[5*32], ymm5		;; Save I4-I8						; 38
	vmulpd	ymm5, ymm15, ymm10		;; .951(r2-r10)						;	36-40

	vsubpd	ymm0, ymm0, ymm8		;; R5-R7						; 37-39
	vmulpd	ymm10, ymm14, ymm10		;; .588(r2-r10)						;	37-41

	vaddpd	ymm8, ymm11, ymm7		;; I5+I7						; 38-40
	vmovapd	YMM_TMPS[10*32], ymm2		;; Save R5+R7						; 39
	vmulpd	ymm2, ymm14, ymm4		;; .588(r4-r8)						;	38-42

	L1prefetchw srcreg+d2+L1pd, L1pt

	vsubpd	ymm11, ymm11, ymm7		;; I5-I7						; 39-41
	vmulpd	ymm4, ymm15, ymm4		;; .951(r4-r8)						;	39-43
	vmovapd	ymm7, YMM_TMPS[2*32]		;; r3-r9

	vaddpd	ymm9, ymm9, ymm6		;; A6 = A6 + I6						; 40-42
	vmulpd	ymm6, ymm13, ymm7		;; .809(r3-r9)						;	40-44

	vsubpd	ymm3, ymm3, ymm1		;; B6 = B6 - R6						; 41-43
	vmulpd	ymm1, ymm12, ymm7		;; .309(r3-r9)						;	41-45
	vmovapd	YMM_TMPS[11*32], ymm8		;; Save I5+I7						; 41
	vmovapd	ymm8, [srcreg+32]		;; r1-r11

	vsubpd	ymm7, ymm8, ymm7		;; r1-(r3-r9)-r11					; 42-44
	vmovapd	YMM_TMPS[7*32], ymm11		;; Save I5-I7						; 42
	vmulpd	ymm11, ymm12, ymm0		;; .309(r5-r7)						;	42-46

	vaddpd	ymm5, ymm5, ymm2		;; .951(r2-r10)+.588(r4-r8) (even#2)			; 43-45
	vmulpd	ymm2, ymm13, ymm0		;; .809(r5-r7)						;	43-47

	vsubpd	ymm10, ymm10, ymm4		;; .588(r2-r10)-.951(r4-r8) (even#4)			; 44-46
	vmovapd	ymm4, YMM_TMPS[9*32]		;; i2+i10

	vaddpd	ymm6, ymm8, ymm6		;; r1+.809(r3-r9)-r11					; 45-47

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm8, ymm8, ymm1		;; r1-.309(r3-r9)-r11					; 46-48
	vmulpd	ymm1, ymm12, ymm4		;; .309(i2+i10)						;	44-48

	vaddpd	ymm7, ymm7, ymm0		;; r1-(r3-r9)+(r5-r7)-r11				; 47-49
	vmulpd	ymm0, ymm13, ymm4		;; .809(i2+i10)						;	45-49

	vaddpd	ymm6, ymm6, ymm11		;; r1+.809(r3-r9)+.309(r5-r7)-r11 (odd#2)		; 48-50
	vmovapd	ymm11, YMM_TMPS[13*32]		;; i4+i8

	vsubpd	ymm8, ymm8, ymm2		;; r1-.309(r3-r9)-.809(r5-r7)-r11 (odd#4)		; 49-51
	vmulpd	ymm2, ymm13, ymm11		;; .809(i4+i8)						;	46-50

	L1prefetchw srcreg+2*d2+L1pd, L1pt

	vsubpd	ymm4, ymm4, ymm11		;; (i2+i10)-(i4+i8)					; 50-52
	vmulpd	ymm11, ymm12, ymm11		;; .309(i4+i8)						;	47-51

	vaddpd	ymm1, ymm1, ymm2		;; .309(i2+i10)+.809(i4+i8)				; 51-53
	vmovapd	ymm2, [screg+4*64]		;; sine
	vmulpd	ymm3, ymm3, ymm2		;; I6 = B6 * sine					;	48-52
	vmulpd	ymm9, ymm9, ymm2		;; R6 = A6 * sine					;	49-53
	vmovapd	ymm2, YMM_TMPS[6*32]		;; i3+i9

	vaddpd	ymm0, ymm0, ymm11		;; .809(i2+i10)+.309(i4+i8)				; 52-54
	vmulpd	ymm11, ymm14, ymm2		;; .588(i3+i9)						;	50-54
	vmulpd	ymm2, ymm15, ymm2		;; .951(i3+i9)						;	51-55

	vaddpd	ymm4, ymm4, ymm3		;; (i2+i10)-(i4+i8)+i6					; 53-55

	vaddpd	ymm1, ymm1, ymm3		;; .309(i2+i10)+.809(i4+i8)+i6 (even#2)			; 54-56

	vsubpd	ymm0, ymm0, ymm3		;; .809(i2+i10)+.309(i4+i8)-i6 (even#4)			; 55-57

	vaddpd	ymm3, ymm7, ymm4		;; final R6						; 56-58

	vsubpd	ymm7, ymm7, ymm4		;; final R16						; 57-59
	vmovapd	ymm4, YMM_TMPS[11*32]		;; i5+i7
	vmovapd	[srcreg+2*d2+d1], ymm3		;; Save R6						; 59
	vmulpd	ymm3, ymm15, ymm4		;; .951(i5+i7)						;	52-56
	vmulpd	ymm4, ymm14, ymm4		;; .588(i5+i7)						;	53-57

	L1prefetchw srcreg+2*d2+d1+L1pd, L1pt

	vaddpd	ymm11, ymm11, ymm3		;; .588(i3+i9)+.951(i5+i7) (odd#2)			; 58-60
	vmovapd	ymm3, YMM_TMPS[8*32]		;; r2+r10

	vsubpd	ymm2, ymm2, ymm4		;; .951(i3+i9)-.588(i5+i7) (odd#4)			; 59-61

	vaddpd	ymm4, ymm6, ymm5		;; real-cols row #2 (odd#2 + even#2)			; 60-62
	vsubpd	ymm6, ymm6, ymm5		;; real-cols row #10 (odd#2 - even#2)			; 61-63

	L1prefetchw srcreg+3*d2+L1pd, L1pt

	vaddpd	ymm5, ymm8, ymm10		;; real-cols row #4 (odd#4 + even#4)			; 62-64
	vmovapd	[srcreg+2*d2+d1+32], ymm7	;; Save R16						; 62
	vsubpd	ymm8, ymm8, ymm10		;; real-cols row #8 (odd#4 - even#4)			; 63-65

	vsubpd	ymm10, ymm1, ymm11		;; imag-cols row #10 (even#2 - odd#2)			; 64-66
	vaddpd	ymm1, ymm1, ymm11		;; imag-cols row #2 (even#2 + odd#2)			; 65-67
	vsubpd	ymm11, ymm0, ymm2		;; imag-cols row #8 (even#4 - odd#4)			; 66-68
	vaddpd	ymm0, ymm0, ymm2		;; imag-cols row #4 (even#4 + odd#4)			; 67-69

	L1prefetchw srcreg+3*d2+d1+L1pd, L1pt

	vaddpd	ymm2, ymm6, ymm10		;; final R10						; 68-70
	vsubpd	ymm6, ymm6, ymm10		;; final R12						; 69-71

	vaddpd	ymm10, ymm4, ymm1		;; final R2						; 70-72
	vmulpd	ymm7, ymm13, ymm3		;; .809(r2+r10)						;	70-74

	vsubpd	ymm4, ymm4, ymm1		;; final R20						; 71-73
	vmulpd	ymm1, ymm12, ymm3		;; .309(r2+r10)						;	71-75
	vmovapd	[srcreg+4*d2+d1], ymm2		;; Save R10						; 71

	L1prefetchw srcreg+4*d2+L1pd, L1pt

	vaddpd	ymm2, ymm8, ymm11		;; final R8						; 72-74
	vmovapd	[srcreg+d1+32], ymm6		;; Save R12						; 72
	vmovapd	ymm6, YMM_TMPS[12*32]		;; r4+r8
	vmovapd	[srcreg+d1], ymm10		;; Save R2						; 73
	vmulpd	ymm10, ymm12, ymm6		;; .309(r4+r8)						;	72-76

	vsubpd	ymm8, ymm8, ymm11		;; final R14						; 73-75
	vmulpd	ymm11, ymm13, ymm6		;; .809(r4+r8)						;	73-77

	vmovapd	[srcreg+4*d2+d1+32], ymm4	;; Save R20						; 74
	vaddpd	ymm4, ymm5, ymm0		;; final R4						; 74-76
	vmovapd	[srcreg+3*d2+d1], ymm2		;; Save R8						; 75
	vmovapd	ymm2, YMM_TMPS[4*32]		;; r3+r9
	vmovapd	[srcreg+d2+d1+32], ymm8		;; Save R14						; 76
	vmulpd	ymm8, ymm12, ymm2		;; .309(r3+r9)						;	74-78

	vsubpd	ymm5, ymm5, ymm0		;; final R18						; 75-77
	vmulpd	ymm0, ymm13, ymm2		;; .809(r3+r9)						;	75-79

	vaddpd	ymm3, ymm3, ymm6		;; (r2+r10)+(r4+r8)					; 76-78
	vmovapd	ymm6, YMM_TMPS[10*32]		;; r5+r7
	vmovapd	[srcreg+d2+d1], ymm4		;; Save R4						; 77
	vmulpd	ymm4, ymm13, ymm6		;; .809(r5+r7)						;	76-80

	vsubpd	ymm7, ymm7, ymm10		;; .809(r2+r10)-.309(r4+r8)				; 77-79
	vmulpd	ymm10, ymm12, ymm6		;; .309(r5+r7)						;	77-81

	vsubpd	ymm1, ymm1, ymm11		;; .309(r2+r10)-.809(r4+r8)				; 78-80
	vmovapd	ymm11, [srcreg]			;; r1+r11
	vmovapd	[srcreg+3*d2+d1+32], ymm5	;; Save R18						; 78
	vmovapd	ymm5, YMM_TMPS[1*32]		;; i2-i10

	vaddpd	ymm3, ymm3, ymm9		;; (r2+r10)+(r4+r8)+r6					; 79-81
	vsubpd	ymm7, ymm7, ymm9		;; .809(r2+r10)-.309(r4+r8)-r6				; 80-82
	vaddpd	ymm1, ymm1, ymm9		;; .309(r2+r10)-.809(r4+r8)+r6				; 81-83

	vaddpd	ymm2, ymm11, ymm2		;; r1+(r3+r9)+r11					; 82-84

	vaddpd	ymm8, ymm11, ymm8		;; r1+.309(r3+r9)+r11					; 83-85
	vmulpd	ymm9, ymm14, ymm5		;; .588(i2-i10)						;	83-87

	vsubpd	ymm11, ymm11, ymm0		;; r1-.809(r3+r9)+r11					; 84-86
	vmulpd	ymm5, ymm15, ymm5		;; .951(i2-i10)						;	84-88
	vmovapd	ymm0, YMM_TMPS[5*32]		;; i4-i8

	vaddpd	ymm2, ymm2, ymm6		;; r1+(r3+r9)+(r5+r7)+r11				; 85-87
	vmulpd	ymm6, ymm15, ymm0		;; .951(i4-i8)						;	85-89

	vsubpd	ymm8, ymm8, ymm4		;; r1+.309(r3+r9)-.809(r5+r7)+r11			; 86-88
	vmulpd	ymm0, ymm14, ymm0		;; .588(i4-i8)						;	86-90
	vmovapd	ymm4, YMM_TMPS[3*32]		;; i3-i9

	vaddpd	ymm11, ymm11, ymm10		;; r1-.809(r3+r9)+.309(r5+r7)+r11			; 87-89

	vaddpd	ymm10, ymm2, ymm3		;; real-cols row #1 (and final R1)			; 88-90
	vmovapd	[srcreg], ymm10			;; Save final R1					; 91
	vmulpd	ymm10, ymm15, ymm4		;; .951(i3-i9)						;	87-91
	vmulpd	ymm4, ymm14, ymm4		;; .588(i3-i9)						;	88-92

	vsubpd	ymm2, ymm2, ymm3		;; real-cols row #11 (and final R11)			; 89-91
	vmovapd	ymm3, YMM_TMPS[7*32]		;; i5-i7
	vmovapd	[srcreg+32], ymm2		;; Save final R11					; 92
	vmulpd	ymm2, ymm14, ymm3		;; .588(i5-i7)						;	89-93

	vaddpd	ymm9, ymm9, ymm6		;; .588(i2-i10)+.951(i4-i8)				; 90-92
	vmulpd	ymm3, ymm15, ymm3		;; .951(i5-i7)						;	90-94

	vsubpd	ymm5, ymm5, ymm0		;; .951(i2-i10)-.588(i4-i8)				; 91-93

	vaddpd	ymm0, ymm8, ymm7		;; real-cols row #3					; 92-94
	vsubpd	ymm8, ymm8, ymm7		;; real-cols row #9					; 93-95

	vaddpd	ymm10, ymm10, ymm2		;; .951(i3-i9)+.588(i5-i7)				; 94-96
	vsubpd	ymm4, ymm4, ymm3		;; .588(i3-i9)-.951(i5-i7)				; 95-97

	L1prefetchw srcreg+4*d2+d1+L1pd, L1pt

	vaddpd	ymm3, ymm11, ymm1		;; real-cols row #5					; 96-98
	vsubpd	ymm11, ymm11, ymm1 		;; real-cols row #7					; 97-99

	vaddpd	ymm1, ymm9, ymm10		;; imag-cols row #3 (even#3 + odd#3)			; 98-100
	vsubpd	ymm9, ymm9, ymm10		;; imag-cols row #9 (even#3 - odd#3)			; 99-101
	vaddpd	ymm10, ymm5, ymm4		;; imag-cols row #5 (even#5 + odd#5)			; 100-102
	vsubpd	ymm5, ymm5, ymm4		;; imag-cols row #7 (even#5 - odd#5)			; 101-103

	vaddpd	ymm4, ymm0, ymm1		;; final R3						; 102-104
	vsubpd	ymm0, ymm0, ymm1		;; final R19						; 103-105
	vaddpd	ymm1, ymm8, ymm9		;; final R9						; 104-106
	vsubpd	ymm8, ymm8, ymm9		;; final R13						; 105-107
	vmovapd	[srcreg+d2], ymm4		;; Save R3						; 105

	vaddpd	ymm2, ymm3, ymm10		;; final R5						; 106-108
	vmovapd	[srcreg+4*d2+32], ymm0		;; Save R19						; 106

	vsubpd	ymm3, ymm3, ymm10		;; final R17						; 107-109
	vmovapd	[srcreg+4*d2], ymm1		;; Save R9						; 107

	vaddpd	ymm7, ymm11, ymm5		;; final R7						; 108-110
	vmovapd	[srcreg+d2+32], ymm8		;; Save R13						; 108

	vsubpd	ymm11, ymm11, ymm5		;; final R15						; 109-111
	vmovapd	[srcreg+2*d2], ymm2		;; Save R5						; 109
	vmovapd	[srcreg+3*d2+32], ymm3		;; Save R17						; 110
	vmovapd	[srcreg+3*d2], ymm7		;; Save R7						; 111
	vmovapd	[srcreg+2*d2+32], ymm11		;; Save R15						; 112

	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

ENDIF


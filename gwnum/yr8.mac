; Copyright 2011-2012 - Mersenne Research, Inc.  All rights reserved
; Author:  George Woltman
; Email: woltman@alum.mit.edu
;

;;
;; Macros for a radix-8 step in an FFT.  This is used in a radix-4 FFT
;; with an odd number of levels.
;;


;;
;; ************************************* eight-complex-fft8 variants ******************************************
;;
;; In the all-complex split premultiplier case, we apply part of the roots of -1 at the
;; end of the first pass.  Also, in the r4delay case we apply part of the first level
;; twiddles at the end of the first pass.  Thus we have 8 sin/cos multiplies instead
;; of the usual 7.
;;

yr8_sg8cl_eight_complex_fft8 MACRO srcreg,srcinc,d1,d2,d4,dstreg,dstinc,e1,e2,e4,screg,scinc,maxrpt,L1pt,L1pd

	;; On input the 64-byte cache lines hold these data values:
	;;	0	+1K	+1K	+1K	8K	+1K	+1K	+1K
	;;	1	...
	;;	2	...
	;;	3	...

	;; These are swizzled to:
	;;	0	+1	+1	+1	8K	+1	+1	+1
	;;	1K	...
	;;	2K	...
	;;	3K	...

	;; Swizzle using 8 shufpds and 8 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm1, [srcreg]			;; R1
	vmovapd	ymm7, [srcreg+d1]		;; R2
	vshufpd	ymm0, ymm1, ymm7, 15		;; Shuffle R1 and R2 to create R1/R2 hi
	vshufpd	ymm1, ymm1, ymm7, 0		;; Shuffle R1 and R2 to create R1/R2 low

	vmovapd	ymm2, [srcreg+d2]		;; R3
	vmovapd	ymm7, [srcreg+d2+d1]		;; R4
	vshufpd	ymm3, ymm2, ymm7, 15		;; Shuffle R3 and R4 to create R3/R4 hi
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle R3 and R4 to create R3/R4 low

	vperm2f128 ymm4, ymm0, ymm3, 32		;; Shuffle R1/R2 hi and R3/R4 hi (first R2)
	vperm2f128 ymm0, ymm0, ymm3, 49		;; Shuffle R1/R2 hi and R3/R4 hi (first R4)

	vperm2f128 ymm3, ymm1, ymm2, 32		;; Shuffle R1/R2 low and R3/R4 low (first R1)
	vperm2f128 ymm1, ymm1, ymm2, 49		;; Shuffle R1/R2 low and R3/R4 low (first R3)

	vmovapd	ymm6, [srcreg+d4]		;; R5
	vmovapd	ymm2, [srcreg+d4+d1]		;; R6
	vshufpd	ymm5, ymm6, ymm2, 15		;; Shuffle R5 and R6 to create R5/R6 hi
	vshufpd	ymm6, ymm6, ymm2, 0		;; Shuffle R5 and R6 to create R5/R6 low

	vmovapd	ymm2, [srcreg+d4+d2]		;; R7
	vmovapd	ymm7, [srcreg+d4+d2+d1]		;; R8
	vmovapd	[dstreg+32], ymm0		;; Save first R4
	vshufpd	ymm0, ymm2, ymm7, 15		;; Shuffle R7 and R8 to create R7/R8 hi
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle R7 and R8 to create R7/R8 low

	vperm2f128 ymm7, ymm5, ymm0, 32		;; Shuffle R5/R6 hi and R7/R8 hi (first R6)
	vperm2f128 ymm5, ymm5, ymm0, 49		;; Shuffle R5/R6 hi and R7/R8 hi (first R8)

	vperm2f128 ymm0, ymm6, ymm2, 32		;; Shuffle R5/R6 low and R7/R8 low (first R5)
	vperm2f128 ymm6, ymm6, ymm2, 49		;; Shuffle R5/R6 low and R7/R8 low (first R7)

	L1prefetch srcreg+L1pd, L1pt

	vaddpd	ymm2, ymm3, ymm0		;; R1 + R5 (new R1)
	vsubpd	ymm3, ymm3, ymm0		;; R1 - R5 (new R5)

	vaddpd	ymm0, ymm1, ymm6		;; R3 + R7 (new R3)
	vsubpd	ymm1, ymm1, ymm6		;; R3 - R7 (new R7)

	L1prefetch srcreg+d1+L1pd, L1pt

	vaddpd	ymm6, ymm4, ymm7		;; R2 + R6 (new R2)
	vsubpd	ymm4, ymm4, ymm7		;; R2 - R6 (new R6)

	vmovapd	ymm7, [dstreg+32]		;; Reload first R4
	vmovapd	[dstreg+e4], ymm3		;; Save new R5

	vaddpd	ymm3, ymm7, ymm5		;; R4 + R8 (new R4)
	vsubpd	ymm7, ymm7, ymm5		;; R4 - R8 (new R8)

	L1prefetch srcreg+d2+L1pd, L1pt

	vaddpd	ymm5, ymm2, ymm0		;; R1 + R3 (newer R1)
	vsubpd	ymm2, ymm2, ymm0		;; R1 - R3 (newer R3)

	vaddpd	ymm0, ymm6, ymm3		;; R2 + R4 (newer R2)
	vsubpd	ymm6, ymm6, ymm3		;; R2 - R4 (newer R4)

	vmovapd	[dstreg+e4+e2], ymm1		;; Save new R7
	vmovapd	ymm1, [srcreg+32]		;; I1
	vmovapd	[dstreg+e4+e2+e1], ymm7		;; Save new R8
	vmovapd	ymm7, [srcreg+d1+32]		;; I2
	vmovapd	[dstreg+e1], ymm0		;; Save newer R2
	vshufpd	ymm0, ymm1, ymm7, 15		;; Shuffle I1 and I2 to create I1/I2 hi
	vshufpd	ymm1, ymm1, ymm7, 0		;; Shuffle I1 and I2 to create I1/I2 low

	vmovapd	ymm7, [srcreg+d2+d1+32]		;; I4
	vmovapd	[dstreg+e2], ymm2		;; Save newer R3
	vmovapd	ymm2, [srcreg+d2+32]		;; I3
	vshufpd	ymm3, ymm2, ymm7, 15		;; Shuffle I3 and I4 to create I3/I4 hi
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle I3 and I4 to create I3/I4 low

	vmovapd	[dstreg+e4+e1], ymm4		;; Save new R6
	vperm2f128 ymm4, ymm0, ymm3, 32		;; Shuffle I1/I2 hi and I3/I4 hi (first I2)
	vperm2f128 ymm0, ymm0, ymm3, 49		;; Shuffle I1/I2 hi and I3/I4 hi (first I4)

	vperm2f128 ymm3, ymm1, ymm2, 32		;; Shuffle I1/I2 low and I3/I4 low (first I1)
	vperm2f128 ymm1, ymm1, ymm2, 49		;; Shuffle I1/I2 low and I3/I4 low (first I3)

	vmovapd	ymm2, [srcreg+d4+d1+32]		;; I6
	vmovapd	[dstreg+e2+e1], ymm6		;; Save newer R4
	vmovapd	ymm6, [srcreg+d4+32]		;; I5
	vmovapd	[dstreg], ymm5			;; Save newer R1
	vshufpd	ymm5, ymm6, ymm2, 15		;; Shuffle I5 and I6 to create I5/I6 hi
	vshufpd	ymm6, ymm6, ymm2, 0		;; Shuffle I5 and I6 to create I5/I6 low

	vmovapd	ymm2, [srcreg+d4+d2+32]		;; I7
	vmovapd	ymm7, [srcreg+d4+d2+d1+32]	;; I8
	vmovapd	[dstreg+e2+e1+32], ymm0		;; Save first I4
	vshufpd	ymm0, ymm2, ymm7, 15		;; Shuffle I7 and I8 to create I7/I8 hi
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle I7 and I8 to create I7/I8 low

	vperm2f128 ymm7, ymm5, ymm0, 32		;; Shuffle I5/I6 hi and I7/I8 hi (first I6)
	vperm2f128 ymm5, ymm5, ymm0, 49		;; Shuffle I5/I6 hi and I7/I8 hi (first I8)

	vperm2f128 ymm0, ymm6, ymm2, 32		;; Shuffle I5/I6 low and I7/I8 low (first I5)
	vperm2f128 ymm6, ymm6, ymm2, 49		;; Shuffle I5/I6 low and I7/I8 low (first I7)

	L1prefetch srcreg+d2+d1+L1pd, L1pt

	vaddpd	ymm2, ymm3, ymm0		;; I1 + I5 (new I1)
	vsubpd	ymm3, ymm3, ymm0		;; I1 - I5 (new I5)

	vaddpd	ymm0, ymm1, ymm6		;; I3 + I7 (new I3)
	vsubpd	ymm1, ymm1, ymm6		;; I3 - I7 (new I7)

	L1prefetch srcreg+d4+L1pd, L1pt

	vaddpd	ymm6, ymm2, ymm0		;; I1 + I3 (newer I1)
	vsubpd	ymm2, ymm2, ymm0		;; I1 - I3 (newer I3)

	vmovapd	ymm0, [dstreg+e2+e1+32]		;; Reload first I4
	vmovapd	[dstreg+32], ymm6		;; Save newer I1

	vaddpd	ymm6, ymm4, ymm7		;; I2 + I6 (new I2)
	vsubpd	ymm4, ymm4, ymm7		;; I2 - I6 (new I6)

	L1prefetch srcreg+d4+d1+L1pd, L1pt

	vaddpd	ymm7, ymm0, ymm5		;; I4 + I8 (new I4)
	vsubpd	ymm0, ymm0, ymm5		;; I4 - I8 (new I8)

	vaddpd	ymm5, ymm6, ymm7		;; I2 + I4 (newer I2)
	vsubpd	ymm6, ymm6, ymm7		;; I2 - I4 (newer I4)

	vmovapd	ymm7, [dstreg+e4+e2]		;; Reload new R7
	vmovapd	[dstreg+e2+32], ymm2		;; Save newer I3

 	vaddpd	ymm2, ymm3, ymm7		;; I5 + R7 (newer I5)
	vsubpd	ymm3, ymm3, ymm7		;; I5 - R7 (newer I7)

	vmovapd	ymm7, [dstreg+e4]		;; Reload new R5
	vmovapd	[dstreg+e1+32], ymm5		;; Save newer I2

	vaddpd	ymm5, ymm7, ymm1		;; R5 + I7 (newer R7)
	vsubpd	ymm7, ymm7, ymm1		;; R5 - I7 (newer R5)

	vmovapd	ymm1, [dstreg+e4+e1]		;; Reload new R6
	vmovapd	[dstreg+e2+e1+32], ymm6		;; Save newer I4

	vaddpd	ymm6, ymm1, ymm0		;; R6 + I8 (new R8)
	vsubpd	ymm1, ymm1, ymm0		;; R6 - I8 (new R6)

	vmovapd	ymm0, [dstreg+e4+e2+e1]		;; Reload new R8
	vmovapd	[dstreg+e4+32], ymm2		;; Save newer I5

	vaddpd	ymm2, ymm4, ymm0		;; I6 + R8 (new I6)
	vsubpd	ymm4, ymm4, ymm0		;; I6 - R8 (new I8)

	L1prefetch srcreg+d4+d2+L1pd, L1pt

	vsubpd	ymm0, ymm1, ymm2		;; R6 = R6 - I6
	vaddpd	ymm1, ymm1, ymm2		;; I6 = R6 + I6
	vsubpd	ymm2, ymm6, ymm4		;; R8 = R8 - I8
	vaddpd	ymm6, ymm6, ymm4		;; I8 = R8 + I8

	vmovapd	ymm4, YMM_SQRTHALF
	vmulpd	ymm0, ymm0, ymm4		;; R6 = R6 * SQRTHALF (newer R6)
	vmulpd	ymm1, ymm1, ymm4		;; I6 = I6 * SQRTHALF (newer I6)
	vmulpd	ymm2, ymm2, ymm4		;; R8 = R8 * SQRTHALF (newer R8)
	vmulpd	ymm6, ymm6, ymm4		;; I8 = I8 * SQRTHALF (newer I8)

	vmovapd	[dstreg+e4], ymm7		;; Save newer R5

;; the last level

	vsubpd	ymm4, ymm5, ymm6		;; R7 - I8 (last R7)
	vaddpd	ymm5, ymm5, ymm6		;; R7 + I8 (last R8)

	L1prefetch srcreg+d4+d2+d1+L1pd, L1pt

	vsubpd	ymm6, ymm3, ymm2		;; I7 - R8 (last I8)
	vaddpd	ymm3, ymm3, ymm2		;; I7 + R8 (last I7)

	vmovapd	ymm2, [screg+192+32]		;; cosine/sine
	vmulpd	ymm7, ymm4, ymm2		;; A7 = R7 * cosine/sine
	vsubpd	ymm7, ymm7, ymm3		;; A7 = A7 - I7
	vmulpd	ymm3, ymm3, ymm2		;; B7 = I7 * cosine/sine
	vaddpd	ymm3, ymm3, ymm4		;; B7 = B7 + R7

	vmovapd	ymm2, [screg+448+32]		;; cosine/sine
	vmulpd	ymm4, ymm5, ymm2		;; A8 = R8 * cosine/sine
	vsubpd	ymm4, ymm4, ymm6		;; A8 = A8 - I8
	vmulpd	ymm6, ymm6, ymm2		;; B8 = I8 * cosine/sine
	vaddpd	ymm6, ymm6, ymm5		;; B8 = B8 + R8

	vmovapd	ymm2, [screg+192]		;; sine
	vmulpd	ymm7, ymm7, ymm2		;; A7 = A7 * sine (final R7)
	vmulpd	ymm3, ymm3, ymm2		;; B7 = B7 * sine (final I7)
	vmovapd	ymm2, [screg+448]		;; sine
	vmulpd	ymm4, ymm4, ymm2		;; A8 = A8 * sine (final R8)
	vmulpd	ymm6, ymm6, ymm2		;; B8 = B8 * sine (final I8)

	vmovapd	ymm2, [dstreg+e4]		;; Reload newer R5
	vsubpd	ymm5, ymm2, ymm0		;; R5 - R6 (last R6)
	vaddpd	ymm2, ymm2, ymm0		;; R5 + R6 (last R5)

	vmovapd	ymm0, [dstreg+e4+32]		;; Reload newer I5
	vmovapd	[dstreg+e4+e2], ymm7		;; Save R7

	vsubpd	ymm7, ymm0, ymm1		;; I5 - I6 (last I6)
	vaddpd	ymm0, ymm0, ymm1		;; I5 + I6 (last I5)

	vmovapd	ymm1, [screg+320+32]		;; cosine/sine
	vmovapd	[dstreg+e4+e2+32], ymm3		;; Save I7
	vmulpd	ymm3, ymm5, ymm1		;; A6 = R6 * cosine/sine
	vsubpd	ymm3, ymm3, ymm7		;; A6 = A6 - I6
	vmulpd	ymm7, ymm7, ymm1		;; B6 = I6 * cosine/sine
	vaddpd	ymm7, ymm7, ymm5		;; B6 = B6 + R6

	vmovapd	ymm1, [screg+64+32]		;; cosine/sine
	vmulpd	ymm5, ymm2, ymm1		;; A5 = R5 * cosine/sine
	vsubpd	ymm5, ymm5, ymm0		;; A5 = A5 - I5
	vmulpd	ymm0, ymm0, ymm1		;; B5 = I5 * cosine/sine
	vaddpd	ymm0, ymm0, ymm2		;; B5 = B5 + R5

	vmovapd	ymm1, [screg+320]		;; sine
	vmulpd	ymm3, ymm3, ymm1		;; A6 = A6 * sine (final R6)
	vmulpd	ymm7, ymm7, ymm1		;; B6 = B6 * sine (final I6)
	vmovapd	ymm1, [screg+64]		;; sine
	vmulpd	ymm5, ymm5, ymm1		;; A5 = A5 * sine (final R5)
	vmulpd	ymm0, ymm0, ymm1		;; B5 = B5 * sine (final I5)

	vmovapd	ymm2, [dstreg]			;; Reload newer R1
	vmovapd	ymm1, [dstreg+e1]		;; Reload newer R2
	vmovapd	[dstreg+e4+e2+e1], ymm4		;; Save R8

	vsubpd	ymm4, ymm2, ymm1		;; R1 - R2 (last R2)
	vaddpd	ymm2, ymm2, ymm1		;; R1 + R2 (last R1)

	vmovapd	ymm1, [dstreg+32]		;; Reload newer I1
	vmovapd	[dstreg+e4+e2+e1+32], ymm6	;; Save I8
	vmovapd	ymm6, [dstreg+e1+32]		;; Reload newer I2
	vmovapd	[dstreg+e4+e1], ymm3		;; Save R6

	vsubpd	ymm3, ymm1, ymm6		;; I1 - I2 (last I2)
	vaddpd	ymm1, ymm1, ymm6		;; I1 + I2 (last I1)

	vmovapd	ymm6, [screg+256+32]		;; cosine/sine
	vmovapd	[dstreg+e4+e1+32], ymm7		;; Save I6
	vmulpd	ymm7, ymm4, ymm6		;; A2 = R2 * cosine/sine
	vsubpd	ymm7, ymm7, ymm3		;; A2 = A2 - I2
	vmulpd	ymm3, ymm3, ymm6		;; B2 = I2 * cosine/sine
	vaddpd	ymm3, ymm3, ymm4		;; B2 = B2 + R2

	vmovapd	ymm6, [screg+0+32]		;; cosine/sine
	vmulpd	ymm4, ymm2, ymm6		;; A1 = R1 * cosine/sine
	vsubpd	ymm4, ymm4, ymm1		;; A1 = A1 - I1
	vmulpd	ymm1, ymm1, ymm6		;; B1 = I1 * cosine/sine
	vaddpd	ymm1, ymm1, ymm2		;; B1 = B1 + R1

	vmovapd	ymm6, [screg+256]		;; sine
	vmulpd	ymm7, ymm7, ymm6		;; A2 = A2 * sine (final R2)
	vmulpd	ymm3, ymm3, ymm6		;; B2 = B2 * sine (final I2)
	vmovapd	ymm6, [screg+0]			;; sine
	vmulpd	ymm4, ymm4, ymm6		;; A1 = A1 * sine (final R1)
	vmulpd	ymm1, ymm1, ymm6		;; B1 = B1 * sine (final I1)

	vmovapd	ymm2, [dstreg+e2]		;; Reload newer R3
	vmovapd	ymm6, [dstreg+e2+e1+32]		;; Reload newer I4
	vmovapd	[dstreg+e4], ymm5		;; Save R5

	vsubpd	ymm5, ymm2, ymm6		;; R3 - I4 (last R3)
	vaddpd	ymm2, ymm2, ymm6		;; R3 + I4 (last R4)

	vmovapd	ymm6, [dstreg+e2+32]		;; Reload newer I3
	vmovapd	[dstreg+e4+32], ymm0		;; Save I5
	vmovapd	ymm0, [dstreg+e2+e1]		;; Reload newer R4
	vmovapd	[dstreg+e1], ymm7		;; Save R2

	vsubpd	ymm7, ymm6, ymm0		;; I3 - R4 (last I4)
	vaddpd	ymm6, ymm6, ymm0		;; I3 + R4 (last I3)

	vmovapd	ymm0, [screg+128+32]		;; cosine/sine
	vmovapd	[dstreg+e1+32], ymm3		;; Save I2
	vmulpd	ymm3, ymm5, ymm0		;; A3 = R3 * cosine/sine
	vsubpd	ymm3, ymm3, ymm6		;; A3 = A3 - I3
	vmulpd	ymm6, ymm6, ymm0		;; B3 = I3 * cosine/sine
	vaddpd	ymm6, ymm6, ymm5		;; B3 = B3 + R3

	vmovapd	ymm0, [screg+384+32]		;; cosine/sine
	vmulpd	ymm5, ymm2, ymm0		;; A4 = R4 * cosine/sine
	vsubpd	ymm5, ymm5, ymm7		;; A4 = A4 - I4
	vmulpd	ymm7, ymm7, ymm0		;; B4 = I4 * cosine/sine
	vaddpd	ymm7, ymm7, ymm2		;; B4 = B4 + R4

	vmovapd	ymm0, [screg+128]
	vmulpd	ymm3, ymm3, ymm0		;; A3 = A3 * sine (final R3)
	vmulpd	ymm6, ymm6, ymm0		;; B3 = B3 * sine (final I3)
	vmovapd	ymm0, [screg+384]		;; sine
	vmulpd	ymm5, ymm5, ymm0		;; A4 = A4 * sine (final R4)
	vmulpd	ymm7, ymm7, ymm0		;; B4 = B4 * sine (final I4)

	vmovapd	[dstreg], ymm4			;; Save R1
	vmovapd	[dstreg+32], ymm1		;; Save I1
	vmovapd	[dstreg+e2], ymm3		;; Save R3
	vmovapd	[dstreg+e2+32], ymm6		;; Save I3
	vmovapd	[dstreg+e2+e1], ymm5		;; Save R4
	vmovapd	[dstreg+e2+e1+32], ymm7		;; Save I4

	bump	srcreg, srcinc
	bump	dstreg, dstinc
	bump	screg, scinc
	ENDM

;; 64-bit version

IFDEF X86_64

yr8_sg8cl_eight_complex_fft8 MACRO srcreg,srcinc,d1,d2,d4,dstreg,dstinc,e1,e2,e4,screg,scinc,maxrpt,L1pt,L1pd

	;; On input the 64-byte cache lines hold these data values:
	;;	0	+1K	+1K	+1K	8K	+1K	+1K	+1K
	;;	1	...
	;;	2	...
	;;	3	...

	;; These are swizzled to:
	;;	0	+1	+1	+1	8K	+1	+1	+1
	;;	1K	...
	;;	2K	...
	;;	3K	...

	;; Swizzle using 8 shufpds and 8 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm1, [srcreg]			;; R1
	vmovapd	ymm7, [srcreg+d1]		;; R2
	vshufpd	ymm0, ymm1, ymm7, 15		;; Shuffle R1 and R2 to create R1/R2 hi
	vshufpd	ymm1, ymm1, ymm7, 0		;; Shuffle R1 and R2 to create R1/R2 low

	vmovapd	ymm2, [srcreg+d2]		;; R3
	vmovapd	ymm7, [srcreg+d2+d1]		;; R4
	vshufpd	ymm3, ymm2, ymm7, 15		;; Shuffle R3 and R4 to create R3/R4 hi
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle R3 and R4 to create R3/R4 low

	vmovapd	ymm6, [srcreg+d4]		;; R5
	vmovapd	ymm7, [srcreg+d4+d1]		;; R6
	vshufpd	ymm5, ymm6, ymm7, 15		;; Shuffle R5 and R6 to create R5/R6 hi
	vshufpd	ymm6, ymm6, ymm7, 0		;; Shuffle R5 and R6 to create R5/R6 low

	vmovapd	ymm4, [srcreg+d4+d2]		;; R7
	vmovapd	ymm8, [srcreg+d4+d2+d1]		;; R8
	vshufpd	ymm7, ymm4, ymm8, 15		;; Shuffle R7 and R8 to create R7/R8 hi
	vshufpd	ymm4, ymm4, ymm8, 0		;; Shuffle R7 and R8 to create R7/R8 low

	vperm2f128 ymm8, ymm1, ymm2, 32		;; Shuffle R1/R2 low and R3/R4 low (first R1)
	vperm2f128 ymm1, ymm1, ymm2, 49		;; Shuffle R1/R2 low and R3/R4 low (first R3)
	vmovapd	ymm15, [srcreg+32]		;; I1

	vperm2f128 ymm2, ymm6, ymm4, 32		;; Shuffle R5/R6 low and R7/R8 low (first R5)
	vperm2f128 ymm6, ymm6, ymm4, 49		;; Shuffle R5/R6 low and R7/R8 low (first R7)
	vmovapd	ymm14, [srcreg+d1+32]		;; I2

	vperm2f128 ymm4, ymm0, ymm3, 32		;; Shuffle R1/R2 hi and R3/R4 hi (first R2)
	vperm2f128 ymm0, ymm0, ymm3, 49		;; Shuffle R1/R2 hi and R3/R4 hi (first R4)
	vmovapd	ymm12, [srcreg+d2+32]		;; I3

	vperm2f128 ymm3, ymm5, ymm7, 32		;; Shuffle R5/R6 hi and R7/R8 hi (first R6)
	vperm2f128 ymm5, ymm5, ymm7, 49		;; Shuffle R5/R6 hi and R7/R8 hi (first R8)
	vmovapd	ymm13, [srcreg+d2+d1+32]	;; I4

	vaddpd	ymm7, ymm8, ymm2		;; R1 + R5 (new R1)
	vsubpd	ymm8, ymm8, ymm2		;; R1 - R5 (new R5)
	vmovapd	ymm11, [srcreg+d4+32]		;; I5

	vaddpd	ymm2, ymm1, ymm6		;; R3 + R7 (new R3)
	vsubpd	ymm1, ymm1, ymm6		;; R3 - R7 (new R7)
	vmovapd	ymm10, [srcreg+d4+d1+32]	;; I6

	vaddpd	ymm6, ymm4, ymm3		;; R2 + R6 (new R2)
	vsubpd	ymm4, ymm4, ymm3		;; R2 - R6 (new R6)
	vmovapd	ymm9, [srcreg+d4+d2+32]		;; I7

	vaddpd	ymm3, ymm0, ymm5		;; R4 + R8 (new R4)
	vsubpd	ymm0, ymm0, ymm5		;; R4 - R8 (new R8)
	L1prefetch srcreg+L1pd, L1pt

	vaddpd	ymm5, ymm7, ymm2		;; R1 + R3 (newer R1)
	vsubpd	ymm7, ymm7, ymm2		;; R1 - R3 (newer R3)

	vaddpd	ymm2, ymm6, ymm3		;; R2 + R4 (newer R2)
	vsubpd	ymm6, ymm6, ymm3		;; R2 - R4 (newer R4)
	L1prefetch srcreg+d1+L1pd, L1pt

	vmovapd	[dstreg], ymm5			;; Temporarily save R1
	vmovapd	[dstreg+e1], ymm2		;; Temporarily save R2

	vshufpd	ymm2, ymm15, ymm14, 15		;; Shuffle I1 and I2 to create I1/I2 hi
	vshufpd	ymm15, ymm15, ymm14, 0		;; Shuffle I1 and I2 to create I1/I2 low
	vmovapd	ymm14, [srcreg+d4+d2+d1+32]	;; I8

	vshufpd	ymm3, ymm12, ymm13, 15		;; Shuffle I3 and I4 to create I3/I4 hi
	vshufpd	ymm12, ymm12, ymm13, 0		;; Shuffle I3 and I4 to create I3/I4 low
	L1prefetch srcreg+d2+L1pd, L1pt

	vshufpd	ymm5, ymm11, ymm10, 15		;; Shuffle I5 and I6 to create I5/I6 hi
	vshufpd	ymm11, ymm11, ymm10, 0		;; Shuffle I5 and I6 to create I5/I6 low

	vshufpd	ymm10, ymm9, ymm14, 15		;; Shuffle I7 and I8 to create I7/I8 hi
	vshufpd	ymm9, ymm9, ymm14, 0		;; Shuffle I7 and I8 to create I7/I8 low
	L1prefetch srcreg+d2+d1+L1pd, L1pt

	vperm2f128 ymm14, ymm2, ymm3, 32	;; Shuffle I1/I2 hi and I3/I4 hi (first I2)
	vperm2f128 ymm13, ymm5, ymm10, 32	;; Shuffle I5/I6 hi and I7/I8 hi (first I6)
	vperm2f128 ymm2, ymm2, ymm3, 49		;; Shuffle I1/I2 hi and I3/I4 hi (first I4)
	vperm2f128 ymm5, ymm5, ymm10, 49	;; Shuffle I5/I6 hi and I7/I8 hi (first I8)
	vmovapd	ymm3, YMM_SQRTHALF

	vsubpd	ymm10, ymm14, ymm13		;; I2 - I6 (new I6)		; 1-3 - starting clock counts here for no particularly good reason
	vaddpd	ymm14, ymm14, ymm13		;; I2 + I6 (new I2)		; 2-4

	vsubpd	ymm13, ymm2, ymm5		;; I4 - I8 (new I8)		; 3-5
	vaddpd	ymm2, ymm2, ymm5		;; I4 + I8 (new I4)		; 4-6

	vperm2f128 ymm5, ymm15, ymm12, 32	;; Shuffle I1/I2 low and I3/I4 low (first I1)
	vperm2f128 ymm15, ymm15, ymm12, 49	;; Shuffle I1/I2 low and I3/I4 low (first I3)

	vperm2f128 ymm12, ymm11, ymm9, 32	;; Shuffle I5/I6 low and I7/I8 low (first I5)
	vperm2f128 ymm11, ymm11, ymm9, 49	;; Shuffle I5/I6 low and I7/I8 low (first I7)

	vaddpd	ymm9, ymm10, ymm0		;; I6 + R8 (new I6)		; 5-7
	vsubpd	ymm10, ymm10, ymm0		;; I6 - R8 (new I8)		; 6-8
	L1prefetch srcreg+d4+d1+L1pd, L1pt

	vaddpd	ymm0, ymm4, ymm13		;; R6 + I8 (new R8)		; 7-9
	vsubpd	ymm4, ymm4, ymm13		;; R6 - I8 (new R6)		; 8-10
	vmulpd	ymm9, ymm9, ymm3		;; I6 = I6 * SQRTHALF		;  8-12

	vaddpd	ymm13, ymm15, ymm11		;; I3 + I7 (new I3)		; 9-11
	vmulpd	ymm10, ymm10, ymm3		;; I8 = I8 * SQRTHALF		;  9-13

	vsubpd	ymm15, ymm15, ymm11		;; I3 - I7 (new I7)		; 10-12
	vmulpd	ymm0, ymm0, ymm3		;; R8 = R8 * SQRTHALF		;  10-14

	vaddpd	ymm11, ymm5, ymm12		;; I1 + I5 (new I1)		; 11-13
	vmulpd	ymm4, ymm4, ymm3		;; R6 = R6 * SQRTHALF		;  11-15
	L1prefetch srcreg+d4+L1pd, L1pt

	vsubpd	ymm5, ymm5, ymm12		;; I1 - I5 (new I5)		; 12-14

	vsubpd	ymm12, ymm14, ymm2		;; I2 - I4 (newer I4)		; 13-15
	vaddpd	ymm14, ymm14, ymm2		;; I2 + I4 (newer I2)		; 14-16

	vsubpd	ymm2, ymm11, ymm13		;; I1 - I3 (newer I3)		; 15-17
	vaddpd	ymm11, ymm11, ymm13		;; I1 + I3 (newer I1)		; 16-18
	L1prefetch srcreg+d4+d2+L1pd, L1pt

	vsubpd	ymm13, ymm7, ymm12		;; R3 - I4 (last R3)		; 17-19
	vaddpd	ymm3, ymm2, ymm6		;; I3 + R4 (last I3)		; 18-20
	vaddpd	ymm7, ymm7, ymm12		;; R3 + I4 (last R4)		; 19-21
	vsubpd	ymm2, ymm2, ymm6		;; I3 - R4 (last I4)		; 20-22

	vmovapd	ymm12, [screg+128+32]		;; cosine/sine
	vmulpd	ymm6, ymm13, ymm12		;; A3 = R3 * cosine/sine	;  20-24
	vmulpd	ymm12, ymm3, ymm12		;; B3 = I3 * cosine/sine	;  21-25
	vsubpd	ymm6, ymm6, ymm3		;; A3 = A3 - I3			; 25-27
	vaddpd	ymm12, ymm12, ymm13		;; B3 = B3 + R3			; 26-28
	vmovapd	ymm3, [screg+384+32]		;; cosine/sine

	vsubpd	ymm13, ymm0, ymm10		;; R8 - I8 (newer R8)		; 21-23
	vaddpd	ymm0, ymm0, ymm10		;; R8 + I8 (newer I8)		; 22-24
	vsubpd	ymm10, ymm4, ymm9		;; R6 - I6 (newer R6)		; 23-25
	vaddpd	ymm4, ymm4, ymm9		;; R6 + I6 (newer I6)		; 24-26
	L1prefetch srcreg+d4+d2+d1+L1pd, L1pt

	vmulpd	ymm9, ymm7, ymm3		;; A4 = R4 * cosine/sine	;  22-26
	vmulpd	ymm3, ymm2, ymm3		;; B4 = I4 * cosine/sine	;  23-27
	vsubpd	ymm9, ymm9, ymm2		;; A4 = A4 - I4			; 27-29
	vmovapd	ymm2, [screg+128]
	vaddpd	ymm3, ymm3, ymm7		;; B4 = B4 + R4			; 28-30

	vmulpd	ymm6, ymm6, ymm2		;; A3 = A3 * sine (final R3)	;  28-32
	vmulpd	ymm12, ymm12, ymm2		;; B3 = B3 * sine (final I3)	;  29-33
	vmovapd	ymm2, [screg+384]		;; sine

	vaddpd	ymm7, ymm8, ymm15		;; R5 + I7 (newer R7)		; 29-31
	vsubpd	ymm8, ymm8, ymm15		;; R5 - I7 (newer R5)		; 30-32

	vmulpd	ymm9, ymm9, ymm2		;; A4 = A4 * sine (final R4)	;  30-34
	vmulpd	ymm3, ymm3, ymm2		;; B4 = B4 * sine (final I4)	;  31-35
	vmovapd	ymm2, [screg+192+32]		;; cosine/sine

	vsubpd	ymm15, ymm5, ymm1		;; I5 - R7 (newer I7)		; 31-33
 	vaddpd	ymm5, ymm5, ymm1		;; I5 + R7 (newer I5)		; 32-34

	vsubpd	ymm1, ymm7, ymm0		;; R7 - I8 (last R7)		; 33-35
	vmovapd	[dstreg+e2], ymm6		;; Save R3			; 33
	vaddpd	ymm6, ymm15, ymm13		;; I7 + R8 (last I7)		; 34-36
	vmovapd	[dstreg+e2+32], ymm12		;; Save I3			; 34

	vaddpd	ymm7, ymm7, ymm0		;; R7 + I8 (last R8)		; 35-37
	vmovapd	ymm12, [screg+448+32]		;; cosine/sine

	vsubpd	ymm15, ymm15, ymm13		;; I7 - R8 (last I8)		; 36-38
	vmulpd	ymm13, ymm1, ymm2		;; A7 = R7 * cosine/sine	;  36-40
	vmovapd	[dstreg+e2+e1], ymm9		;; Save R4			; 35

	vsubpd	ymm9, ymm8, ymm10		;; R5 - R6 (last R6)		; 37-39
	vmulpd	ymm2, ymm6, ymm2		;; B7 = I7 * cosine/sine	;  37-41
	vmovapd	[dstreg+e2+e1+32], ymm3		;; Save I4			; 36

	vsubpd	ymm3, ymm5, ymm4		;; I5 - I6 (last I6)		; 38-40
	vmulpd	ymm0, ymm7, ymm12		;; A8 = R8 * cosine/sine	;  38-42

	vaddpd	ymm8, ymm8, ymm10		;; R5 + R6 (last R5)		; 39-41
	vmovapd	ymm10, [screg+320+32]		;; cosine/sine			
	vmulpd	ymm12, ymm15, ymm12		;; B8 = I8 * cosine/sine	;  39-43

	vaddpd	ymm5, ymm5, ymm4		;; I5 + I6 (last I5)		; 40-42
	vmulpd	ymm4, ymm9, ymm10		;; A6 = R6 * cosine/sine	;  40-44

	vsubpd	ymm13, ymm13, ymm6		;; A7 = A7 - I7			; 41-43
	vmovapd	ymm6, [screg+64+32]		;; cosine/sine
	vmulpd	ymm10, ymm3, ymm10		;; B6 = I6 * cosine/sine	;  41-45

	vaddpd	ymm2, ymm2, ymm1		;; B7 = B7 + R7			; 42-44
	vmulpd	ymm1, ymm8, ymm6		;; A5 = R5 * cosine/sine	;  42-46

	vsubpd	ymm0, ymm0, ymm15		;; A8 = A8 - I8			; 43-45
	vmovapd	ymm15, [screg+192]		;; sine
	vmulpd	ymm6, ymm5, ymm6		;; B5 = I5 * cosine/sine	;  43-47

	vaddpd	ymm12, ymm12, ymm7		;; B8 = B8 + R8			; 44-46
	vmovapd	ymm7, [dstreg]			;; Reload R1
	vmulpd	ymm13, ymm13, ymm15		;; A7 = A7 * sine (final R7)	;  44-48
	vmovapd	[dstreg+e4+e2], ymm13		;; Save R7			; 49

	vmovapd	ymm13, [dstreg+e1]		;; Reload R2
	vmulpd	ymm2, ymm2, ymm15		;; B7 = B7 * sine (final I7)	;  45-49
	vsubpd	ymm15, ymm7, ymm13		;; R1 - R2 (last R2)		; 45-47
	vmovapd	[dstreg+e4+e2+32], ymm2		;; Save I7			; 50

	vmovapd	ymm2, [screg+448]		;; sine
	vmulpd	ymm0, ymm0, ymm2		;; A8 = A8 * sine (final R8)	;  46-50
	vmovapd	[dstreg+e4+e2+e1], ymm0		;; Save R8			; 51
	vsubpd	ymm0, ymm11, ymm14		;; I1 - I2 (last I2)		; 46-48

	vaddpd	ymm7, ymm7, ymm13		;; R1 + R2 (last R1)		; 47-49
	vmovapd	ymm13, [screg+256+32]		;; cosine/sine
	vmulpd	ymm12, ymm12, ymm2		;; B8 = B8 * sine (final I8)	;  47-51

	vaddpd	ymm11, ymm11, ymm14		;; I1 + I2 (last I1)		; 48-50
	vmulpd	ymm14, ymm15, ymm13		;; A2 = R2 * cosine/sine	;  48-52
	vmovapd	ymm2, [screg+0+32]		;; cosine/sine

	vsubpd	ymm4, ymm4, ymm3		;; A6 = A6 - I6			; 49-51
	vmulpd	ymm13, ymm0, ymm13		;; B2 = I2 * cosine/sine	;  49-53
	vmovapd	ymm3, [screg+320]		;; sine

	vaddpd	ymm10, ymm10, ymm9		;; B6 = B6 + R6			; 50-52
	vmulpd	ymm9, ymm7, ymm2		;; A1 = R1 * cosine/sine	;  50-54

	vsubpd	ymm1, ymm1, ymm5		;; A5 = A5 - I5			; 51-53
	vmulpd	ymm2, ymm11, ymm2		;; B1 = I1 * cosine/sine	;  51-55
	vmovapd	ymm5, [screg+64]		;; sine

	vaddpd	ymm6, ymm6, ymm8		;; B5 = B5 + R5			; 52-54
	vmulpd	ymm4, ymm4, ymm3		;; A6 = A6 * sine (final R6)	;  52-56
	vmovapd	ymm8, [screg+256]		;; sine

	vsubpd	ymm14, ymm14, ymm0		;; A2 = A2 - I2			; 53-55
	vmulpd	ymm10, ymm10, ymm3		;; B6 = B6 * sine (final I6)	;  53-57
	vmovapd	ymm0, [screg+0]			;; sine

	vaddpd	ymm13, ymm13, ymm15		;; B2 = B2 + R2			; 54-56
	vmulpd	ymm1, ymm1, ymm5		;; A5 = A5 * sine (final R5)	;  54-58
	vmovapd	[dstreg+e4+e2+e1+32], ymm12	;; Save I8			; 52

	vsubpd	ymm9, ymm9, ymm11		;; A1 = A1 - I1			; 55-57
	vmulpd	ymm6, ymm6, ymm5		;; B5 = B5 * sine (final I5)	;  55-59

	vaddpd	ymm2, ymm2, ymm7		;; B1 = B1 + R1			; 56-58
	vmulpd	ymm14, ymm14, ymm8		;; A2 = A2 * sine (final R2)	;  56-60

	vmulpd	ymm13, ymm13, ymm8		;; B2 = B2 * sine (final I2)	;  57-61
	vmovapd	[dstreg+e4+e1], ymm4		;; Save R6			; 57

	vmulpd	ymm9, ymm9, ymm0		;; A1 = A1 * sine (final R1)	;  58-62
	vmovapd	[dstreg+e4+e1+32], ymm10	;; Save I6			; 58

	vmulpd	ymm2, ymm2, ymm0		;; B1 = B1 * sine (final I1)	;  59-63
	vmovapd	[dstreg+e4], ymm1		;; Save R5			; 59

	vmovapd	[dstreg+e4+32], ymm6		;; Save I5			; 60
	vmovapd	[dstreg+e1], ymm14		;; Save R2			; 61
	vmovapd	[dstreg+e1+32], ymm13		;; Save I2			; 62
	vmovapd	[dstreg], ymm9			;; Save R1			; 63
	vmovapd	[dstreg+32], ymm2		;; Save I1			; 64

	bump	srcreg, srcinc
	bump	dstreg, dstinc
	bump	screg, scinc
	ENDM

ENDIF

;;
;; ************************************* eight-complex-unfft8 variants ******************************************
;;

;
; Radix-8 inverse FFT building block for all-complex delayed twiddle multipliers from
; the first few levels (r4delay).  Has 8 premult/sin/cos multipliers.
;

yr8_sg8cl_eight_complex_unfft8 MACRO srcreg,srcinc,d1,d2,d4,dstreg,dstinc,e1,e2,e4,screg,scinc,maxrpt,L1pt,L1pd

	vmovapd	ymm0, [screg+64+32]		;; cosine/sine
	vmovapd	ymm4, [srcreg+d4]		;; R5
	vmulpd	ymm5, ymm4, ymm0		;; A5 = R5 * cosine/sine
	vmovapd	ymm2, [srcreg+d4+32]		;; I5
	vaddpd	ymm5, ymm5, ymm2		;; A5 = A5 + I5
	vmulpd	ymm2, ymm2, ymm0		;; B5 = I5 * cosine/sine

	vmovapd	ymm0, [screg+320+32]		;; cosine/sine
	vmovapd	ymm6, [srcreg+d4+d1]		;; R6
	vmulpd	ymm7, ymm6, ymm0		;; A6 = R6 * cosine/sine
	vsubpd	ymm2, ymm2, ymm4		;; B5 = B5 - R5
	vmovapd ymm4, [srcreg+d4+d1+32]		;; I6
	vaddpd	ymm7, ymm7, ymm4		;; A6 = A6 + I6
	vmulpd	ymm4, ymm4, ymm0		;; B6 = I6 * cosine/sine
	vsubpd	ymm4, ymm4, ymm6		;; B6 = B6 - R6

	vmovapd	ymm6, [screg+64]		;; sine
	vmulpd	ymm5, ymm5, ymm6		;; A5 = A5 * sine (first R5)
	vmulpd	ymm2, ymm2, ymm6		;; B5 = B5 * sine (first I5)
	vmovapd	ymm6, [screg+320]		;; sine
	vmulpd	ymm7, ymm7, ymm6		;; A6 = A6 * sine (first R6)
	vmulpd	ymm4, ymm4, ymm6		;; B6 = B6 * sine (first I6)

	vsubpd	ymm6, ymm5, ymm7		;; R5 - R6 (new R6)
	vaddpd	ymm5, ymm5, ymm7		;; R5 + R6 (new R5)

	L1prefetch srcreg+L1pd, L1pt

	vsubpd	ymm7, ymm2, ymm4		;; I5 - I6 (new I6)
	vaddpd	ymm2, ymm2, ymm4		;; I5 + I6 (new I5)

	vmovapd	ymm0, [screg+192+32]		;; cosine/sine
	vmovapd	ymm4, [srcreg+d4+d2]		;; R7
	vmulpd	ymm3, ymm4, ymm0		;; A7 = R7 * cosine/sine
	vmovapd	ymm1, [srcreg+d4+d2+32]		;; I7
	vaddpd	ymm3, ymm3, ymm1		;; A7 = A7 + I7
	vmulpd	ymm1, ymm1, ymm0		;; B7 = I7 * cosine/sine

	vmovapd	ymm0, [screg+448+32]		;; cosine/sine
	vmovapd	[dstreg+e4+e1], ymm6		;; Save new R6
	vmovapd	ymm6, [srcreg+d4+d2+d1]		;; R8
	vmovapd	[dstreg+e4+e1+32], ymm7		;; Save new I6
	vmulpd	ymm7, ymm6, ymm0		;; A8 = R8 * cosine/sine
	vsubpd	ymm1, ymm1, ymm4		;; B7 = B7 - R7
	vmovapd ymm4, [srcreg+d4+d2+d1+32]	;; I8
	vaddpd	ymm7, ymm7, ymm4		;; A8 = A8 + I8
	vmulpd	ymm4, ymm4, ymm0		;; B8 = I8 * cosine/sine
	vsubpd	ymm4, ymm4, ymm6		;; B8 = B8 - R8

	vmovapd	ymm0, [screg+192]		;; sine
	vmulpd	ymm3, ymm3, ymm0		;; A7 = A7 * sine (first R7)
	vmulpd	ymm1, ymm1, ymm0		;; B7 = B7 * sine (first I7)
	vmovapd	ymm0, [screg+448]		;; sine
	vmulpd	ymm7, ymm7, ymm0		;; A8 = A8 * sine (first R8)
	vmulpd	ymm4, ymm4, ymm0		;; B8 = B8 * sine (first I8)

	vsubpd	ymm6, ymm7, ymm3		;; R8 - R7 (new I8)
	vaddpd	ymm7, ymm7, ymm3		;; R8 + R7 (new R7)

	L1prefetch srcreg+d1+L1pd, L1pt

	vsubpd	ymm3, ymm1, ymm4		;; I7 - I8 (new R8)
	vaddpd	ymm1, ymm1, ymm4		;; I7 + I8 (new I7)

	vmovapd	ymm0, [screg+0+32]		;; cosine/sine
	vmovapd	ymm4, [srcreg]			;; R1
	vmovapd	[dstreg+e4], ymm5		;; Save new R5
	vmulpd	ymm5, ymm4, ymm0		;; A1 = R1 * cosine/sine

	vmovapd	[dstreg+e4+32], ymm2		;; Save new I5
	vmovapd	ymm2, [screg+256+32]		;; cosine/sine
	vmovapd	[dstreg+e4+e2+e1+32], ymm6	;; Save new I8
	vmovapd	ymm6, [srcreg+d1]		;; R2
	vmovapd	[dstreg+e4+e2], ymm7		;; Save new R7
	vmulpd	ymm7, ymm6, ymm2		;; A2 = R2 * cosine/sine

	vmovapd	[dstreg+e4+e2+e1], ymm3		;; Save new R8
	vmovapd	ymm3, [srcreg+32]		;; I1
	vaddpd	ymm5, ymm5, ymm3		;; A1 = A1 + I1
	vmulpd	ymm3, ymm3, ymm0		;; B1 = I1 * cosine/sine
	vmovapd	ymm0, [srcreg+d1+32]		;; I2
	vaddpd	ymm7, ymm7, ymm0		;; A2 = A2 + I2
	vmulpd	ymm0, ymm0, ymm2		;; B2 = I2 * cosine/sine
	vsubpd	ymm3, ymm3, ymm4		;; B1 = B1 - R1
	vsubpd	ymm0, ymm0, ymm6		;; B2 = B2 - R2

	vmovapd	ymm2, [screg+0]			;; sine
	vmulpd	ymm5, ymm5, ymm2		;; A1 = A1 * sine (first R1)
	vmovapd	ymm4, [screg+256]		;; sine
	vmulpd	ymm7, ymm7, ymm4		;; A2 = A2 * sine (first R2)
	vmulpd	ymm3, ymm3, ymm2		;; B1 = B1 * sine (first I1)
	vmulpd	ymm0, ymm0, ymm4		;; B2 = B2 * sine (first I2)

	vsubpd	ymm2, ymm5, ymm7		;; R1 - R2 (new R2)
	vaddpd	ymm5, ymm5, ymm7		;; R1 + R2 (new R1)

	L1prefetch srcreg+d2+L1pd, L1pt

	vsubpd	ymm4, ymm3, ymm0		;; I1 - I2 (new I2)
	vaddpd	ymm3, ymm3, ymm0		;; I1 + I2 (new I1)

	vmovapd	ymm0, [screg+128+32]		;; cosine/sine
	vmovapd	ymm6, [srcreg+d2]		;; R3
	vmulpd	ymm7, ymm6, ymm0		;; A3 = R3 * cosine/sine
	vmovapd	[dstreg+e4+e2+32], ymm1		;; Save new I7
	vmovapd	ymm1, [srcreg+d2+32]		;; I3
	vaddpd	ymm7, ymm7, ymm1		;; A3 = A3 + I3
	vmulpd	ymm1, ymm1, ymm0		;; B3 = I3 * cosine/sine

	vmovapd	ymm0, [screg+384+32]		;; cosine/sine
	vmovapd	[dstreg+e1], ymm2		;; Save new R2
	vmovapd	ymm2, [srcreg+d2+d1]		;; R4
	vmovapd	[dstreg+e1+32], ymm4		;; Save new I2
	vmulpd	ymm4, ymm2, ymm0		;; A4 = R4 * cosine/sine
	vsubpd	ymm1, ymm1, ymm6		;; B3 = B3 - R3
	vmovapd ymm6, [srcreg+d2+d1+32]		;; I4
	vaddpd	ymm4, ymm4, ymm6		;; A4 = A4 + I4
	vmulpd	ymm6, ymm6, ymm0		;; B4 = I4 * cosine/sine
	vsubpd	ymm6, ymm6, ymm2		;; B4 = B4 - R4

	vmovapd	ymm0, [screg+128]		;; sine
	vmulpd	ymm7, ymm7, ymm0		;; A3 = A3 * sine (first R3)
	vmulpd	ymm1, ymm1, ymm0		;; B3 = B3 * sine (first I3)
	vmovapd	ymm0, [screg+384]		;; sine
	vmulpd	ymm4, ymm4, ymm0		;; A4 = A4 * sine (first R4)
	vmulpd	ymm6, ymm6, ymm0		;; B4 = B4 * sine (first I4)

	vsubpd	ymm0, ymm4, ymm7		;; R4 - R3 (new I4)
	vaddpd	ymm4, ymm4, ymm7		;; R4 + R3 (new R3)

	L1prefetch srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm7, ymm1, ymm6		;; I3 - I4 (new R4)
	vaddpd	ymm1, ymm1, ymm6		;; I3 + I4 (new I3)

	vmovapd	ymm2, [dstreg+e4+32]		;; Reload new I5
	vmovapd	ymm6, [dstreg+e4+e2+32]		;; Reload new I7
	vmovapd	[dstreg+e2+e1+32], ymm0		;; Save new I4
	vsubpd	ymm0, ymm2, ymm6		;; I5 - I7 (newer R7)
	vaddpd	ymm2, ymm2, ymm6		;; I5 + I7 (newer I5)

	vsubpd	ymm6, ymm5, ymm4		;; R1 - R3 (newer R3)
	vaddpd	ymm4, ymm5, ymm4		;; R1 + R3 (newer R1)

	L1prefetch srcreg+d4+L1pd, L1pt

	vsubpd	ymm5, ymm6, ymm0		;; R3 - R7 (final R7)
	vaddpd	ymm6, ymm6, ymm0		;; R3 + R7 (final R3)

	vsubpd	ymm0, ymm3, ymm1		;; I1 - I3 (newer I3)
	vaddpd	ymm3, ymm3, ymm1		;; I1 + I3 (newer I1)

	vmovapd	ymm1, [dstreg+e4+e2]		;; Reload new R7
	vmovapd	[dstreg+e4+e2], ymm5		;; Save final R7
	vmovapd	ymm5, [dstreg+e4]		;; Reload new R5
	vmovapd	[dstreg+e2], ymm6		;; Save final R3
	vaddpd	ymm6, ymm1, ymm5		;; R7 + R5 (newer R5)
	vsubpd	ymm1, ymm1, ymm5		;; R7 - R5 (newer I7)

	vsubpd	ymm5, ymm3, ymm2		;; I1 - I5 (final I5)
	vaddpd	ymm3, ymm3, ymm2		;; I1 + I5 (final I1)

	L1prefetch srcreg+d4+d1+L1pd, L1pt

	vsubpd	ymm2, ymm4, ymm6		;; R1 - R5 (final R5)
	vaddpd	ymm4, ymm4, ymm6		;; R1 + R5 (final R1)

	vsubpd	ymm6, ymm0, ymm1		;; I3 - I7 (final I7)
	vaddpd	ymm0, ymm0, ymm1		;; I3 + I7 (final I3)

	;; multiply R6/I6 by SQRTHALF - i*SQRTHALF
	vmovapd	ymm1, [dstreg+e4+e1+32]		;; Reload new I6
	vmovapd	[dstreg+e4+32], ymm5		;; Save final I5
	vmovapd	ymm5, [dstreg+e4+e1]		;; Reload new R6
	vmovapd	[dstreg+32], ymm3		;; Save final I1
	vsubpd	ymm3, ymm1, ymm5		;; I6 = I6 - R6
	vaddpd	ymm1, ymm1, ymm5		;; R6 = R6 + I6

	;; multiply R8/I8 by SQRTHALF - i*SQRTHALF
	vmovapd	ymm5, [dstreg+e4+e2+e1+32]	;; Reload new I8
	vmovapd	[dstreg+e4], ymm2		;; Save final R5
	vmovapd	ymm2, [dstreg+e4+e2+e1]		;; Reload new R8
	vmovapd	[dstreg], ymm4			;; Save final R1
	vsubpd	ymm4, ymm5, ymm2		;; I8 = I8 - R8
	vaddpd	ymm5, ymm5, ymm2		;; R8 = R8 + I8

	vmovapd	ymm2, YMM_SQRTHALF
	vmulpd	ymm3, ymm3, ymm2		;; I6 * SQRTHALF
	vmulpd	ymm1, ymm1, ymm2		;; R6 * SQRTHALF
	vmulpd	ymm4, ymm4, ymm2		;; I8 * SQRTHALF
	vmulpd	ymm5, ymm5, ymm2		;; R8 * SQRTHALF

	vmovapd	ymm2, [dstreg+e1]		;; Reload new R2
	vmovapd	[dstreg+e4+e2+32], ymm6		;; Save final I7
	vaddpd	ymm6, ymm2, ymm7		;; R2 + R4 (newer R2)
	vsubpd	ymm2, ymm2, ymm7		;; R2 - R4 (newer R4)

	vsubpd	ymm7, ymm5, ymm1		;; R8 - R6 (newer I8)
	vaddpd	ymm5, ymm5, ymm1		;; R8 + R6 (newer R6)

	L1prefetch srcreg+d4+d2+L1pd, L1pt

	vsubpd	ymm1, ymm3, ymm4		;; I6 - I8 (newer R8)
	vaddpd	ymm3, ymm3, ymm4		;; I6 + I8 (newer I6)

	vsubpd	ymm4, ymm6, ymm5		;; R2 - R6 (final R6)
	vaddpd	ymm6, ymm6, ymm5		;; R2 + R6 (final R2)

	vmovapd	ymm5, [dstreg+e1+32]		;; Reload new I2
	vmovapd	[dstreg+e4+e1], ymm4		;; Save final R6
	vmovapd	ymm4, [dstreg+e2+e1+32]		;; Reload new I4
	vmovapd	[dstreg+e1], ymm6		;; Save final R2
	vaddpd	ymm6, ymm5, ymm4		;; I2 + I4 (newer I2)
	vsubpd	ymm5, ymm5, ymm4		;; I2 - I4 (newer I4)

	vsubpd	ymm4, ymm2, ymm1		;; R4 - R8 (final R8)
	vaddpd	ymm2, ymm2, ymm1		;; R4 + R8 (final R4)

	vsubpd	ymm1, ymm6, ymm3		;; I2 - I6 (final I6)
	vaddpd	ymm6, ymm6, ymm3		;; I2 + I6 (final I2)

	L1prefetch srcreg+d4+d2+d1+L1pd, L1pt

	vsubpd	ymm3, ymm5, ymm7		;; I4 - I8 (final I8)
	vaddpd	ymm5, ymm5, ymm7		;; I4 + I8 (final I4)

	;; Swizzle using 4 shufpds and 4 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm7, [dstreg+32]		;; Reload final I1
	vmovapd	[dstreg+e4+e2+e1], ymm4		;; Save final R8
	vshufpd	ymm4, ymm7, ymm6, 0		;; Shuffle I1 and I2 to create I1/I2 low
	vshufpd	ymm7, ymm7, ymm6, 15		;; Shuffle I1 and I2 to create I1/I2 hi

	vshufpd	ymm6, ymm0, ymm5, 0		;; Shuffle I3 and I4 to create I3/I4 low
	vshufpd	ymm0, ymm0, ymm5, 15		;; Shuffle I3 and I4 to create I3/I4 hi

	vperm2f128 ymm5, ymm4, ymm6, 32		;; Shuffle I1/I2 low and I3/I4 low (final I1)
	vperm2f128 ymm4, ymm4, ymm6, 49		;; Shuffle I1/I2 low and I3/I4 low (final I3)

	vperm2f128 ymm6, ymm7, ymm0, 32		;; Shuffle I1/I2 hi and I3/I4 hi (final I2)
	vperm2f128 ymm7, ymm7, ymm0, 49		;; Shuffle I1/I2 hi and I3/I4 hi (final I4)

	;; Swizzle using 4 shufpds and 4 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm0, [dstreg]			;; Reload final R1
	vmovapd	[dstreg+32], ymm5		;; Save I1
	vmovapd	ymm5, [dstreg+e1]		;; Reload final R2
	vmovapd	[dstreg+e2+32], ymm4		;; Save I3
	vshufpd	ymm4, ymm0, ymm5, 0		;; Shuffle R1 and R2 to create R1/R2 low
	vshufpd	ymm0, ymm0, ymm5, 15		;; Shuffle R1 and R2 to create R1/R2 hi

	vmovapd	ymm5, [dstreg+e2]		;; Reload final R3
	vmovapd	[dstreg+e1+32], ymm6		;; Save I2
	vshufpd	ymm6, ymm5, ymm2, 0		;; Shuffle R3 and R4 to create R3/R4 low
	vshufpd	ymm5, ymm5, ymm2, 15		;; Shuffle R3 and R4 to create R3/R4 hi

	vperm2f128 ymm2, ymm4, ymm6, 32		;; Shuffle R1/R2 low and R3/R4 low (final R1)
	vperm2f128 ymm4, ymm4, ymm6, 49		;; Shuffle R1/R2 low and R3/R4 low (final R3)

	vperm2f128 ymm6, ymm0, ymm5, 32		;; Shuffle R1/R2 hi and R3/R4 hi (final R2)
	vperm2f128 ymm0, ymm0, ymm5, 49		;; Shuffle R1/R2 hi and R3/R4 hi (final R4)

	;; Swizzle using 4 shufpds and 4 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm5, [dstreg+e4+32]		;; Reload final I5
	vmovapd	[dstreg+e2+e1+32], ymm7		;; Save I4
	vshufpd	ymm7, ymm5, ymm1, 0		;; Shuffle I5 and I6 to create I5/I6 low
	vshufpd	ymm5, ymm5, ymm1, 15		;; Shuffle I5 and I6 to create I5/I6 hi

	vmovapd	ymm1, [dstreg+e4+e2+32]		;; Reload final I7
	vmovapd	[dstreg], ymm2			;; Save R1
	vshufpd	ymm2, ymm1, ymm3, 0		;; Shuffle I7 and I8 to create I7/I8 low
	vshufpd	ymm1, ymm1, ymm3, 15		;; Shuffle I7 and I8 to create I7/I8 hi

	vperm2f128 ymm3, ymm7, ymm2, 32		;; Shuffle I5/I6 low and I7/I8 low (final I5)
	vperm2f128 ymm7, ymm7, ymm2, 49		;; Shuffle I5/I6 low and I7/I8 low (final I7)

	vperm2f128 ymm2, ymm5, ymm1, 32		;; Shuffle I5/I6 hi and I7/I8 hi (final I6)
	vperm2f128 ymm5, ymm5, ymm1, 49		;; Shuffle I5/I6 hi and I7/I8 hi (final I8)

	;; Swizzle using 4 shufpds and 4 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm1, [dstreg+e4]		;; Reload final R5
	vmovapd	[dstreg+e2], ymm4		;; Save R3
	vmovapd	ymm4, [dstreg+e4+e1]		;; Reload final R6
	vmovapd	[dstreg+e1], ymm6		;; Save R2
	vshufpd	ymm6, ymm1, ymm4, 0		;; Shuffle R5 and R6 to create R5/R6 low
	vshufpd	ymm1, ymm1, ymm4, 15		;; Shuffle R5 and R6 to create R5/R6 hi

	vmovapd	ymm4, [dstreg+e4+e2]		;; Reload final R7
	vmovapd	[dstreg+e2+e1], ymm0		;; Save R4
	vmovapd	ymm0, [dstreg+e4+e2+e1]		;; Reload final R8
	vmovapd	[dstreg+e4+32], ymm3		;; Save I5
	vshufpd	ymm3, ymm4, ymm0, 0		;; Shuffle R7 and R8 to create R7/R8 low
	vshufpd	ymm4, ymm4, ymm0, 15		;; Shuffle R7 and R8 to create R7/R8 hi

	vperm2f128 ymm0, ymm6, ymm3, 32		;; Shuffle R5/R6 low and R7/R8 low (final R5)
	vperm2f128 ymm6, ymm6, ymm3, 49		;; Shuffle R5/R6 low and R7/R8 low (final R7)

	vperm2f128 ymm3, ymm1, ymm4, 32		;; Shuffle R5/R6 hi and R7/R8 hi (final R6)
	vperm2f128 ymm1, ymm1, ymm4, 49		;; Shuffle R5/R6 hi and R7/R8 hi (final R8)

	vmovapd	[dstreg+e4+e2+32], ymm7		;; Save I7
	vmovapd	[dstreg+e4+e1+32], ymm2		;; Save I6
	vmovapd	[dstreg+e4+e2+e1+32], ymm5	;; Save I8
	vmovapd	[dstreg+e4], ymm0		;; Save R5
	vmovapd	[dstreg+e4+e2], ymm6		;; Save R7
	vmovapd	[dstreg+e4+e1], ymm3		;; Save R6
	vmovapd	[dstreg+e4+e2+e1], ymm1		;; Save R8

	bump	srcreg, srcinc
	bump	dstreg, dstinc
	bump	screg, scinc
	ENDM

;; 64-bit version

IFDEF X86_64

yr8_sg8cl_eight_complex_unfft8 MACRO srcreg,srcinc,d1,d2,d4,dstreg,dstinc,e1,e2,e4,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd	ymm0, [screg+0+32]		;; cosine/sine
	vmovapd	ymm3, [srcreg+32]		;; I1
	vmulpd	ymm8, ymm3, ymm0		;; B1 = I1 * cosine/sine	; 1-5

	vmovapd	ymm2, [screg+256+32]		;; cosine/sine
	vmovapd	ymm9, [srcreg+d1+32]		;; I2
	vmulpd	ymm10, ymm9, ymm2		;; B2 = I2 * cosine/sine	; 2-6

	vmovapd	ymm4, [srcreg]			;; R1
	vmulpd	ymm0, ymm4, ymm0		;; A1 = R1 * cosine/sine	; 3-7

	vmovapd	ymm6, [srcreg+d1]		;; R2
	vmulpd	ymm2, ymm6, ymm2		;; A2 = R2 * cosine/sine	; 4-8

	vmovapd	ymm5, [screg+128+32]		;; cosine/sine
	vmovapd	ymm1, [srcreg+d2+32]		;; I3
	vmulpd	ymm7, ymm1, ymm5		;; B3 = I3 * cosine/sine	; 5-9

	vsubpd	ymm8, ymm8, ymm4		;; B1 = B1 - R1			; 6-8
	vmovapd	ymm11, [screg+384+32]		;; cosine/sine
	vmovapd ymm12, [srcreg+d2+d1+32]	;; I4
	vmulpd	ymm4, ymm12, ymm11		;; B4 = I4 * cosine/sine	; 6-10

	vsubpd	ymm10, ymm10, ymm6		;; B2 = B2 - R2			; 7-9
	vmovapd	ymm13, [srcreg+d2]		;; R3
	vmulpd	ymm5, ymm13, ymm5		;; A3 = R3 * cosine/sine	; 7-11

	vaddpd	ymm0, ymm0, ymm3		;; A1 = A1 + I1			; 8-10
	vmovapd	ymm14, [srcreg+d2+d1]		;; R4
	vmulpd	ymm11, ymm14, ymm11		;; A4 = R4 * cosine/sine	; 8-12

	vaddpd	ymm2, ymm2, ymm9		;; A2 = A2 + I2			; 9-11
	vmovapd	ymm15, [screg+0]		;; sine
	vmulpd	ymm8, ymm8, ymm15		;; B1 = B1 * sine (first I1)	; 9-13

	vsubpd	ymm7, ymm7, ymm13		;; B3 = B3 - R3			; 10-12
	vmovapd	ymm6, [screg+256]		;; sine
	vmulpd	ymm10, ymm10, ymm6		;; B2 = B2 * sine (first I2)	; 10-14

	vsubpd	ymm4, ymm4, ymm14		;; B4 = B4 - R4			; 11-13
	vmulpd	ymm0, ymm0, ymm15		;; A1 = A1 * sine (first R1)	; 11-15
	vmovapd	ymm3, [screg+128]		;; sine

	vaddpd	ymm5, ymm5, ymm1		;; A3 = A3 + I3			; 12-14
	vmulpd	ymm2, ymm2, ymm6		;; A2 = A2 * sine (first R2)	; 12-16
	vmovapd	ymm9, [screg+384]		;; sine

	vaddpd	ymm11, ymm11, ymm12		;; A4 = A4 + I4			; 13-15
	vmulpd	ymm7, ymm7, ymm3		;; B3 = B3 * sine (first I3)	; 13-17
	vmovapd	ymm13, [screg+64+32]		;; cosine/sine

	vmulpd	ymm4, ymm4, ymm9		;; B4 = B4 * sine (first I4)	; 14-18
	vmovapd	ymm14, [srcreg+d4]		;; R5

	vaddpd	ymm12, ymm8, ymm10		;; I1 + I2 (new I1)		; 15-17
	vmulpd	ymm5, ymm5, ymm3		;; A3 = A3 * sine (first R3)	; 15-19
	vmovapd	ymm15, [srcreg+d4+32]		;; I5

	vsubpd	ymm8, ymm8, ymm10		;; I1 - I2 (new I2)		; 16-18
	vmulpd	ymm11, ymm11, ymm9		;; A4 = A4 * sine (first R4)	; 16-20
	vmovapd	ymm1, [screg+320+32]		;; cosine/sine

	vsubpd	ymm9, ymm0, ymm2		;; R1 - R2 (new R2)		; 17-19
	vmovapd	ymm6, [srcreg+d4+d1]		;; R6

	vaddpd	ymm0, ymm0, ymm2		;; R1 + R2 (new R1)		; 18-20
	vmulpd	ymm2, ymm14, ymm13		;; A5 = R5 * cosine/sine	; 18-22
	vmovapd ymm3, [srcreg+d4+d1+32]		;; I6
	vmovapd	[dstreg+32], ymm12		;; Save new I1			; 18

	vaddpd	ymm12, ymm7, ymm4		;; I3 + I4 (new I3)		; 19-21
	vmulpd	ymm13, ymm15, ymm13		;; B5 = I5 * cosine/sine	; 19-23
	vmovapd	ymm10, [screg+192+32]		;; cosine/sine
	vmovapd	[dstreg+e1+32], ymm8		;; Save new I2			; 19

	vsubpd	ymm7, ymm7, ymm4		;; I3 - I4 (new R4)		; 20-22
	vmulpd	ymm4, ymm6, ymm1		;; A6 = R6 * cosine/sine	; 20-24

	vsubpd	ymm8, ymm11, ymm5		;; R4 - R3 (new I4)		; 21-23
	vmulpd	ymm1, ymm3, ymm1		;; B6 = I6 * cosine/sine	; 21-25

	vaddpd	ymm11, ymm11, ymm5		;; R4 + R3 (new R3)		; 22-24
	vmovapd	ymm5, [srcreg+d4+d2]		;; R7
	vmovapd	[dstreg+e2+32], ymm12		;; Save new I3			; 22
	vmulpd	ymm12, ymm5, ymm10		;; A7 = R7 * cosine/sine	; 22-26

	vaddpd	ymm2, ymm2, ymm15		;; A5 = A5 + I5			; 23-25
	vmovapd	ymm15, [srcreg+d4+d2+32]	;; I7
	vmulpd	ymm10, ymm15, ymm10		;; B7 = I7 * cosine/sine	; 23-27

	vsubpd	ymm13, ymm13, ymm14		;; B5 = B5 - R5			; 24-26
	vmovapd	ymm14, [screg+448+32]		;; cosine/sine
	vmovapd	[dstreg+e2+e1+32], ymm8		;; Save new I4			; 24
	vmovapd	ymm8, [srcreg+d4+d2+d1]		;; R8

	vaddpd	ymm4, ymm4, ymm3		;; A6 = A6 + I6			; 25-27
	vmulpd	ymm3, ymm8, ymm14		;; A8 = R8 * cosine/sine	; 24-28

	vsubpd	ymm1, ymm1, ymm6		;; B6 = B6 - R6			; 26-28
	vmovapd ymm6, [srcreg+d4+d2+d1+32]	;; I8
	vmulpd	ymm14, ymm6, ymm14		;; B8 = I8 * cosine/sine	; 25-29

	vaddpd	ymm12, ymm12, ymm15		;; A7 = A7 + I7			; 27-29
	vmovapd	ymm15, [screg+64]		;; sine
	vmulpd	ymm2, ymm2, ymm15		;; A5 = A5 * sine (first R5)	; 26-30
	vmulpd	ymm13, ymm13, ymm15		;; B5 = B5 * sine (first I5)	; 27-31
	vmovapd	ymm15, [screg+320]		;; sine

	vsubpd	ymm10, ymm10, ymm5		;; B7 = B7 - R7			; 28-30
	vmulpd	ymm4, ymm4, ymm15		;; A6 = A6 * sine (first R6)	; 28-32
	vmovapd	ymm5, [screg+192]		;; sine

	vaddpd	ymm3, ymm3, ymm6		;; A8 = A8 + I8			; 29-31
	vmulpd	ymm1, ymm1, ymm15		;; B6 = B6 * sine (first I6)	; 29-33
	vmovapd	ymm6, [screg+448]		;; sine

	vsubpd	ymm14, ymm14, ymm8		;; B8 = B8 - R8			; 30-32
	vmulpd	ymm12, ymm12, ymm5		;; A7 = A7 * sine (first R7)	; 30-34
	vmovapd	ymm15, YMM_SQRTHALF

	vsubpd	ymm8, ymm0, ymm11		;; R1 - R3 (newer R3)		; 31-33
	vmulpd	ymm10, ymm10, ymm5		;; B7 = B7 * sine (first I7)	; 31-35

	vaddpd	ymm0, ymm0, ymm11		;; R1 + R3 (newer R1)		; 32-34
	vmulpd	ymm3, ymm3, ymm6		;; A8 = A8 * sine (first R8)	; 32-36
	L1prefetch srcreg+L1pd, L1pt

	vsubpd	ymm11, ymm2, ymm4		;; R5 - R6 (new R6)		; 33-35
	vmulpd	ymm14, ymm14, ymm6		;; B8 = B8 * sine (first I8)	; 33-37

	vaddpd	ymm2, ymm2, ymm4		;; R5 + R6 (new R5)		; 34-36

	vsubpd	ymm4, ymm13, ymm1		;; I5 - I6 (new I6)		; 35-37
	vaddpd	ymm13, ymm13, ymm1		;; I5 + I6 (new I5)		; 36-38
	L1prefetch srcreg+d1+L1pd, L1pt

	vsubpd	ymm1, ymm3, ymm12		;; R8 - R7 (new I8)		; 37-39
	vaddpd	ymm3, ymm3, ymm12		;; R8 + R7 (new R7)		; 38-40

	vsubpd	ymm12, ymm10, ymm14		;; I7 - I8 (new R8)		; 39-41
	vaddpd	ymm10, ymm10, ymm14		;; I7 + I8 (new I7)		; 40-42
	L1prefetch srcreg+d2+L1pd, L1pt

	vsubpd	ymm14, ymm4, ymm11		;; I6 = I6 - R6			; 41-43
	vaddpd	ymm11, ymm11, ymm4		;; R6 = R6 + I6			; 42-44

	vsubpd	ymm4, ymm1, ymm12		;; I8 = I8 - R8			; 43-45

	vaddpd	ymm12, ymm12, ymm1		;; R8 = R8 + I8			; 44-46
	vmulpd	ymm14, ymm14, ymm15		;; I6 * SQRTHALF		; 44-48
	L1prefetch srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm1, ymm13, ymm10		;; I5 - I7 (newer R7)		; 45-47
	vmulpd	ymm11, ymm11, ymm15		;; R6 * SQRTHALF		; 45-49

	vaddpd	ymm13, ymm13, ymm10		;; I5 + I7 (newer I5)		; 46-48
	vmulpd	ymm4, ymm4, ymm15		;; I8 * SQRTHALF		; 46-50

	vsubpd	ymm10, ymm9, ymm7		;; R2 - R4 (newer R4)		; 47-49
	vmulpd	ymm12, ymm12, ymm15		;; R8 * SQRTHALF		; 47-51

	vaddpd	ymm9, ymm9, ymm7		;; R2 + R4 (newer R2)		; 48-50
	L1prefetch srcreg+d4+L1pd, L1pt

	vaddpd	ymm7, ymm3, ymm2		;; R7 + R5 (newer R5)		; 49-51
	vsubpd	ymm3, ymm3, ymm2		;; R7 - R5 (newer I7)		; 50-52

	vaddpd	ymm2, ymm8, ymm1		;; R3 + R7 (final R3)		; 51-53
	vsubpd	ymm8, ymm8, ymm1		;; R3 - R7 (final R7)		; 52-54
	L1prefetch srcreg+d4+d1+L1pd, L1pt

	vsubpd	ymm1, ymm14, ymm4		;; I6 - I8 (newer R8)		; 53-55
	vaddpd	ymm14, ymm14, ymm4		;; I6 + I8 (newer I6)		; 54-56
	vaddpd	ymm4, ymm12, ymm11		;; R8 + R6 (newer R6)		; 55-57
	vsubpd	ymm12, ymm12, ymm11		;; R8 - R6 (newer I8)		; 56-58
	L1prefetch srcreg+d4+d2+L1pd, L1pt

	vaddpd	ymm11, ymm10, ymm1		;; R4 + R8 (final R4)		; 57-59
	vaddpd	ymm15, ymm0, ymm7		;; R1 + R5 (final R1)		; 58-60
	vaddpd	ymm6, ymm9, ymm4		;; R2 + R6 (final R2)		; 59-61

	vshufpd	ymm5, ymm2, ymm11, 0		;; Shuffle R3 and R4 to create R3/R4 low	; 60
	vsubpd	ymm10, ymm10, ymm1		;; R4 - R8 (final R8)				; 60-62
	vmovapd	ymm1, [dstreg+e1+32]		;; Reload new I2

	vshufpd	ymm2, ymm2, ymm11, 15		;; Shuffle R3 and R4 to create R3/R4 hi		; 61
	vsubpd	ymm0, ymm0, ymm7		;; R1 - R5 (final R5)				; 61-63
	vmovapd	ymm11, [dstreg+e2+e1+32]	;; Reload new I4

	vshufpd	ymm7, ymm15, ymm6, 0		;; Shuffle R1 and R2 to create R1/R2 low	; 62
	vsubpd	ymm9, ymm9, ymm4		;; R2 - R6 (final R6)				; 62-64
	L1prefetch srcreg+d4+d2+d1+L1pd, L1pt

	vshufpd	ymm15, ymm15, ymm6, 15		;; Shuffle R1 and R2 to create R1/R2 hi		; 63
	vaddpd	ymm6, ymm1, ymm11		;; I2 + I4 (newer I2)				; 63-65

	vperm2f128 ymm4, ymm7, ymm5, 32		;; Shuffle R1/R2 low and R3/R4 low (final R1)	; 64-65
	vsubpd	ymm1, ymm1, ymm11		;; I2 - I4 (newer I4)				; 64-66
	vmovapd	ymm11, [dstreg+32]		;; Reload new I1

	vperm2f128 ymm7, ymm7, ymm5, 49		;; Shuffle R1/R2 low and R3/R4 low (final R3)	; 65-66
	vmovapd	ymm5, [dstreg+e2+32]		;; Reload new I3
	vmovapd	[dstreg], ymm4			;; Save R1					; 66
	vaddpd	ymm4, ymm11, ymm5		;; I1 + I3 (newer I1)				; 65-67

	vsubpd	ymm11, ymm11, ymm5		;; I1 - I3 (newer I3)				; 66-68
	vperm2f128 ymm5, ymm15, ymm2, 32	;; Shuffle R1/R2 hi and R3/R4 hi (final R2)	; 66-67

	vperm2f128 ymm15, ymm15, ymm2, 49	;; Shuffle R1/R2 hi and R3/R4 hi (final R4)	; 67-68
	vaddpd	ymm2, ymm6, ymm14		;; I2 + I6 (final I2)				; 67-69
	vmovapd	[dstreg+e2], ymm7		;; Save R3					; 67

	vsubpd	ymm6, ymm6, ymm14		;; I2 - I6 (final I6)				; 68-70
	vmovapd	[dstreg+e1], ymm5		;; Save R2					; 68

	vshufpd	ymm5, ymm0, ymm9, 0		;; Shuffle R5 and R6 to create R5/R6 low	; 69
	vaddpd	ymm14, ymm4, ymm13		;; I1 + I5 (final I1)				; 69-71
	vmovapd	[dstreg+e2+e1], ymm15		;; Save R4					; 69

	vshufpd	ymm0, ymm0, ymm9, 15		;; Shuffle R5 and R6 to create R5/R6 hi		; 70
	vaddpd	ymm9, ymm1, ymm12		;; I4 + I8 (final I4)				; 70-72

	vshufpd	ymm15, ymm8, ymm10, 0		;; Shuffle R7 and R8 to create R7/R8 low	; 71
	vaddpd	ymm7, ymm11, ymm3		;; I3 + I7 (final I3)				; 71-73

	vshufpd	ymm8, ymm8, ymm10, 15		;; Shuffle R7 and R8 to create R7/R8 hi		; 72
	vsubpd	ymm4, ymm4, ymm13		;; I1 - I5 (final I5)				; 72-74

	vshufpd	ymm13, ymm14, ymm2, 0		;; Shuffle I1 and I2 to create I1/I2 low	; 73
	vsubpd	ymm1, ymm1, ymm12		;; I4 - I8 (final I8)				; 73-75

	vshufpd	ymm14, ymm14, ymm2, 15		;; Shuffle I1 and I2 to create I1/I2 hi		; 74
	vsubpd	ymm11, ymm11, ymm3		;; I3 - I7 (final I7)				; 74-76

	vshufpd	ymm3, ymm7, ymm9, 0		;; Shuffle I3 and I4 to create I3/I4 low	; 75
	vshufpd	ymm7, ymm7, ymm9, 15		;; Shuffle I3 and I4 to create I3/I4 hi		; 76

	vshufpd	ymm9, ymm4, ymm6, 0		;; Shuffle I5 and I6 to create I5/I6 low
	vshufpd	ymm4, ymm4, ymm6, 15		;; Shuffle I5 and I6 to create I5/I6 hi

	vshufpd	ymm6, ymm11, ymm1, 0		;; Shuffle I7 and I8 to create I7/I8 low
	vshufpd	ymm11, ymm11, ymm1, 15		;; Shuffle I7 and I8 to create I7/I8 hi

	vperm2f128 ymm1, ymm5, ymm15, 32	;; Shuffle R5/R6 low and R7/R8 low (final R5)
	vperm2f128 ymm5, ymm5, ymm15, 49	;; Shuffle R5/R6 low and R7/R8 low (final R7)
	vperm2f128 ymm15, ymm0, ymm8, 32	;; Shuffle R5/R6 hi and R7/R8 hi (final R6)
	vperm2f128 ymm0, ymm0, ymm8, 49		;; Shuffle R5/R6 hi and R7/R8 hi (final R8)

	vmovapd	[dstreg+e4], ymm1		;; Save R5
	vmovapd	[dstreg+e4+e2], ymm5		;; Save R7
	vmovapd	[dstreg+e4+e1], ymm15		;; Save R6
	vmovapd	[dstreg+e4+e2+e1], ymm0		;; Save R8

	vperm2f128 ymm8, ymm13, ymm3, 32	;; Shuffle I1/I2 low and I3/I4 low (final I1)
	vperm2f128 ymm13, ymm13, ymm3, 49	;; Shuffle I1/I2 low and I3/I4 low (final I3)
	vperm2f128 ymm3, ymm14, ymm7, 32	;; Shuffle I1/I2 hi and I3/I4 hi (final I2)
	vperm2f128 ymm14, ymm14, ymm7, 49	;; Shuffle I1/I2 hi and I3/I4 hi (final I4)

	vmovapd	[dstreg+32], ymm8		;; Save I1
	vmovapd	[dstreg+e2+32], ymm13		;; Save I3
	vmovapd	[dstreg+e1+32], ymm3		;; Save I2
	vmovapd	[dstreg+e2+e1+32], ymm14	;; Save I4

	vperm2f128 ymm7, ymm9, ymm6, 32		;; Shuffle I5/I6 low and I7/I8 low (final I5)
	vperm2f128 ymm9, ymm9, ymm6, 49		;; Shuffle I5/I6 low and I7/I8 low (final I7)
	vperm2f128 ymm6, ymm4, ymm11, 32	;; Shuffle I5/I6 hi and I7/I8 hi (final I6)
	vperm2f128 ymm4, ymm4, ymm11, 49	;; Shuffle I5/I6 hi and I7/I8 hi (final I8)

	vmovapd	[dstreg+e4+32], ymm7		;; Save I5
	vmovapd	[dstreg+e4+e2+32], ymm9		;; Save I7
	vmovapd	[dstreg+e4+e1+32], ymm6		;; Save I6
	vmovapd	[dstreg+e4+e2+e1+32], ymm4	;; Save I8

	bump	srcreg, srcinc
	bump	dstreg, dstinc
	bump	screg, scinc
	ENDM

ENDIF


;;
;; ************************************* eight-complex-with-square and variants ******************************************
;;

;;
;; The last three levels of the forward FFT are performed.
;; No sin/cos multipliers are needed.
;;

yr8_8cl_eight_complex_fft_final_preload MACRO
	ENDM
yr8_8cl_eight_complex_fft_final MACRO srcreg,srcinc,d1,d2,d4
	yr8_8c_simple_fft_part1 srcreg,d1,d2,d4
	yr8_8c_simple_fft_part2 srcreg,d1,d2,d4
	bump	srcreg, srcinc
	ENDM

yr8_8c_simple_fft_part1 MACRO srcreg,d1,d2,d4
	vmovapd	ymm0, [srcreg]			;; R1
	vmovapd	ymm1, [srcreg+d4]		;; R5
	vaddpd	ymm2, ymm0, ymm1		;; R1 + R5 (new R1)
	vsubpd	ymm0, ymm0, ymm1		;; R1 - R5 (new R5)

	vmovapd	ymm1, [srcreg+d2]		;; R3
	vmovapd	ymm4, [srcreg+d4+d2]		;; R7
	vaddpd	ymm3, ymm1, ymm4		;; R3 + R7 (new R3)
	vsubpd	ymm1, ymm1, ymm4		;; R3 - R7 (new R7)

	vmovapd	ymm4, [srcreg+32]		;; I1
	vmovapd	ymm5, [srcreg+d4+32]		;; I5
	vaddpd	ymm6, ymm4, ymm5		;; I1 + I5 (new I1)
	vsubpd	ymm4, ymm4, ymm5		;; I1 - I5 (new I5)

	vaddpd	ymm7, ymm2, ymm3		;; R1 + R3 (final R1)
	vsubpd	ymm2, ymm2, ymm3		;; R1 - R3 (final R3)

	vmovapd	[srcreg], ymm7			;; Save R1
	vmovapd	[srcreg+d2], ymm2		;; Save R3

	vmovapd	ymm5, [srcreg+d2+32]		;; I3
	vmovapd	ymm7, [srcreg+d4+d2+32]		;; I7
	vaddpd	ymm3, ymm5, ymm7		;; I3 + I7 (new I3)
	vsubpd	ymm5, ymm5, ymm7		;; I3 - I7 (new I7)

	vaddpd	ymm7, ymm6, ymm3		;; I1 + I3 (final I1)
	vsubpd	ymm6, ymm6, ymm3		;; I1 - I3 (final I3)

	vmovapd	[srcreg+32], ymm7		;; Save I1
	vmovapd	[srcreg+d2+32], ymm6		;; Save I3

	vaddpd	ymm7, ymm4, ymm1		;; I5 + R7 (final I5)
	vsubpd	ymm6, ymm0, ymm5		;; R5 - I7 (final R5)
	vsubpd	ymm4, ymm4, ymm1		;; I5 - R7 (final I7)
	vaddpd	ymm0, ymm0, ymm5		;; R5 + I7 (final R7)

	vmovapd	[srcreg+d4+32], ymm7		;; Save I5
	vmovapd	[srcreg+d4], ymm6		;; Save R5
	vmovapd	[srcreg+d4+d2+32], ymm4		;; Save I7
	vmovapd	[srcreg+d4+d2], ymm0		;; Save R7

	vmovapd	ymm0, [srcreg+d1]		;; R2
	vmovapd	ymm7, [srcreg+d4+d1]		;; R6
	vaddpd	ymm2, ymm0, ymm7		;; R2 + R6 (new R2)
	vsubpd	ymm0, ymm0, ymm7		;; R2 - R6 (new R6)

	vmovapd	ymm1, [srcreg+d2+d1]		;; R4
	vmovapd	ymm7, [srcreg+d4+d2+d1]		;; R8
	vaddpd	ymm3, ymm1, ymm7		;; R4 + R8 (new R4)
	vsubpd	ymm1, ymm1, ymm7		;; R4 - R8 (new R8)

	vmovapd	ymm4, [srcreg+d1+32]		;; I2
	vmovapd	ymm7, [srcreg+d4+d1+32]		;; I6
	vaddpd	ymm6, ymm4, ymm7		;; I2 + I6 (new I2)
	vsubpd	ymm4, ymm4, ymm7		;; I2 - I6 (new I6)

	vaddpd	ymm7, ymm2, ymm3		;; R2 + R4 (final R2)
	vsubpd	ymm2, ymm2, ymm3		;; R2 - R4 (final R4)

	vmovapd	[srcreg+d1], ymm7		;; Save R2
	vmovapd	[srcreg+d2+d1], ymm2		;; Save R4

	vmovapd	ymm5, [srcreg+d2+d1+32]		;; I4
	vmovapd	ymm7, [srcreg+d4+d2+d1+32]	;; I8
	vaddpd	ymm3, ymm5, ymm7		;; I4 + I8 (new I4)
	vsubpd	ymm5, ymm5, ymm7		;; I4 - I8 (new I8)

	vaddpd	ymm7, ymm6, ymm3		;; I2 + I4 (final I2)
	vsubpd	ymm6, ymm6, ymm3		;; I2 - I4 (final I4)

	vmovapd	[srcreg+d1+32], ymm7		;; Save I2
	vmovapd	[srcreg+d2+d1+32], ymm6		;; Save I4

	vaddpd	ymm7, ymm4, ymm1		;; I6 + R8 (new I6)
	vsubpd	ymm6, ymm0, ymm5		;; R6 - I8 (new R6)
	vsubpd	ymm4, ymm4, ymm1		;; I6 - R8 (new I8)
	vaddpd	ymm0, ymm0, ymm5		;; R6 + I8 (new R8)

	vsubpd	ymm1, ymm6, ymm7		;; R6 = R6 - I6
	vaddpd	ymm6, ymm6, ymm7		;; I6 = R6 + I6
	vmovapd	ymm5, YMM_SQRTHALF
	vmulpd	ymm1, ymm1, ymm5		;; R6 = R6 * SQRTHALF (final R6)
	vmulpd	ymm6, ymm6, ymm5		;; I6 = I6 * SQRTHALF (final I6)

	vsubpd	ymm2, ymm0, ymm4		;; R8 = R8 - I8
	vaddpd	ymm0, ymm0, ymm4		;; I8 = R8 + I8
	vmulpd	ymm2, ymm2, ymm5		;; R8 = R8 * SQRTHALF (final R8)
	vmulpd	ymm0, ymm0, ymm5		;; I8 = I8 * SQRTHALF (final I8)

	vmovapd	[srcreg+d4+d1], ymm1		;; Save R6
	vmovapd	[srcreg+d4+d1+32], ymm6		;; Save I6
	vmovapd	[srcreg+d4+d2+d1], ymm2		;; Save R8
	vmovapd	[srcreg+d4+d2+d1+32], ymm0	;; Save I8
	ENDM

yr8_8c_simple_fft_part2 MACRO srcreg,d1,d2,d4
	vmovapd	ymm0, [srcreg+d4]		;; R5
	vmovapd	ymm7, [srcreg+d4+d1] 		;; R6
	vsubpd	ymm2, ymm0, ymm7		;; R5 - R6 (new R6)
	vaddpd	ymm0, ymm0, ymm7		;; R5 + R6 (new R5)

	vmovapd	ymm1, [srcreg+d4+32]		;; I5
	vmovapd	ymm7, [srcreg+d4+d1+32]		;; I6
	vsubpd	ymm3, ymm1, ymm7		;; I5 - I6 (new I6)
	vaddpd	ymm1, ymm1, ymm7		;; I5 + I6 (new I5)

	vmovapd	[srcreg+d4+d1], ymm2		;; Save R6
	vmovapd	[srcreg+d4], ymm0		;; Save R5
	vmovapd	[srcreg+d4+d1+32], ymm3		;; Save I6
	vmovapd	[srcreg+d4+32], ymm1		;; Save I5

	vmovapd	ymm5, [srcreg+d4+d2]		;; R7
	vmovapd	ymm7, [srcreg+d4+d2+d1+32]	;; I8
	vsubpd	ymm4, ymm5, ymm7		;; R7 - I8 (new R7)
	vaddpd	ymm5, ymm5, ymm7		;; R7 + I8 (new R8)

	vmovapd	ymm1, [srcreg+d4+d2+32]		;; I7
	vmovapd	ymm7, [srcreg+d4+d2+d1]		;; R8
	vsubpd	ymm0, ymm1, ymm7		;; I7 - R8 (new I8)
	vaddpd	ymm1, ymm1, ymm7		;; I7 + R8 (new I7)

	vmovapd	[srcreg+d4+d2], ymm4		;; Save R7
	vmovapd	[srcreg+d4+d2+d1], ymm5		;; Save R8
	vmovapd	[srcreg+d4+d2+d1+32], ymm0	;; Save I8
	vmovapd	[srcreg+d4+d2+32], ymm1		;; Save I7

	vmovapd	ymm4, [srcreg]			;; R1
	vmovapd	ymm7, [srcreg+d1]		;; R2
	vsubpd	ymm3, ymm4, ymm7		;; R1 - R2 (new R2)
	vaddpd	ymm4, ymm4, ymm7		;; R1 + R2 (new R1)

	vmovapd	ymm6, [srcreg+32]		;; I1
	vmovapd	ymm7, [srcreg+d1+32]		;; I2
	vsubpd	ymm5, ymm6, ymm7		;; I1 - I2 (new I2)
	vaddpd	ymm6, ymm6, ymm7		;; I1 + I2 (new I1)

	vmovapd	[srcreg+d1], ymm3		;; Save R2
	vmovapd	[srcreg], ymm4			;; Save R1
	vmovapd	[srcreg+d1+32], ymm5		;; Save I2
	vmovapd	[srcreg+32], ymm6		;; Save I1

	vmovapd	ymm6, [srcreg+d2]		;; R3
	vmovapd	ymm7, [srcreg+d2+d1+32]		;; I4
	vsubpd	ymm5, ymm6, ymm7		;; R3 - I4 (new R3)
	vaddpd	ymm6, ymm6, ymm7		;; R3 + I4 (new R4)

	vmovapd	ymm4, [srcreg+d2+32]		;; I3
	vmovapd	ymm7, [srcreg+d2+d1]		;; R4
	vsubpd	ymm3, ymm4, ymm7		;; I3 - R4 (new I4)
	vaddpd	ymm4, ymm4, ymm7		;; I3 + R4 (new I3)

	vmovapd	[srcreg+d2], ymm5		;; Save R3
	vmovapd	[srcreg+d2+d1], ymm6		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm3		;; Save I4
	vmovapd	[srcreg+d2+32], ymm4		;; Save I3
	ENDM

;;
;; The last three levels of the forward FFT are performed, point-wise
;; squaring, and first three levels of the inverse FFT are performed.
;; No sin/cos multipliers are needed.
;;

yr8_8cl_eight_complex_with_square_preload MACRO
	ENDM
yr8_8cl_eight_complex_with_square MACRO srcreg,srcinc,d1,d2,d4,maxrpt,L1pt,L1pd
	yr8_8c_simple_fft_part1 srcreg,d1,d2,d4
	yr8_8c_simple_fft_with_square srcreg,d1,d2,d4,L1pt,L1pd
	yr8_8c_simple_unfft srcreg,d1,d2,d4
	bump	srcreg, srcinc
	ENDM

yr8_8c_simple_fft_with_square MACRO srcreg,d1,d2,d4,L1pt,L1pd
	vmovapd	ymm0, [srcreg+d4]		;; R5
	vmovapd	ymm7, [srcreg+d4+d1] 		;; R6
	vsubpd	ymm2, ymm0, ymm7		;; R5 - R6 (new R6)
	vaddpd	ymm0, ymm0, ymm7		;; R5 + R6 (new R5)

	vmovapd	ymm1, [srcreg+d4+32]		;; I5
	vmovapd	ymm7, [srcreg+d4+d1+32]		;; I6
	vsubpd	ymm3, ymm1, ymm7		;; I5 - I6 (new I6)
	vaddpd	ymm1, ymm1, ymm7		;; I5 + I6 (new I5)

	L1prefetchw srcreg+L1pd, L1pt

	yp_complex_square ymm2, ymm3, ymm7	;; Square R6/I6
	yp_complex_square ymm0, ymm1, ymm7	;; Square R5/I5

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm4, ymm0, ymm2		;; R5 - R6 (new R6)
	vaddpd	ymm0, ymm0, ymm2		;; R5 + R6 (new R5)

	vsubpd	ymm5, ymm1, ymm3		;; I5 - I6 (new I6)
	vaddpd	ymm1, ymm1, ymm3		;; I5 + I6 (new I5)

	vmovapd	[srcreg+d4+d1], ymm4		;; Save R6
	vmovapd	[srcreg+d4], ymm0		;; Save R5
	vmovapd	[srcreg+d4+d1+32], ymm5		;; Save I6
	vmovapd	[srcreg+d4+32], ymm1		;; Save I5

	vmovapd	ymm5, [srcreg+d4+d2]		;; R7
	vmovapd	ymm7, [srcreg+d4+d2+d1+32]	;; I8
	vsubpd	ymm4, ymm5, ymm7		;; R7 - I8 (new R7)
	vaddpd	ymm5, ymm5, ymm7		;; R7 + I8 (new R8)

	vmovapd	ymm1, [srcreg+d4+d2+32]		;; I7
	vmovapd	ymm7, [srcreg+d4+d2+d1]		;; R8
	vsubpd	ymm0, ymm1, ymm7		;; I7 - R8 (new I8)
	vaddpd	ymm1, ymm1, ymm7		;; I7 + R8 (new I7)

	L1prefetchw srcreg+d2+L1pd, L1pt

	yp_complex_square ymm5, ymm0, ymm7	;; Square R8/I8
	yp_complex_square ymm4, ymm1, ymm7	;; Square R7/I7

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm2, ymm5, ymm4		;; R8 - R7 (new I8)
	vaddpd	ymm5, ymm5, ymm4		;; R8 + R7 (new R7)

	vsubpd	ymm3, ymm1, ymm0		;; I7 - I8 (new R8)
	vaddpd	ymm1, ymm1, ymm0		;; I7 + I8 (new I7)

	vmovapd	[srcreg+d4+d2+d1+32], ymm2	;; Save I8
	vmovapd	[srcreg+d4+d2], ymm5		;; Save R7
	vmovapd	[srcreg+d4+d2+d1], ymm3		;; Save R8
	vmovapd	[srcreg+d4+d2+32], ymm1		;; Save I7

	vmovapd	ymm4, [srcreg]			;; R1
	vmovapd	ymm7, [srcreg+d1]		;; R2
	vsubpd	ymm3, ymm4, ymm7		;; R1 - R2 (new R2)
	vaddpd	ymm4, ymm4, ymm7		;; R1 + R2 (new R1)

	vmovapd	ymm6, [srcreg+32]		;; I1
	vmovapd	ymm7, [srcreg+d1+32]		;; I2
	vsubpd	ymm5, ymm6, ymm7		;; I1 - I2 (new I2)
	vaddpd	ymm6, ymm6, ymm7		;; I1 + I2 (new I1)

	L1prefetchw srcreg+d4+L1pd, L1pt

	yp_complex_square ymm3, ymm5, ymm7	;; Square R2/I2
	yp_complex_square ymm4, ymm6, ymm7	;; Square R1/I1

	L1prefetchw srcreg+d4+d1+L1pd, L1pt

	vsubpd	ymm0, ymm4, ymm3		;; R1 - R2 (new R2)
	vaddpd	ymm4, ymm4, ymm3		;; R1 + R2 (new R1)

	vsubpd	ymm1, ymm6, ymm5		;; I1 - I2 (new I2)
	vaddpd	ymm6, ymm6, ymm5		;; I1 + I2 (new I1)

	vmovapd	[srcreg+d1], ymm0		;; Save R2
	vmovapd	[srcreg], ymm4			;; Save R1
	vmovapd	[srcreg+d1+32], ymm1		;; Save I2
	vmovapd	[srcreg+32], ymm6		;; Save I1

	vmovapd	ymm6, [srcreg+d2]		;; R3
	vmovapd	ymm7, [srcreg+d2+d1+32]		;; I4
	vsubpd	ymm5, ymm6, ymm7		;; R3 - I4 (new R3)
	vaddpd	ymm6, ymm6, ymm7		;; R3 + I4 (new R4)

	vmovapd	ymm4, [srcreg+d2+32]		;; I3
	vmovapd	ymm7, [srcreg+d2+d1]		;; R4
	vsubpd	ymm3, ymm4, ymm7		;; I3 - R4 (new I4)
	vaddpd	ymm4, ymm4, ymm7		;; I3 + R4 (new I3)

	L1prefetchw srcreg+d4+d2+L1pd, L1pt

	yp_complex_square ymm6, ymm3, ymm7	;; Square R4/I4
	yp_complex_square ymm5, ymm4, ymm7	;; Square R3/I3

	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt

	vsubpd	ymm0, ymm6, ymm5		;; R4 - R3 (new I4)
	vaddpd	ymm6, ymm6, ymm5		;; R4 + R3 (new R3)

	vsubpd	ymm1, ymm4, ymm3		;; I3 - I4 (new R4)
	vaddpd	ymm4, ymm4, ymm3		;; I3 + I4 (new I3)

	vmovapd	[srcreg+d2+d1+32], ymm0		;; Save I4
	vmovapd	[srcreg+d2], ymm6		;; Save R3
	vmovapd	[srcreg+d2+d1], ymm1		;; Save R4
	vmovapd	[srcreg+d2+32], ymm4		;; Save I3
	ENDM

yr8_8c_simple_unfft MACRO srcreg,d1,d2,d4
	vmovapd	ymm1, [srcreg+d4+32]		;; I5
	vmovapd	ymm7, [srcreg+d4+d2+32]		;; I7
	vsubpd	ymm5, ymm1, ymm7		;; I5 - I7 (new R7)
	vaddpd	ymm1, ymm1, ymm7		;; I5 + I7 (new I5)

	vmovapd	ymm2, [srcreg]			;; R1
	vmovapd	ymm7, [srcreg+d2]		;; R3
	vsubpd	ymm4, ymm2, ymm7		;; R1 - R3 (new R3)
	vaddpd	ymm2, ymm2, ymm7		;; R1 + R3 (new R1)

	vsubpd	ymm0, ymm4, ymm5		;; R3 - R7 (final R7)
	vaddpd	ymm4, ymm4, ymm5		;; R3 + R7 (final R3)

	vmovapd	ymm3, [srcreg+d4+d2]		;; R7
	vmovapd	ymm7, [srcreg+d4]		;; R5
	vaddpd	ymm6, ymm3, ymm7		;; R7 + R5 (new R5)
	vsubpd	ymm3, ymm3, ymm7		;; R7 - R5 (new I7)

	vmovapd	[srcreg+d4+d2], ymm0		;; Save R7
	vmovapd	[srcreg+d2], ymm4		;; Save R3

	vmovapd	ymm4, [srcreg+32]		;; I1
	vmovapd	ymm7, [srcreg+d2+32]		;; I3
	vsubpd	ymm5, ymm4, ymm7		;; I1 - I3 (new I3)
	vaddpd	ymm4, ymm4, ymm7		;; I1 + I3 (new I1)

	vsubpd	ymm7, ymm5, ymm3		;; I3 - I7 (final I7)
	vaddpd	ymm5, ymm5, ymm3		;; I3 + I7 (final I3)

	vmovapd	[srcreg+d4+d2+32], ymm7		;; Save I7
	vmovapd	[srcreg+d2+32], ymm5		;; Save I3

	vsubpd	ymm0, ymm2, ymm6		;; R1 - R5 (final R5)
	vaddpd	ymm2, ymm2, ymm6		;; R1 + R5 (final R1)

	vsubpd	ymm6, ymm4, ymm1		;; I1 - I5 (final I5)
	vaddpd	ymm4, ymm4, ymm1		;; I1 + I5 (final I1)

	vmovapd	[srcreg+d4], ymm0		;; Save R5
	vmovapd	[srcreg], ymm2			;; Save R1
	vmovapd	[srcreg+d4+32], ymm6		;; Save I5
	vmovapd	[srcreg+32], ymm4		;; Save I1

	;; multiply R6/I6 by SQRTHALF - i*SQRTHALF
	vmovapd	ymm4, [srcreg+d4+d1+32]		;; I6
	vmovapd	ymm7, [srcreg+d4+d1]		;; R6
	vsubpd	ymm0, ymm4, ymm7		;; I6 = I6 - R6
	vaddpd	ymm4, ymm4, ymm7		;; R6 = R6 + I6

	;; multiply R8/I8 by SQRTHALF - i*SQRTHALF
	vmovapd	ymm2, [srcreg+d4+d2+d1+32]	;; I8
	vmovapd	ymm7, [srcreg+d4+d2+d1]		;; R8
	vsubpd	ymm1, ymm2, ymm7		;; I8 = I8 - R8
	vaddpd	ymm2, ymm2, ymm7		;; R8 = R8 + I8

	vmovapd	ymm5, YMM_SQRTHALF
	vmulpd	ymm0, ymm0, ymm5		;; I6 * SQRTHALF
	vmulpd	ymm4, ymm4, ymm5		;; R6 * SQRTHALF
	vmulpd	ymm1, ymm1, ymm5		;; I8 * SQRTHALF
	vmulpd	ymm2, ymm2, ymm5		;; R8 * SQRTHALF

	vmovapd	ymm6, [srcreg+d1]		;; R2
	vmovapd	ymm7, [srcreg+d2+d1]		;; R4
	vaddpd	ymm3, ymm6, ymm7		;; R2 + R4 (new R2)
	vsubpd	ymm6, ymm6, ymm7		;; R2 - R4 (new R4)

	vsubpd	ymm5, ymm2, ymm4		;; R8 - R6 (new I8)
	vaddpd	ymm2, ymm2, ymm4		;; R8 + R6 (new R6)

	vsubpd	ymm4, ymm0, ymm1		;; I6 - I8 (new R8)
	vaddpd	ymm0, ymm0, ymm1		;; I6 + I8 (new I6)

	vsubpd	ymm1, ymm3, ymm2		;; R2 - R6 (final R6)
	vaddpd	ymm3, ymm3, ymm2		;; R2 + R6 (final R2)

	vmovapd	[srcreg+d4+d1], ymm1		;; Save R6
	vmovapd	[srcreg+d1], ymm3		;; Save R2

	vmovapd	ymm1, [srcreg+d1+32]		;; I2
	vmovapd	ymm7, [srcreg+d2+d1+32]		;; I4
	vaddpd	ymm3, ymm1, ymm7		;; I2 + I4 (new I2)
	vsubpd	ymm1, ymm1, ymm7		;; I2 - I4 (new I4)

	vsubpd	ymm7, ymm6, ymm4		;; R4 - R8 (final R8)
	vaddpd	ymm6, ymm6, ymm4		;; R4 + R8 (final R4)

	vsubpd	ymm4, ymm3, ymm0		;; I2 - I6 (final I6)
	vaddpd	ymm3, ymm3, ymm0		;; I2 + I6 (final I2)

	vsubpd	ymm0, ymm1, ymm5		;; I4 - I8 (final I8)
	vaddpd	ymm1, ymm1, ymm5		;; I4 + I8 (final I4)

	vmovapd	[srcreg+d4+d2+d1], ymm7		;; Save R8
	vmovapd	[srcreg+d2+d1], ymm6		;; Save R4
	vmovapd	[srcreg+d4+d1+32], ymm4		;; Save I6
	vmovapd	[srcreg+d1+32], ymm3		;; Save I2
	vmovapd	[srcreg+d4+d2+d1+32], ymm0	;; Save I8
	vmovapd	[srcreg+d2+d1+32], ymm1		;; Save I4
	ENDM

;;
;; The last three levels of the forward FFT are performed, point-wise
;; multiplication, and first three levels of the inverse FFT are performed.
;; No sin/cos multipliers are needed.
;;

yr8_8cl_eight_complex_with_mult_preload MACRO
	ENDM
yr8_8cl_eight_complex_with_mult MACRO srcreg,srcinc,d1,d2,d4
	yr8_8c_simple_fft_part1 srcreg,d1,d2,d4
	yr8_8c_simple_fft_with_mult srcreg,srcreg+rbp,d1,d2,d4
	yr8_8c_simple_unfft srcreg,d1,d2,d4
	bump	srcreg, srcinc
	ENDM

yr8_8c_simple_fft_with_mult MACRO srcreg,altsrc,d1,d2,d4
	vmovapd	ymm0, [srcreg+d4]		;; R5
	vmovapd	ymm7, [srcreg+d4+d1] 		;; R6
	vsubpd	ymm2, ymm0, ymm7		;; R5 - R6 (new R6)
	vaddpd	ymm0, ymm0, ymm7		;; R5 + R6 (new R5)

	vmovapd	ymm1, [srcreg+d4+32]		;; I5
	vmovapd	ymm7, [srcreg+d4+d1+32]		;; I6
	vsubpd	ymm3, ymm1, ymm7		;; I5 - I6 (new I6)
	vaddpd	ymm1, ymm1, ymm7		;; I5 + I6 (new I5)

	yp_complex_mult ymm2, ymm3, [altsrc+d4+d1], [altsrc+d4+d1+32], ymm6, ymm7 ;; Mult R6/I6
	yp_complex_mult ymm0, ymm1, [altsrc+d4], [altsrc+d4+32], ymm6, ymm7 ;; Mult R5/I5

	vsubpd	ymm4, ymm0, ymm2		;; R5 - R6 (new R6)
	vaddpd	ymm0, ymm0, ymm2		;; R5 + R6 (new R5)

	vsubpd	ymm5, ymm1, ymm3		;; I5 - I6 (new I6)
	vaddpd	ymm1, ymm1, ymm3		;; I5 + I6 (new I5)

	vmovapd	[srcreg+d4+d1], ymm4		;; Save R6
	vmovapd	[srcreg+d4], ymm0		;; Save R5
	vmovapd	[srcreg+d4+d1+32], ymm5		;; Save I6
	vmovapd	[srcreg+d4+32], ymm1		;; Save I5

	vmovapd	ymm5, [srcreg+d4+d2]		;; R7
	vmovapd	ymm7, [srcreg+d4+d2+d1+32]	;; I8
	vsubpd	ymm4, ymm5, ymm7		;; R7 - I8 (new R7)
	vaddpd	ymm5, ymm5, ymm7		;; R7 + I8 (new R8)

	vmovapd	ymm1, [srcreg+d4+d2+32]		;; I7
	vmovapd	ymm7, [srcreg+d4+d2+d1]		;; R8
	vsubpd	ymm0, ymm1, ymm7		;; I7 - R8 (new I8)
	vaddpd	ymm1, ymm1, ymm7		;; I7 + R8 (new I7)

	yp_complex_mult ymm5, ymm0, [altsrc+d4+d2+d1], [altsrc+d4+d2+d1+32], ymm6, ymm7 ;; Mult R8/I8
	yp_complex_mult ymm4, ymm1, [altsrc+d4+d2], [altsrc+d4+d2+32], ymm6, ymm7 ;; Mult R7/I7

	vsubpd	ymm2, ymm5, ymm4		;; R8 - R7 (new I8)
	vaddpd	ymm5, ymm5, ymm4		;; R8 + R7 (new R7)

	vsubpd	ymm3, ymm1, ymm0		;; I7 - I8 (new R8)
	vaddpd	ymm1, ymm1, ymm0		;; I7 + I8 (new I7)

	vmovapd	[srcreg+d4+d2+d1+32], ymm2	;; Save I8
	vmovapd	[srcreg+d4+d2], ymm5		;; Save R7
	vmovapd	[srcreg+d4+d2+d1], ymm3		;; Save R8
	vmovapd	[srcreg+d4+d2+32], ymm1		;; Save I7

	vmovapd	ymm4, [srcreg]			;; R1
	vmovapd	ymm7, [srcreg+d1]		;; R2
	vsubpd	ymm3, ymm4, ymm7		;; R1 - R2 (new R2)
	vaddpd	ymm4, ymm4, ymm7		;; R1 + R2 (new R1)

	vmovapd	ymm6, [srcreg+32]		;; I1
	vmovapd	ymm7, [srcreg+d1+32]		;; I2
	vsubpd	ymm5, ymm6, ymm7		;; I1 - I2 (new I2)
	vaddpd	ymm6, ymm6, ymm7		;; I1 + I2 (new I1)

	yp_complex_mult ymm3, ymm5, [altsrc+d1], [altsrc+d1+32], ymm0, ymm7 ;; Mult R2/I2
	yp_complex_mult ymm4, ymm6, [altsrc], [altsrc+32], ymm0, ymm7 ;; Mult R1/I1

	vsubpd	ymm0, ymm4, ymm3		;; R1 - R2 (new R2)
	vaddpd	ymm4, ymm4, ymm3		;; R1 + R2 (new R1)

	vsubpd	ymm1, ymm6, ymm5		;; I1 - I2 (new I2)
	vaddpd	ymm6, ymm6, ymm5		;; I1 + I2 (new I1)

	vmovapd	[srcreg+d1], ymm0		;; Save R2
	vmovapd	[srcreg], ymm4			;; Save R1
	vmovapd	[srcreg+d1+32], ymm1		;; Save I2
	vmovapd	[srcreg+32], ymm6		;; Save I1

	vmovapd	ymm6, [srcreg+d2]		;; R3
	vmovapd	ymm7, [srcreg+d2+d1+32]		;; I4
	vsubpd	ymm5, ymm6, ymm7		;; R3 - I4 (new R3)
	vaddpd	ymm6, ymm6, ymm7		;; R3 + I4 (new R4)

	vmovapd	ymm4, [srcreg+d2+32]		;; I3
	vmovapd	ymm7, [srcreg+d2+d1]		;; R4
	vsubpd	ymm3, ymm4, ymm7		;; I3 - R4 (new I4)
	vaddpd	ymm4, ymm4, ymm7		;; I3 + R4 (new I3)

	yp_complex_mult ymm6, ymm3, [altsrc+d2+d1], [altsrc+d2+d1+32], ymm0, ymm7 ;; Mult R4/I4
	yp_complex_mult ymm5, ymm4, [altsrc+d2], [altsrc+d2+32], ymm0, ymm7 ;; Mult R3/I3

	vsubpd	ymm0, ymm6, ymm5		;; R4 - R3 (new I4)
	vaddpd	ymm6, ymm6, ymm5		;; R4 + R3 (new R3)

	vsubpd	ymm1, ymm4, ymm3		;; I3 - I4 (new R4)
	vaddpd	ymm4, ymm4, ymm3		;; I3 + I4 (new I3)

	vmovapd	[srcreg+d2+d1+32], ymm0		;; Save I4
	vmovapd	[srcreg+d2], ymm6		;; Save R3
	vmovapd	[srcreg+d2+d1], ymm1		;; Save R4
	vmovapd	[srcreg+d2+32], ymm4		;; Save I3
	ENDM

;;
;; Point-wise multiplication and first three levels of the inverse FFT are performed.
;; No sin/cos multipliers are needed.
;;

yr8_8cl_eight_complex_with_mulf_preload MACRO
	ENDM
yr8_8cl_eight_complex_with_mulf MACRO srcreg,srcinc,d1,d2,d4
	yr8_8c_simple_fft_with_mulf srcreg,d1,d2,d4
	yr8_8c_simple_unfft srcreg,d1,d2,d4
	bump	srcreg, srcinc
	ENDM

yr8_8c_simple_fft_with_mulf MACRO srcreg,d1,d2,d4
	vmovapd	ymm2, [srcreg+d4+d1][rbx]	;; R6
	vmovapd	ymm3, [srcreg+d4+d1+32][rbx]	;; I6
	vmovapd	ymm0, [srcreg+d4][rbx]		;; R5
	vmovapd	ymm1, [srcreg+d4+32][rbx]	;; I5

	yp_complex_mult ymm2, ymm3, [srcreg+d4+d1][rbp], [srcreg+d4+d1+32][rbp], ymm6, ymm7 ;; Mult R6/I6
	yp_complex_mult ymm0, ymm1, [srcreg+d4][rbp], [srcreg+d4+32][rbp], ymm6, ymm7 ;; Mult R5/I5

	vsubpd	ymm4, ymm0, ymm2		;; R5 - R6 (new R6)
	vaddpd	ymm0, ymm0, ymm2		;; R5 + R6 (new R5)

	vsubpd	ymm5, ymm1, ymm3		;; I5 - I6 (new I6)
	vaddpd	ymm1, ymm1, ymm3		;; I5 + I6 (new I5)

	vmovapd	[srcreg+d4+d1], ymm4		;; Save R6
	vmovapd	[srcreg+d4], ymm0		;; Save R5
	vmovapd	[srcreg+d4+d1+32], ymm5		;; Save I6
	vmovapd	[srcreg+d4+32], ymm1		;; Save I5

	vmovapd	ymm5, [srcreg+d4+d2+d1][rbx]	;; R8
	vmovapd	ymm0, [srcreg+d4+d2+d1+32][rbx]	;; I8
	vmovapd	ymm4, [srcreg+d4+d2][rbx]	;; R7
	vmovapd	ymm1, [srcreg+d4+d2+32][rbx]	;; I7

	yp_complex_mult ymm5, ymm0, [srcreg+d4+d2+d1][rbp], [srcreg+d4+d2+d1+32][rbp], ymm6, ymm7 ;; Mult R8/I8
	yp_complex_mult ymm4, ymm1, [srcreg+d4+d2][rbp], [srcreg+d4+d2+32][rbp], ymm6, ymm7 ;; Mult R7/I7

	vsubpd	ymm2, ymm5, ymm4		;; R8 - R7 (new I8)
	vaddpd	ymm5, ymm5, ymm4		;; R8 + R7 (new R7)

	vsubpd	ymm3, ymm1, ymm0		;; I7 - I8 (new R8)
	vaddpd	ymm1, ymm1, ymm0		;; I7 + I8 (new I7)

	vmovapd	[srcreg+d4+d2+d1+32], ymm2	;; Save I8
	vmovapd	[srcreg+d4+d2], ymm5		;; Save R7
	vmovapd	[srcreg+d4+d2+d1], ymm3		;; Save R8
	vmovapd	[srcreg+d4+d2+32], ymm1		;; Save I7

	vmovapd	ymm3, [srcreg+d1][rbx]		;; R2
	vmovapd	ymm5, [srcreg+d1+32][rbx]	;; I2
	vmovapd	ymm4, [srcreg][rbx]		;; R1
	vmovapd	ymm6, [srcreg+32][rbx]		;; I1

	yp_complex_mult ymm3, ymm5, [srcreg+d1][rbp], [srcreg+d1+32][rbp], ymm0, ymm7 ;; Mult R2/I2
	yp_complex_mult ymm4, ymm6, [srcreg][rbp], [srcreg+32][rbp], ymm0, ymm7 ;; Mult R1/I1

	vsubpd	ymm0, ymm4, ymm3		;; R1 - R2 (new R2)
	vaddpd	ymm4, ymm4, ymm3		;; R1 + R2 (new R1)

	vsubpd	ymm1, ymm6, ymm5		;; I1 - I2 (new I2)
	vaddpd	ymm6, ymm6, ymm5		;; I1 + I2 (new I1)

	vmovapd	[srcreg+d1], ymm0		;; Save R2
	vmovapd	[srcreg], ymm4			;; Save R1
	vmovapd	[srcreg+d1+32], ymm1		;; Save I2
	vmovapd	[srcreg+32], ymm6		;; Save I1

	vmovapd	ymm6, [srcreg+d2+d1][rbx]	;; R4
	vmovapd	ymm3, [srcreg+d2+d1+32][rbx]	;; I4
	vmovapd	ymm5, [srcreg+d2][rbx]		;; R3
	vmovapd	ymm4, [srcreg+d2+32][rbx]	;; I3

	yp_complex_mult ymm6, ymm3, [srcreg+d2+d1][rbp], [srcreg+d2+d1+32][rbp], ymm0, ymm7 ;; Mult R4/I4
	yp_complex_mult ymm5, ymm4, [srcreg+d2][rbp], [srcreg+d2+32][rbp], ymm0, ymm7 ;; Mult R3/I3

	vsubpd	ymm0, ymm6, ymm5		;; R4 - R3 (new I4)
	vaddpd	ymm6, ymm6, ymm5		;; R4 + R3 (new R3)

	vsubpd	ymm1, ymm4, ymm3		;; I3 - I4 (new R4)
	vaddpd	ymm4, ymm4, ymm3		;; I3 + I4 (new I3)

	vmovapd	[srcreg+d2+d1+32], ymm0		;; Save I4
	vmovapd	[srcreg+d2], ymm6		;; Save R3
	vmovapd	[srcreg+d2+d1], ymm1		;; Save R4
	vmovapd	[srcreg+d2+32], ymm4		;; Save I3
	ENDM


;; 64-bit version

IFDEF X86_64

yr8_8cl_eight_complex_with_square_preload MACRO
	ENDM
yr8_8cl_eight_complex_with_square MACRO srcreg,srcinc,d1,d2,d4,maxrpt,L1pt,L1pd

	vmovapd	ymm0, [srcreg]			;; R1
	vmovapd	ymm1, [srcreg+d4]		;; R5
	vaddpd	ymm2, ymm0, ymm1		;; R1 + R5 (new R1)		; 1-3
	vsubpd	ymm0, ymm0, ymm1		;; R1 - R5 (new R5)		; 2-4

	vmovapd	ymm1, [srcreg+d2]		;; R3
	vmovapd	ymm4, [srcreg+d4+d2]		;; R7
	vaddpd	ymm3, ymm1, ymm4		;; R3 + R7 (new R3)		; 3-5
	vsubpd	ymm1, ymm1, ymm4		;; R3 - R7 (new R7)		; 4-6

	vmovapd	ymm4, [srcreg+d1]		;; R2
	vmovapd	ymm6, [srcreg+d4+d1]		;; R6
	vaddpd	ymm5, ymm4, ymm6		;; R2 + R6 (new R2)		; 5-7
	vsubpd	ymm4, ymm4, ymm6		;; R2 - R6 (new R6)		; 6-8

	vmovapd	ymm6, [srcreg+d2+d1]		;; R4
	vmovapd	ymm8, [srcreg+d4+d2+d1]		;; R8
	vaddpd	ymm7, ymm6, ymm8		;; R4 + R8 (new R4)		; 7-9
	vsubpd	ymm6, ymm6, ymm8		;; R4 - R8 (new R8)		; 8-10
	vmovapd	ymm9, [srcreg+d1+32]		;; I2

	vaddpd	ymm8, ymm2, ymm3		;; R1 + R3 (newer R1)		; 9-11
	vsubpd	ymm2, ymm2, ymm3		;; R1 - R3 (newer R3)		; 10-12
	vmovapd	ymm10, [srcreg+d4+d1+32]	;; I6

	vaddpd	ymm3, ymm5, ymm7		;; R2 + R4 (newer R2)		; 11-13
	vsubpd	ymm5, ymm5, ymm7		;; R2 - R4 (newer R4)		; 12-14
	vmovapd	ymm11, [srcreg+d2+d1+32]	;; I4

	vmovapd	[srcreg], ymm8			;; Temporarily save R1		; 12
	vsubpd	ymm8, ymm9, ymm10		;; I2 - I6 (new I6)		; 13-15
	vaddpd	ymm9, ymm9, ymm10		;; I2 + I6 (new I2)		; 14-16
	vmovapd	ymm12, [srcreg+d4+d2+d1+32]	;; I8

	vsubpd	ymm10, ymm11, ymm12		;; I4 - I8 (new I8)		; 15-17
	vaddpd	ymm11, ymm11, ymm12		;; I4 + I8 (new I4)		; 16-18
	vmovapd	ymm13, YMM_SQRTHALF

	vaddpd	ymm12, ymm8, ymm6		;; I6 + R8 (new I6)		; 17-19
	vsubpd	ymm8, ymm8, ymm6		;; I6 - R8 (new I8)		; 18-20
	vmovapd	ymm6, [srcreg+32]		;; I1

	vsubpd	ymm14, ymm4, ymm10		;; R6 - I8 (new R6)		; 19-21
	vaddpd	ymm4, ymm4, ymm10		;; R6 + I8 (new R8)		; 20-22
	vmovapd	ymm10, [srcreg+d4+32]		;; I5

	vmulpd	ymm12, ymm12, ymm13		;; I6 = I6 * SQRTHALF (newer I6) ; 20-24
	vmulpd	ymm8, ymm8, ymm13		;; I8 = I8 * SQRTHALF (newer I8) ; 21-25
	vmulpd	ymm14, ymm14, ymm13		;; R6 = R6 * SQRTHALF (newer R6) ; 22-26
	vmulpd	ymm4, ymm4, ymm13		;; R8 = R8 * SQRTHALF (newer R8) ; 23-27
	vmovapd	ymm15, [srcreg+d2+32]		;; I3

	vaddpd	ymm13, ymm6, ymm10		;; I1 + I5 (new I1)		; 21-23
	vsubpd	ymm6, ymm6, ymm10		;; I1 - I5 (new I5)		; 22-24

	vmovapd	ymm10, [srcreg+d4+d2+32]	;; I7
	vaddpd	ymm7, ymm15, ymm10		;; I3 + I7 (new I3)		; 23-25
	vsubpd	ymm15, ymm15, ymm10		;; I3 - I7 (new I7)		; 24-26

	vsubpd	ymm10, ymm9, ymm11		;; I2 - I4 (newer I4)		; 25-27
	vaddpd	ymm9, ymm9, ymm11		;; I2 + I4 (newer I2)		; 26-28

	vsubpd	ymm11, ymm13, ymm7		;; I1 - I3 (newer I3)		; 27-29
	vaddpd	ymm13, ymm13, ymm7		;; I1 + I3 (newer I1)		; 28-30

	L1prefetchw srcreg+L1pd, L1pt

	vaddpd	ymm7, ymm2, ymm10		;; R3 + I4 (last R4)		; 29-31
	vsubpd	ymm2, ymm2, ymm10		;; R3 - I4 (last R3)		; 30-32

	vsubpd	ymm10, ymm11, ymm5		;; I3 - R4 (last I4)		; 31-33
	vaddpd	ymm11, ymm11, ymm5		;; I3 + R4 (last I3)		; 32-34

	vmulpd	ymm5, ymm7, ymm7		;; R4 * R4			; 32-36
	vmulpd	ymm7, ymm10, ymm7		;; I4 * R4 (I4/2)		; 34-38
	vmulpd	ymm10, ymm10, ymm10		;; I4 * I4			; 35-39
	vsubpd	ymm5, ymm5, ymm10		;; R4^2 - I4^2 (R4)		; 40-42

	vsubpd	ymm10, ymm14, ymm12		;; R6 = R6 - I6			; 33-35
	vaddpd	ymm14, ymm14, ymm12		;; I6 = R6 + I6			; 34-36

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm12, ymm0, ymm15		;; R5 - I7 (newer R5)		; 35-37
	vaddpd	ymm0, ymm0, ymm15		;; R5 + I7 (newer R7)		; 36-68

	vmulpd	ymm15, ymm2, ymm2		;; R3 * R3			; 36-40
	vmulpd	ymm2, ymm11, ymm2		;; I3 * R3 (I3/2)		; 37-41
	vmulpd	ymm11, ymm11, ymm11		;; I3 * I3			; 38-42
	vsubpd	ymm15, ymm15, ymm11		;; R3^2 - I3^2 (R3)		; 43-45

	vaddpd	ymm11, ymm6, ymm1		;; I5 + R7 (newer I5)		; 37-39
	vsubpd	ymm6, ymm6, ymm1		;; I5 - R7 (newer I7)		; 38-40

	vsubpd	ymm1, ymm12, ymm10		;; R5 - R6 (last R6)		; 39-41
	vaddpd	ymm12, ymm12, ymm10		;; R5 + R6 (last R5)		; 41-43

	L1prefetchw srcreg+d2+L1pd, L1pt

	vsubpd	ymm10, ymm11, ymm14		;; I5 - I6 (last I6)		; 42-44
	vaddpd	ymm11, ymm11, ymm14		;; I5 + I6 (last I5)		; 44-46

	vmulpd	ymm14, ymm1, ymm1		;; R6 * R6			; 42-45
	vmulpd	ymm1, ymm10, ymm1		;; I6 * R6 (I6/2)		; 45-49
	vmulpd	ymm10, ymm10, ymm10		;; I6 * I6			; 46-50
	vsubpd	ymm14, ymm14, ymm10		;; R6^2 - I6^2 (R6)		; 51-53

	vsubpd	ymm10, ymm4, ymm8		;; R8 = R8 - I8			; 45-47
	vaddpd	ymm4, ymm4, ymm8		;; I8 = R8 + I8			; 46-48

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vaddpd	ymm8, ymm5, ymm15		;; R4 + R3 (new R3)		; 47-49
	vsubpd	ymm5, ymm5, ymm15		;; R4 - R3 (new I4)		; 48-50

	vmulpd	ymm15, ymm12, ymm12		;; R5 * R5			; 47-51
	vmulpd	ymm12, ymm11, ymm12		;; I5 * R5 (I5/2)		; 48-52
	vmulpd	ymm11, ymm11, ymm11		;; I5 * I5			; 49-53
	vmovapd	[srcreg+d2], ymm8		;; Temporarily save R3		; 50

	vsubpd	ymm8, ymm6, ymm10		;; I7 - R8 (last I8)		; 49-51
	vaddpd	ymm6, ymm6, ymm10		;; I7 + R8 (last I7)		; 50-52

	vaddpd	ymm10, ymm0, ymm4		;; R7 + I8 (last R8)		; 52-54
	vsubpd	ymm0, ymm0, ymm4		;; R7 - I8 (last R7)		; 53-55

	L1prefetchw srcreg+d4+L1pd, L1pt

	vaddpd	ymm4, ymm13, ymm9		;; I1 + I2 (last I1)		; 54-56
	vsubpd	ymm13, ymm13, ymm9		;; I1 - I2 (last I2)		; 55-57

	vsubpd	ymm15, ymm15, ymm11		;; R5^2 - I5^2 (R5)		; 56-58

	vmulpd	ymm9, ymm8, ymm8		;; I8 * I8			; 52-56
	vmulpd	ymm8, ymm8, ymm10		;; I8 * R8 (I8/2)		; 55-59
	vmulpd	ymm10, ymm10, ymm10		;; R8 * R8			; 56-60
	vsubpd	ymm10, ymm10, ymm9		;; R8^2 - I8^2 (R8)		; 61-63

	vmovapd	ymm9, [srcreg]			;; Reload R1
	vaddpd	ymm11, ymm9, ymm3		;; R1 + R2 (last R1)		; 57-59
	vsubpd	ymm9, ymm9, ymm3		;; R1 - R2 (last R2)		; 58-60

	vmulpd	ymm3, ymm6, ymm6		;; I7 * I7			; 57-61
	vmulpd	ymm6, ymm6, ymm0		;; I7 * R7 (I7/2)		; 58-62
	vmulpd	ymm0, ymm0, ymm0		;; R7 * R7			; 59-63
	vsubpd	ymm0, ymm0, ymm3		;; R7^2 - I7^2 (R7)		; 64-66

	vsubpd	ymm3, ymm2, ymm7		;; I3/2 - I4/2 (new R4/2)	; 59-61
	vaddpd	ymm2, ymm2, ymm7		;; I3/2 + I4/2 (new I3/2)	; 60-62

	vmulpd	ymm7, ymm4, ymm4		;; I1 * I1			; 60-64
	vmulpd	ymm4, ymm4, ymm11		;; I1 * R1 (I1/2)		; 61-65
	vmulpd	ymm11, ymm11, ymm11		;; R1 * R1			; 62-66
	vsubpd	ymm11, ymm11, ymm7		;; R1^2 - R1^2 (R1)		; 67-69

	vsubpd	ymm7, ymm12, ymm1		;; I5/2 - I6/2 (new I6/2)	; 62-64
	vaddpd	ymm12, ymm12, ymm1		;; I5/2 + I6/2 (new I5/2)	; 63-65

	vsubpd	ymm1, ymm6, ymm8		;; I7/2 - I8/2 (new R8/2)	; 65-67
	vaddpd	ymm6, ymm6, ymm8		;; I7/2 + I8/2 (new I7/2)	; 66-68

	vmulpd	ymm8, ymm13, ymm13		;; I2 * I2			; 63-67
	vmulpd	ymm13, ymm13, ymm9		;; I2 * R2 (I2/2)		; 64-68
	vmulpd	ymm9, ymm9, ymm9		;; R2 * R2			; 65-69
	vsubpd	ymm9, ymm9, ymm8		;; R2^2 - I2^2 (R2)		; 70-72

	vsubpd	ymm8, ymm15, ymm14		;; R5 - R6 (new R6)		; 68-70
	vaddpd	ymm15, ymm15, ymm14		;; R5 + R6 (new R5)		; 69-71

	vsubpd	ymm14, ymm4, ymm13		;; I1/2 - I2/2 (new I2/2)	; 71-73
	vaddpd	ymm4, ymm4, ymm13		;; I1/2 + I2/2 (new I1/2)	; 72-74

	vmovapd ymm13, YMM_TWO
	vmulpd	ymm3, ymm3, ymm13		;; R4/2 * 2			; 66-70
	vmulpd	ymm2, ymm2, ymm13		;; I3/2 * 2			; 67-71
	vmulpd	ymm7, ymm7, ymm13		;; I6/2 * 2			; 68-72
	vmulpd	ymm12, ymm12, ymm13		;; I5/2 * 2			; 69-73
	vmulpd	ymm1, ymm1, ymm13		;; R8/2 * 2			; 70-74
	vmulpd	ymm6, ymm6, ymm13		;; I7/2 * 2			; 71-75
	vmulpd	ymm14, ymm14, ymm13		;; I2/2 * 2			; 74-78
	vmulpd	ymm4, ymm4, ymm13		;; I1/2 * 2			; 75-79

	vsubpd	ymm13, ymm10, ymm0		;; R8 - R7 (new I8)		; 73-75
	vaddpd	ymm10, ymm10, ymm0		;; R8 + R7 (new R7)		; 74-76

	L1prefetchw srcreg+d4+d1+L1pd, L1pt

	vsubpd	ymm0, ymm11, ymm9		;; R1 - R2 (new R2)		; 75-77
	vaddpd	ymm11, ymm11, ymm9		;; R1 + R2 (new R1)		; 76-78

	;; multiply R6/I6 by SQRTHALF - i*SQRTHALF
	vsubpd	ymm9, ymm7, ymm8		;; I6 = I6 - R6			; 77-79
	vaddpd	ymm8, ymm8, ymm7		;; R6 = R6 + I6			; 78-80

	;; multiply R8/I8 by SQRTHALF - i*SQRTHALF
	vsubpd	ymm7, ymm13, ymm1		;; I8 = I8 - R8			; 79-81
	vaddpd	ymm1, ymm1, ymm13		;; R8 = R8 + I8			; 80-82

	vmovapd	ymm13, YMM_SQRTHALF
	vmulpd	ymm9, ymm9, ymm13		;; I6 * SQRTHALF		; 80-84
	vmulpd	ymm8, ymm8, ymm13		;; R6 * SQRTHALF		; 81-85
	vmulpd	ymm7, ymm7, ymm13		;; I8 * SQRTHALF		; 82-86
	vmulpd	ymm1, ymm1, ymm13		;; R8 * SQRTHALF		; 83-87

	L1prefetchw srcreg+d4+d2+L1pd, L1pt

	vaddpd	ymm13, ymm0, ymm3		;; R2 + R4 (newer R2)		; 81-83
	vsubpd	ymm0, ymm0, ymm3		;; R2 - R4 (newer R4)		; 82-84

	vaddpd	ymm3, ymm14, ymm5		;; I2 + I4 (newer I2)		; 83-85
	vsubpd	ymm14, ymm14, ymm5		;; I2 - I4 (newer I4)		; 84-86

	vsubpd	ymm5, ymm12, ymm6		;; I5 - I7 (newer R7)		; 85-87
	vaddpd	ymm12, ymm12, ymm6		;; I5 + I7 (newer I5)		; 86-88

	vsubpd	ymm6, ymm4, ymm2		;; I1 - I3 (newer I3)		; 87-89
	vaddpd	ymm4, ymm4, ymm2		;; I1 + I3 (newer I1)		; 88-90

	vsubpd	ymm2, ymm9, ymm7		;; I6 - I8 (newer R8)		; 89-91
	vaddpd	ymm9, ymm9, ymm7		;; I6 + I8 (newer I6)		; 90-92

	L1prefetchw srcreg+d4+d2+d1+L1pd, L1pt

	vsubpd	ymm7, ymm1, ymm8		;; R8 - R6 (newer I8)		; 91-93
	vaddpd	ymm1, ymm1, ymm8		;; R8 + R6 (newer R6)		; 92-94

	vsubpd	ymm8, ymm0, ymm2		;; R4 - R8 (final R8)		; 93-95
	vaddpd	ymm0, ymm0, ymm2		;; R4 + R8 (final R4)		; 94-96

	vsubpd	ymm2, ymm3, ymm9		;; I2 - I6 (final I6)		; 95-97
	vaddpd	ymm3, ymm3, ymm9		;; I2 + I6 (final I2)		; 96-98
	vmovapd	[srcreg+d4+d2+d1], ymm8		;; Save R8			; 96

	vsubpd	ymm9, ymm14, ymm7		;; I4 - I8 (final I8)		; 97-99
	vaddpd	ymm14, ymm14, ymm7		;; I4 + I8 (final I4)		; 98-100
	vmovapd	[srcreg+d2+d1], ymm0		;; Save R4			; 97
	vmovapd	[srcreg+d4+d1+32], ymm2		;; Save I6			; 98

	vsubpd	ymm7, ymm13, ymm1		;; R2 - R6 (final R6)		; 99-101
	vaddpd	ymm13, ymm13, ymm1		;; R2 + R6 (final R2)		; 100-102
	vmovapd	[srcreg+d1+32], ymm3		;; Save I2			; 99
	vmovapd	[srcreg+d4+d2+d1+32], ymm9	;; Save I8			; 100

	vmovapd	ymm1, [srcreg+d2]		;; Reload R3
	vsubpd	ymm8, ymm11, ymm1		;; R1 - R3 (newer R3)		; 101-103
	vaddpd	ymm11, ymm11, ymm1		;; R1 + R3 (newer R1)		; 102-104
	vmovapd	[srcreg+d2+d1+32], ymm14	;; Save I4			; 101
	vmovapd	[srcreg+d4+d1], ymm7		;; Save R6			; 102

	vaddpd	ymm1, ymm10, ymm15		;; R7 + R5 (newer R5)		; 103-105
	vsubpd	ymm10, ymm10, ymm15		;; R7 - R5 (newer I7)		; 104-106
	vmovapd	[srcreg+d1], ymm13		;; Save R2			; 103

	vsubpd	ymm15, ymm4, ymm12		;; I1 - I5 (final I5)		; 105-107
	vaddpd	ymm4, ymm4, ymm12		;; I1 + I5 (final I1)		; 106-108

	vsubpd	ymm12, ymm8, ymm5		;; R3 - R7 (final R7)		; 107-109
	vaddpd	ymm8, ymm8, ymm5		;; R3 + R7 (final R3)		; 108-110
	vmovapd	[srcreg+d4+32], ymm15		;; Save I5			; 108

	vsubpd	ymm5, ymm6, ymm10		;; I3 - I7 (final I7)		; 109-111
	vaddpd	ymm6, ymm6, ymm10		;; I3 + I7 (final I3)		; 110-112
	vmovapd	[srcreg+32], ymm4		;; Save I1			; 109
	vmovapd	[srcreg+d4+d2], ymm12		;; Save R7			; 110

	vsubpd	ymm10, ymm11, ymm1		;; R1 - R5 (final R5)		; 111-113
	vaddpd	ymm11, ymm11, ymm1		;; R1 + R5 (final R1)		; 112-114
	vmovapd	[srcreg+d2], ymm8		;; Save R3			; 111
	vmovapd	[srcreg+d4+d2+32], ymm5		;; Save I7			; 112

	vmovapd	[srcreg+d2+32], ymm6		;; Save I3			; 113
	vmovapd	[srcreg+d4], ymm10		;; Save R5			; 114
	vmovapd	[srcreg], ymm11			;; Save R1			; 115

	bump	srcreg, srcinc
	ENDM

ENDIF


;;
;; ************************************* sixteen-reals-fft8 variants ******************************************
;;

;;
;; Do three-and-7/8 levels of the forward FFT on 16 real data values with 7 sin/cos multipliers.
;; Output is 2 real values needing N+1 more levels and 7 complex values needing N more FFT levels.
;;

;; Used in last levels of pass 1 (split premultiplier and delay cases).  Swizzling.

yr8_sg8cl_2sc_sixteen_reals_fft8 MACRO srcreg,srcinc,d1,d2,d4,dstreg,dstinc,e1,e2,e4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd

	;; On input the 64-byte cache lines hold these data values:
	;;	0	+1K	+1K	+1K	8K	+1K	+1K	+1K
	;;	1	...
	;;	2	...
	;;	3	...

	;; These are swizzled to:
	;;	0	+1	+1	+1	8K	+1	+1	+1
	;;	1K	...
	;;	2K	...
	;;	3K	...

	;; Swizzle using 8 shufpds and 8 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm1, [srcreg]			;; R1
	vmovapd	ymm7, [srcreg+d1]		;; R2
	vshufpd	ymm0, ymm1, ymm7, 15		;; Shuffle R1 and R2 to create R1/R2 hi
	vshufpd	ymm1, ymm1, ymm7, 0		;; Shuffle R1 and R2 to create R1/R2 low

	vmovapd	ymm2, [srcreg+d2]		;; R3
	vmovapd	ymm7, [srcreg+d2+d1]		;; R4
	vshufpd	ymm3, ymm2, ymm7, 15		;; Shuffle R3 and R4 to create R3/R4 hi
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle R3 and R4 to create R3/R4 low

	vperm2f128 ymm4, ymm0, ymm3, 32		;; Shuffle R1/R2 hi and R3/R4 hi (first R2)
	vperm2f128 ymm0, ymm0, ymm3, 49		;; Shuffle R1/R2 hi and R3/R4 hi (first R4)

	vperm2f128 ymm3, ymm1, ymm2, 32		;; Shuffle R1/R2 low and R3/R4 low (first R1)
	vperm2f128 ymm1, ymm1, ymm2, 49		;; Shuffle R1/R2 low and R3/R4 low (first R3)

	vmovapd	ymm2, [srcreg+32]		;; I1
	vmovapd	ymm7, [srcreg+d1+32]		;; I2
	vshufpd	ymm5, ymm2, ymm7, 15		;; Shuffle I1 and I2 to create I1/I2 hi
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle I1 and I2 to create I1/I2 low

	vmovapd	ymm6, [srcreg+d2+32]		;; I3
	vmovapd	ymm7, [srcreg+d2+d1+32]		;; I4
	vmovapd	[dstreg], ymm0			;; Save first R4
	vshufpd	ymm0, ymm6, ymm7, 15		;; Shuffle I3 and I4 to create I3/I4 hi
	vshufpd	ymm6, ymm6, ymm7, 0		;; Shuffle I3 and I4 to create I3/I4 low

	vperm2f128 ymm7, ymm5, ymm0, 32		;; Shuffle I1/I2 hi and I3/I4 hi (first R10)
	vperm2f128 ymm5, ymm5, ymm0, 49		;; Shuffle I1/I2 hi and I3/I4 hi (first R12)

	vperm2f128 ymm0, ymm2, ymm6, 32		;; Shuffle I1/I2 low and I3/I4 low (first R9)
	vperm2f128 ymm2, ymm2, ymm6, 49		;; Shuffle I1/I2 low and I3/I4 low (first R11)

	L1prefetch srcreg+L1pd, L1pt

	vaddpd	ymm6, ymm4, ymm7		;; R2 + R10 (new R2)
	vsubpd	ymm4, ymm4, ymm7		;; R2 - R10 (new R10)

	vaddpd	ymm7, ymm3, ymm0		;; R1 + R9 (new R1)
	vsubpd	ymm3, ymm3, ymm0		;; R1 - R9 (new R9)

	L1prefetch srcreg+d1+L1pd, L1pt

	vaddpd	ymm0, ymm1, ymm2		;; R3 + R11 (new R3)
	vsubpd	ymm1, ymm1, ymm2		;; R3 - R11 (new R11)

	vmovapd	ymm2, [dstreg]			;; Reload first R4
	vmovapd	[dstreg+e1], ymm6		;; Save new R2
	vaddpd	ymm6, ymm2, ymm5		;; R4 + R12 (new R4)
	vsubpd	ymm2, ymm2, ymm5		;; R4 - R12 (new R12)

	vmovapd	ymm5, [srcreg+d4]		;; R5
	vmovapd	[dstreg+e4+e1], ymm4		;; Save new R10
	vmovapd	ymm4, [srcreg+d4+d1]		;; R6
	vmovapd	[dstreg], ymm7			;; Save new R1
	vshufpd	ymm7, ymm5, ymm4, 15		;; Shuffle R5 and R6 to create R5/R6 hi
	vshufpd	ymm5, ymm5, ymm4, 0		;; Shuffle R5 and R6 to create R5/R6 low

	vmovapd	ymm4, [srcreg+d4+d2]		;; R7
	vmovapd	[dstreg+e4], ymm3		;; Save new R9
	vmovapd	ymm3, [srcreg+d4+d2+d1]		;; R8
	vmovapd	[dstreg+e2], ymm0		;; Save new R3
	vshufpd	ymm0, ymm4, ymm3, 15		;; Shuffle R7 and R8 to create R7/R8 hi
	vshufpd	ymm4, ymm4, ymm3, 0		;; Shuffle R7 and R8 to create R7/R8 low

	vperm2f128 ymm3, ymm7, ymm0, 32		;; Shuffle R5/R6 hi and R7/R8 hi (first R6)
	vperm2f128 ymm7, ymm7, ymm0, 49		;; Shuffle R5/R6 hi and R7/R8 hi (first R8)

	vperm2f128 ymm0, ymm5, ymm4, 32		;; Shuffle R5/R6 low and R7/R8 low (first R5)
	vperm2f128 ymm5, ymm5, ymm4, 49		;; Shuffle R5/R6 low and R7/R8 low (first R7)

	vmovapd	ymm4, [srcreg+d4+32]		;; I5
	vmovapd	[dstreg+e4+e2], ymm1		;; Save new R11
	vmovapd	ymm1, [srcreg+d4+d1+32]		;; I6
	vmovapd	[dstreg+e2+e1], ymm6		;; Save new R4
	vshufpd	ymm6, ymm4, ymm1, 15		;; Shuffle I5 and I6 to create I5/I6 hi
	vshufpd	ymm4, ymm4, ymm1, 0		;; Shuffle I5 and I6 to create I5/I6 low

	vmovapd	ymm1, [srcreg+d4+d2+32]		;; I7
	vmovapd	[dstreg+e4+e2+e1], ymm2		;; Save new R12
	vmovapd	ymm2, [srcreg+d4+d2+d1+32]	;; I8
	vmovapd	[dstreg+32], ymm7		;; Save first R8
	vshufpd	ymm7, ymm1, ymm2, 15		;; Shuffle I7 and I8 to create I7/I8 hi
	vshufpd	ymm1, ymm1, ymm2, 0		;; Shuffle I7 and I8 to create I7/I8 low

	vperm2f128 ymm2, ymm6, ymm7, 32		;; Shuffle I5/I6 hi and I7/I8 hi (first R14)
	vperm2f128 ymm6, ymm6, ymm7, 49		;; Shuffle I5/I6 hi and I7/I8 hi (first R16)

	vperm2f128 ymm7, ymm4, ymm1, 32		;; Shuffle I5/I6 low and I7/I8 low (first R13)
	vperm2f128 ymm4, ymm4, ymm1, 49		;; Shuffle I5/I6 low and I7/I8 low (first R15)

	L1prefetch srcreg+d2+L1pd, L1pt

	vaddpd	ymm1, ymm3, ymm2		;; R6 + R14 (new R6)
	vsubpd	ymm3, ymm3, ymm2		;; R6 - R14 (new R14)

	vaddpd	ymm2, ymm0, ymm7		;; R5 + R13 (new R5)
	vsubpd	ymm0, ymm0, ymm7		;; R5 - R13 (new R13)

	L1prefetch srcreg+d2+d1+L1pd, L1pt

	vaddpd	ymm7, ymm5, ymm4		;; R7 + R15 (new R7)
	vsubpd	ymm5, ymm5, ymm4		;; R7 - R15 (new R15)

	vmovapd	ymm4, [dstreg+32]		;; Reload first R8
	vmovapd	[dstreg+e4+e1+32], ymm3		;; Save new R14
	vaddpd	ymm3, ymm4, ymm6		;; R8 + R16 (new R8)
	vsubpd	ymm4, ymm4, ymm6		;; R8 - R16 (new R16)

	;; Even levels 2

	vmovapd	ymm6, [dstreg+e2+e1]		;; Reload new R4
	vmovapd	[dstreg+e4+32], ymm0		;; Save new R13
	vaddpd	ymm0, ymm6, ymm3		;; R4 + R8 (newer R4)
	vsubpd	ymm6, ymm6, ymm3		;; R4 - R8 (newer R8)

	vmovapd	ymm3, [dstreg+e1]		;; Reload new R2
	vmovapd	[dstreg+e4+e2+32], ymm5		;; Save new R15
	vaddpd	ymm5, ymm3, ymm1		;; R2 + R6 (newer R2)
	vsubpd	ymm3, ymm3, ymm1		;; R2 - R6 (newer R6)

	;; Even level 3

	L1prefetch srcreg+d4+L1pd, L1pt

	vaddpd	ymm1, ymm5, ymm0		;; R2 + R4 (newest R2)
	vsubpd	ymm5, ymm5, ymm0		;; R2 - R4 (newest R4)

						;; R6/R8 morphs into newer R6/I6

	;; Premultipliers for even level 4

						;; mul R6/I6 by w^2 = .707 + .707i
	vsubpd	ymm0, ymm3, ymm6		;; R6 = R6 - I6
	vaddpd	ymm3, ymm3, ymm6		;; I6 = R6 + I6
	vmulpd	ymm0, ymm0, YMM_SQRTHALF	;; R6 = R6 * SQRTHALF (newest R6)
	vmulpd	ymm3, ymm3, YMM_SQRTHALF	;; I6 = I6 * SQRTHALF (newest I6)

	;; Odd levels 2

	vmovapd	ymm6, [dstreg]			;; Reload new R1
	vmovapd	[dstreg+e4+e2+e1+32], ymm4	;; Save new R16
	vsubpd	ymm4, ymm6, ymm2		;; R1 - R5 (newer R5)
	vaddpd	ymm6, ymm6, ymm2		;; R1 + R5 (newer R1)

	vmovapd	ymm2, [dstreg+e2]		;; Reload new R3
	vmovapd	[dstreg+e2+e1], ymm5		;; Save newest R4
	vaddpd	ymm5, ymm2, ymm7		;; R3 + R7 (newer R3)
	vsubpd	ymm2, ymm2, ymm7		;; R3 - R7 (newer R7)

	;; Odd level 3

	L1prefetch srcreg+d4+d1+L1pd, L1pt

 	vaddpd	ymm7, ymm6, ymm5		;; R1 + R3 (newest R1)
	vsubpd	ymm6, ymm6, ymm5		;; R1 - R3 (newest R3)

						;; R5/R7 morphs into newest R5/I5

	;; Last level

						;; R1/R2 becomes final R1 and final R2

						;; R3/R4 morphs into R3/I3

	vmovapd	[dstreg+32], ymm1		;; Save R2
	vmovapd	ymm1, [screg1+128+32]		;; cosine/sine for w^4 (8-complex w^2)
	vmulpd	ymm5, ymm6, ymm1		;; A3 = R3 * cosine/sine
	vmovapd	[dstreg], ymm7			;; Save R1
	vmovapd	ymm7, [dstreg+e2+e1]		;; Reload newest R4 which morped into I3
	vsubpd	ymm5, ymm5, ymm7		;; A3 = A3 - I3
	vmulpd	ymm7, ymm7, ymm1		;; B3 = I3 * cosine/sine
	vaddpd	ymm7, ymm7, ymm6		;; B3 = B3 + R3
	vmovapd	ymm1, [screg1+128]		;; sine for w^4 (8-complex w^2)
	vmulpd	ymm5, ymm5, ymm1		;; A3 = A3 * sine (final R3)
	vmulpd	ymm7, ymm7, ymm1		;; B3 = B3 * sine (final I3)

	vsubpd	ymm1, ymm4, ymm0		;; R5 - R6 (final R6)
	vaddpd	ymm4, ymm4, ymm0		;; R5 + R6 (final R5)

	L1prefetch srcreg+d4+d2+L1pd, L1pt

	vsubpd	ymm0, ymm2, ymm3		;; I5 - I6 (final I6)
	vaddpd	ymm2, ymm2, ymm3		;; I5 + I6 (final I5)

	vmovapd	ymm3, [screg1+64+32]		;; cosine/sine for w^2 (8-complex w^1)
	vmulpd	ymm6, ymm4, ymm3		;; A5 = R5 * cosine/sine
	vsubpd	ymm6, ymm6, ymm2		;; A5 = A5 - I5
	vmulpd	ymm2, ymm2, ymm3		;; B5 = I5 * cosine/sine
	vaddpd	ymm2, ymm2, ymm4		;; B5 = B5 + R5

	vmovapd	ymm3, [screg1+320+32]		;; cosine/sine for w^10 (8-complex w^5)
	vmulpd	ymm4, ymm1, ymm3		;; A6 = R6 * cosine/sine
	vsubpd	ymm4, ymm4, ymm0		;; A6 = A6 - I6
	vmulpd	ymm0, ymm0, ymm3		;; B6 = I6 * cosine/sine
	vaddpd	ymm0, ymm0, ymm1		;; B6 = B6 + R6

	vmovapd	ymm3, [screg1+64]		;; sine for w^2 (8-complex w^1)
	vmulpd	ymm6, ymm6, ymm3		;; A5 = A5 * sine (final R5)
	vmulpd	ymm2, ymm2, ymm3		;; B5 = B5 * sine (final I5)
	vmovapd	ymm3, [screg1+320]		;; sine for w^10 (8-complex w^5)
	vmulpd	ymm4, ymm4, ymm3		;; A6 = A6 * sine (final R6)
	vmulpd	ymm0, ymm0, ymm3		;; B6 = B6 * sine (final I6)

	;; Odd levels 2

						;; R9/R13 morphs into newer R9/I9
						;; R11/R15 morphs into newer R11/I11

	;; Even levels 2

						;; R10/R14 morphs into newer R10/I10
						;; R12/R16 morphs into newer R12/I12

	;; Premultipliers for even level 3

						;; mul R10/I10 by w^1 = .924 + .383i
	vmovapd ymm1, [dstreg+e4+e1]		;; Reload new R10
	vmovapd	ymm3, YMM_P924
	vmovapd	[dstreg+e1], ymm5		;; Save R3
	vmulpd	ymm5, ymm1, ymm3		;; R10 * .924
	vmovapd	[dstreg+e1+32], ymm7		;; Save I3
	vmovapd	ymm7, YMM_P383
	vmulpd	ymm1, ymm1, ymm7		;; R10 * .383
	vmovapd	[dstreg+e2+e1], ymm4		;; Save R6
	vmovapd ymm4, [dstreg+e4+e1+32]		;; Reload new R14 which morped into I10
	vmovapd	[dstreg+e2], ymm6		;; Save R5
	vmulpd	ymm6, ymm4, ymm7		;; I10 * .383
	vmulpd	ymm4, ymm4, ymm3		;; I10 * .924
	vsubpd	ymm5, ymm5, ymm6		;; Twiddled R10
	vaddpd	ymm1, ymm1, ymm4		;; Twiddled I10

						;; mul R12/I12 by w^3 = .383 + .924i
	vmovapd ymm6, [dstreg+e4+e2+e1]		;; Reload new R12
	vmulpd	ymm4, ymm6, ymm7		;; R12 * .383
	vmulpd	ymm6, ymm6, ymm3		;; R12 * .924
	vmovapd	[dstreg+e2+e1+32], ymm0		;; Save I6
	vmovapd ymm0, [dstreg+e4+e2+e1+32]	;; Reload new R16 which morped into I12
	vmulpd	ymm3, ymm0, ymm3		;; I12 * .924
	vmulpd	ymm0, ymm0, ymm7		;; I12 * .383
	vsubpd	ymm4, ymm4, ymm3		;; Twiddled R12
	vaddpd	ymm6, ymm6, ymm0		;; Twiddled I12

	;; Even level 3

	vaddpd	ymm7, ymm5, ymm4		;; R10 + R12 (newest R10)
	vsubpd	ymm5, ymm5, ymm4		;; R10 - R12 (newest R12)

	L1prefetch srcreg+d4+d2+d1+L1pd, L1pt

	vaddpd	ymm3, ymm1, ymm6		;; I10 + I12 (newest I10)
	vsubpd	ymm1, ymm1, ymm6		;; I10 - I12 (newest I12)

	;; Premultipliers for odd level 3

						;; mul R11/I11 by SQRTHALF + i*SQRTHALF
	vmovapd	ymm0, [dstreg+e4+e2]		;; Reload new R11
	vmovapd ymm4, [dstreg+e4+e2+32]		;; Reload new R15 which morped into I11
	vsubpd	ymm6, ymm0, ymm4		;; R11 = R11 - I11
	vaddpd	ymm0, ymm0, ymm4		;; I11 = R11 + I11
	vmulpd	ymm6, ymm6, YMM_SQRTHALF	;; R11 = R11 * SQRTHALF
	vmulpd	ymm0, ymm0, YMM_SQRTHALF	;; I11 = I11 * SQRTHALF

	;; Odd level 3

	vmovapd	ymm4, [dstreg+e4]		;; Reload new R9
	vmovapd	[dstreg+e2+32], ymm2		;; Save I5
	vaddpd	ymm2, ymm4, ymm6		;; R9 + R11 (newest R9)
	vsubpd	ymm4, ymm4, ymm6		;; R9 - R11 (newest R11)

	vmovapd ymm6, [dstreg+e4+32]		;; Reload new R13 which morped into I9
	vmovapd	[dstreg+e4+e2+e1+32], ymm1	;; Save newest I12
	vaddpd	ymm1, ymm6, ymm0		;; I9 + I11 (newest I9)
	vsubpd	ymm6, ymm6, ymm0		;; I9 - I11 (newest I11)

	;; Last level

	vsubpd	ymm0, ymm2, ymm7		;; R9 - R10 (final R10)
	vaddpd	ymm2, ymm2, ymm7		;; R9 + R10 (final R9)

	vsubpd	ymm7, ymm1, ymm3		;; I9 - I10 (final I10)
	vaddpd	ymm1, ymm1, ymm3		;; I9 + I10 (final I9)

	vmovapd	ymm3, [screg2+0+32]		;; cosine/sine for w^1
	vmovapd	[dstreg+e4+e2+e1], ymm5		;; Save newest R12
	vmulpd	ymm5, ymm2, ymm3		;; A9 = R9 * cosine/sine
	vsubpd	ymm5, ymm5, ymm1		;; A9 = A9 - I9
	vmulpd	ymm1, ymm1, ymm3		;; B9 = I9 * cosine/sine
	vaddpd	ymm1, ymm1, ymm2		;; B9 = B9 + R9
	vmovapd	ymm3, [screg2+0]		;; sine for w^1
	vmulpd	ymm5, ymm5, ymm3		;; A9 = A9 * sine (final R9)
	vmulpd	ymm1, ymm1, ymm3		;; B9 = B9 * sine (final I9)

	vmovapd	ymm3, [screg2+128+32]		;; cosine/sine for w^9
	vmulpd	ymm2, ymm0, ymm3		;; A10 = R10 * cosine/sine
	vsubpd	ymm2, ymm2, ymm7		;; A10 = A10 - I10
	vmulpd	ymm7, ymm7, ymm3		;; B10 = I10 * cosine/sine
	vaddpd	ymm7, ymm7, ymm0		;; B10 = B10 + R10
	vmovapd	ymm3, [screg2+128]		;; sine for w^9
	vmulpd	ymm2, ymm2, ymm3		;; A10 = A10 * sine (final R10)
	vmulpd	ymm7, ymm7, ymm3		;; B10 = B10 * sine (final I10)

	vmovapd	ymm0, [dstreg+e4+e2+e1+32]	;; Reload newest I12
	vsubpd	ymm3, ymm4, ymm0		;; R11 - I12 (final R11)
	vaddpd	ymm4, ymm4, ymm0		;; R11 + I12 (final R12)

	vmovapd	ymm0, [dstreg+e4+e2+e1]		;; Reload newest R12
	vmovapd	[dstreg+e4], ymm5		;; Save R9
	vaddpd	ymm5, ymm6, ymm0		;; I11 + R12 (final I11)
	vsubpd	ymm6, ymm6, ymm0		;; I11 - R12 (final I12)

	vmovapd	ymm0, [screg2+64+32]		;; cosine/sine for w^5
	vmovapd	[dstreg+e4+32], ymm1		;; Save I9
	vmulpd	ymm1, ymm3, ymm0		;; A11 = R11 * cosine/sine
	vsubpd	ymm1, ymm1, ymm5		;; A11 = A11 - I11
	vmulpd	ymm5, ymm5, ymm0		;; B11 = I11 * cosine/sine
	vaddpd	ymm5, ymm5, ymm3		;; B11 = B11 + R11

	vmovapd	ymm0, [screg2+192+32]		;; cosine/sine for w^13
	vmulpd	ymm3, ymm4, ymm0		;; A12 = R12 * cosine/sine
	vsubpd	ymm3, ymm3, ymm6		;; A12 = A12 - I12
	vmulpd	ymm6, ymm6, ymm0		;; B12 = I12 * cosine/sine
	vaddpd	ymm6, ymm6, ymm4		;; B12 = B12 + R12

	vmovapd	ymm0, [screg2+64]		;; sine for w^5
	vmulpd	ymm1, ymm1, ymm0		;; A11 = A11 * sine (final R11)
	vmulpd	ymm5, ymm5, ymm0		;; B11 = B11 * sine (final I11)

	vmovapd	ymm0, [screg2+192]		;; sine for w^13
	vmulpd	ymm3, ymm3, ymm0		;; A12 = A12 * sine (final R12)
	vmulpd	ymm6, ymm6, ymm0		;; B12 = B12 * sine (final I12)

	vmovapd	[dstreg+e4+e1], ymm2		;; Save R10
	vmovapd	[dstreg+e4+e1+32], ymm7		;; Save I10
	vmovapd	[dstreg+e4+e2], ymm1		;; Save R11
	vmovapd	[dstreg+e4+e2+32], ymm5		;; Save I11
	vmovapd	[dstreg+e4+e2+e1], ymm3		;; Save R12
	vmovapd	[dstreg+e4+e2+e1+32], ymm6	;; Save I12

	bump	srcreg, srcinc
	bump	dstreg, dstinc
	bump	screg1, scinc1
	bump	screg2, scinc2
	ENDM


;;
;; ************************************* sixteen-reals-unfft8 variants ******************************************
;;

;; Used in last levels of pass 1 (split premultiplier and delay cases).  Swizzling.
;; In this particular case, we know that the sin/cos multipliers for R1/I1 are one.  Thus
;; we can get by with only 7 sin/cos multiplies.

yr8_sg8cl_2sc_sixteen_reals_unfft8 MACRO srcreg,srcinc,d1,d2,d4,dstreg,dstinc,e1,e2,e4,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd

	vmovapd	ymm0, [screg1+64+32]		;; cosine/sine for w^2 (8-complex w^1)
	vmovapd	ymm1, [srcreg+d2]		;; R5
	vmulpd	ymm2, ymm1, ymm0		;; A5 = R5 * cosine/sine
	vmovapd	ymm3, [srcreg+d2+32]		;; I5
	vaddpd	ymm2, ymm2, ymm3		;; A5 = A5 + I5
	vmulpd	ymm3, ymm3, ymm0		;; B5 = I5 * cosine/sine
	vsubpd	ymm3, ymm3, ymm1		;; B5 = B5 - R5

	vmovapd	ymm4, [screg1+320+32]		;; cosine/sine for w^10 (8-complex w^5)
	vmovapd	ymm5, [srcreg+d2+d1]		;; R6
	vmulpd	ymm6, ymm5, ymm4		;; A6 = R6 * cosine/sine
	vmovapd	ymm7, [srcreg+d2+d1+32]		;; I6
	vaddpd	ymm6, ymm6, ymm7		;; A6 = A6 + I6
	vmulpd	ymm7, ymm7, ymm4		;; B6 = I6 * cosine/sine
	vsubpd	ymm7, ymm7, ymm5		;; B6 = B6 - R6

	vmovapd	ymm0, [screg1+128+32]		;; cosine/sine for w^4 (8-complex w^2)
	vmovapd	ymm1, [srcreg+d1]		;; R3
	vmulpd	ymm4, ymm1, ymm0		;; A3 = R3 * cosine/sine
	vmovapd	ymm5, [srcreg+d1+32]		;; I3
	vaddpd	ymm4, ymm4, ymm5		;; A3 = A3 + I3
	vmulpd	ymm5, ymm5, ymm0		;; B3 = I3 * cosine/sine
	vsubpd	ymm5, ymm5, ymm1		;; B3 = B3 - R3

	vmovapd ymm0, [screg1+64]		;; sine for w^2 (8-complex w^1)
	vmulpd	ymm2, ymm2, ymm0		;; A5 = A5 * sine (first R5)
	vmulpd	ymm3, ymm3, ymm0		;; B5 = B5 * sine (first I5)

	vmovapd ymm0, [screg1+320]		;; sine for w^10 (8-complex w^5)
	vmulpd	ymm6, ymm6, ymm0		;; A6 = A6 * sine (first R6)
	vmulpd	ymm7, ymm7, ymm0		;; B6 = B6 * sine (first I6)

	vmovapd ymm0, [screg1+128]		;; sine for w^4 (8-complex w^2)
	vmulpd	ymm4, ymm4, ymm0		;; A3 = A3 * sine (first R3)
	vmulpd	ymm5, ymm5, ymm0		;; B3 = B3 * sine (first I3)

						;; R3/I3 morphs into R3/R4

	vaddpd	ymm1, ymm2, ymm6		;; R5 + R6 (new R5)
	vsubpd	ymm2, ymm2, ymm6		;; R5 - R6 (new R6)

	L1prefetch srcreg+L1pd, L1pt

	vaddpd	ymm0, ymm3, ymm7		;; I5 + I6 (new I5)
	vsubpd	ymm3, ymm3, ymm7		;; I5 - I6 (new I6)

	vmovapd	ymm6, [srcreg]			;; R1
	vaddpd	ymm7, ymm6, ymm4		;; R1 + R3 (new R1)
	vsubpd	ymm6, ymm6, ymm4		;; R1 - R3 (new R3)

						;; mul R6/I6 by w^2 = .707 - .707i
	vaddpd	ymm4, ymm3, ymm2		;; R6 = I6 + R6
	vsubpd	ymm3, ymm3, ymm2		;; I6 = I6 - R6
	vmovapd	ymm2, YMM_SQRTHALF
	vmulpd	ymm4, ymm4, ymm2		;; R6 = R6 * SQRTHALF
	vmulpd	ymm3, ymm3, ymm2		;; I6 = I6 * SQRTHALF

						;; R5/I5 morphs into new R5/R7
						;; R6/I6 morph into new R6/R8

	vaddpd	ymm2, ymm7, ymm1		;; R1 + R5 (newer R1)
	vsubpd	ymm7, ymm7, ymm1		;; R1 - R5 (newer R5)

	vmovapd	ymm1, [srcreg+32]		;; R2
	vmovapd	[dstreg], ymm2			;; Save newer R1
	vaddpd	ymm2, ymm1, ymm5		;; R2 + R4 (new R2)
	vsubpd	ymm1, ymm1, ymm5		;; R2 - R4 (new R4)

	vaddpd	ymm5, ymm6, ymm0		;; R3 + R7 (newer R3)
	vsubpd	ymm6, ymm6, ymm0		;; R3 - R7 (newer R7)

	L1prefetch srcreg+d1+L1pd, L1pt

	vaddpd	ymm0, ymm2, ymm4		;; R2 + R6 (newer R2)
	vsubpd	ymm2, ymm2, ymm4		;; R2 - R6 (newer R6)

	vaddpd	ymm4, ymm1, ymm3		;; R4 + R8 (newer R4)
	vsubpd	ymm1, ymm1, ymm3		;; R4 - R8 (newer R8)

	vmovapd	ymm3, [screg2+0+32]		;; cosine/sine for w^1
	vmovapd	[dstreg+e4], ymm7		;; Save newer R5
	vmovapd	ymm7, [srcreg+d4]		;; R9
	vmovapd	[dstreg+e1], ymm5		;; Save newer R3
	vmulpd	ymm5, ymm7, ymm3		;; A9 = R9 * cosine/sine
	vmovapd	[dstreg+e4+e1], ymm6		;; Save newer R7
	vmovapd	ymm6, [srcreg+d4+32]		;; I9
	vaddpd	ymm5, ymm5, ymm6		;; A9 = A9 + I9
	vmulpd	ymm6, ymm6, ymm3		;; B9 = I9 * cosine/sine
	vsubpd	ymm6, ymm6, ymm7		;; B9 = B9 - R9

	vmovapd	ymm3, [screg2+128+32]		;; cosine/sine for w^9
	vmovapd	ymm7, [srcreg+d4+d1]		;; R10
	vmovapd	[dstreg+32], ymm0		;; Save newer R2
	vmulpd	ymm0, ymm7, ymm3		;; A10 = R10 * cosine/sine
	vmovapd	[dstreg+e4+32], ymm2		;; Save newer R6
	vmovapd	ymm2, [srcreg+d4+d1+32]		;; I10
	vaddpd	ymm0, ymm0, ymm2		;; A10 = A10 + I10
	vmulpd	ymm2, ymm2, ymm3		;; B10 = I10 * cosine/sine
	vsubpd	ymm2, ymm2, ymm7		;; B10 = B10 - R10

	vmovapd ymm3, [screg2+0]		;; sine for w^1
	vmulpd	ymm5, ymm5, ymm3		;; A9 = A9 * sine (first R9)
	vmulpd	ymm6, ymm6, ymm3		;; B9 = B9 * sine (first I9)

	vmovapd ymm7, [screg2+128]		;; sine for w^9
	vmulpd	ymm0, ymm0, ymm7		;; A10 = A10 * sine (first R10)
	vmulpd	ymm2, ymm2, ymm7		;; B10 = B10 * sine (first I10)

	vaddpd	ymm3, ymm5, ymm0		;; R9 + R10 (new R9)
	vsubpd	ymm5, ymm5, ymm0		;; R9 - R10 (new R10)

	L1prefetch srcreg+d2+L1pd, L1pt

	vaddpd	ymm7, ymm6, ymm2		;; I9 + I10 (new I9)
	vsubpd	ymm6, ymm6, ymm2		;; I9 - I10 (new I10)

	vmovapd	ymm0, [screg2+64+32]		;; cosine/sine for w^5
	vmovapd	ymm2, [srcreg+d4+d2]		;; R11
	vmovapd	[dstreg+e1+32], ymm4		;; Save newer R4
	vmulpd	ymm4, ymm2, ymm0		;; A11 = R11 * cosine/sine
	vmovapd	[dstreg+e4+e1+32], ymm1		;; Save newer R8
	vmovapd	ymm1, [srcreg+d4+d2+32]		;; I11
	vaddpd	ymm4, ymm4, ymm1		;; A11 = A11 + I11
	vmulpd	ymm1, ymm1, ymm0		;; B11 = I11 * cosine/sine
	vsubpd	ymm1, ymm1, ymm2		;; B11 = B11 - R11

	vmovapd	ymm0, [screg2+192+32]		;; cosine/sine for w^13
	vmovapd	ymm2, [srcreg+d4+d2+d1]		;; R12
	vmovapd	[dstreg+e2+e1], ymm5		;; Save new R10
	vmulpd	ymm5, ymm2, ymm0		;; A12 = R12 * cosine/sine
	vmovapd	[dstreg+e2+e1+32], ymm6		;; Save new I10
	vmovapd	ymm6, [srcreg+d4+d2+d1+32]	;; I12
	vaddpd	ymm5, ymm5, ymm6		;; A12 = A12 + I12
	vmulpd	ymm6, ymm6, ymm0		;; B12 = I12 * cosine/sine
	vsubpd	ymm6, ymm6, ymm2		;; B12 = B12 - R12

	vmovapd ymm0, [screg2+64]		;; sine for w^5
	vmulpd	ymm4, ymm4, ymm0		;; A11 = A11 * sine (first R11)
	vmulpd	ymm1, ymm1, ymm0		;; B11 = B11 * sine (first I11)

	vmovapd ymm2, [screg2+192]		;; sine for w^13
	vmulpd	ymm5, ymm5, ymm2		;; A12 = A12 * sine (first R12)
	vmulpd	ymm6, ymm6, ymm2		;; B12 = B12 * sine (first I12)

	vaddpd	ymm2, ymm5, ymm4		;; R12 + R11 (new R11)
	vsubpd	ymm5, ymm5, ymm4		;; R12 - R11 (new I12)

	L1prefetch srcreg+d2+d1+L1pd, L1pt

	vaddpd	ymm0, ymm1, ymm6		;; I11 + I12 (new I11)
	vsubpd	ymm1, ymm1, ymm6		;; I11 - I12 (new R12)

	vaddpd	ymm4, ymm3, ymm2		;; R9 + R11 (newer R9)
	vsubpd	ymm3, ymm3, ymm2		;; R9 - R11 (newer R11)

	vaddpd	ymm6, ymm7, ymm0		;; I9 + I11 (newer I9)
	vsubpd	ymm7, ymm7, ymm0		;; I9 - I11 (newer I11)

						;; mul R11/I11 by SQRTHALF - i*SQRTHALF
	vaddpd	ymm2, ymm7, ymm3		;; R11 = I11 + R11
	vsubpd	ymm7, ymm7, ymm3		;; I11 = I11 - R11
	vmovapd ymm0, YMM_SQRTHALF
	vmulpd	ymm2, ymm2, ymm0		;; R11 = R11 * SQRTHALF
	vmulpd	ymm7, ymm7, ymm0		;; I11 = I11 * SQRTHALF

						;; R9/I9 morphs into newer R9/R13
						;; R11/I11 morphs into newer R11/R15

	vmovapd	ymm0, [dstreg+e2+e1]		;; Reload new R10
	vaddpd	ymm3, ymm0, ymm1		;; R10 + R12 (newer R10)
	vsubpd	ymm0, ymm0, ymm1		;; R10 - R12 (newer R12)

	vmovapd	ymm1, [dstreg+e2+e1+32]		;; Reload new I10
	vmovapd	[dstreg+e2], ymm4		;; Save newer R9
	vaddpd	ymm4, ymm1, ymm5		;; I10 + I12 (newer I10)
	vsubpd	ymm1, ymm1, ymm5		;; I10 - I12 (newer I12)


						;; mul R10/I10 by w^1 = .924 - .383i
	vmovapd ymm5, YMM_P924
	vmovapd	[dstreg+e4+e2], ymm6		;; Save newer R13
	vmulpd	ymm6, ymm3, ymm5		;; R10 * .924
	vmovapd	[dstreg+e2+e1], ymm2		;; Save newer R11
	vmovapd ymm2, YMM_P383
	vmovapd	[dstreg+e4+e2+e1], ymm7		;; Save newer R15
	vmulpd	ymm7, ymm4, ymm2		;; I10 * .383
	vmulpd	ymm3, ymm3, ymm2		;; R10 * .383
	vmulpd	ymm4, ymm4, ymm5		;; I10 * .924
	vaddpd	ymm6, ymm6, ymm7		;; Twiddled R10
	vsubpd	ymm3, ymm4, ymm3		;; Twiddled I10

	L1prefetch srcreg+d4+L1pd, L1pt

						;; mul R12/I12 by w^3 = .383 - .924i
	vmulpd	ymm7, ymm0, ymm2		;; R12 * .383
	vmulpd	ymm4, ymm1, ymm5		;; I12 * .924
	vmulpd	ymm0, ymm0, ymm5		;; R12 * .924
	vmulpd	ymm1, ymm1, ymm2		;; I12 * .383
	vaddpd	ymm7, ymm7, ymm4		;; Twiddled R12
	vsubpd	ymm0, ymm1, ymm0		;; Twiddled I12

						;; R10/I10 morphs into newer R10/R14
						;; R12/I12 morphs into newer R12/R16

	;; Do last level, shuffle and store

	vmovapd	ymm5, [dstreg]			;; Reload newer R1
	vmovapd	ymm2, [dstreg+e2]		;; Reload newer R9
	vaddpd	ymm4, ymm5, ymm2		;; R1 + R9 (last R1)
	vsubpd	ymm5, ymm5, ymm2		;; R1 - R9 (last R9)

	vmovapd	ymm1, [dstreg+32]		;; Reload newer R2
	vaddpd	ymm2, ymm1, ymm6		;; R2 + R10 (last R2)
	vsubpd	ymm1, ymm1, ymm6		;; R2 - R10 (last R10)

	vmovapd	ymm6, [dstreg+e1]		;; Reload newer R3
	vmovapd	[dstreg+e4+e2+32], ymm3		;; Save newer R14
	vmovapd	ymm3, [dstreg+e2+e1]		;; Reload newer R11
	vmovapd	[dstreg+e4+e2+e1+32], ymm0	;; Save newer R16
	vaddpd	ymm0, ymm6, ymm3		;; R3 + R11 (last R3)
	vsubpd	ymm6, ymm6, ymm3		;; R3 - R11 (last R11)

	vmovapd	ymm3, [dstreg+e1+32]		;; Reload newer R4
	vmovapd	[dstreg+32], ymm5		;; Save last R9
	vaddpd	ymm5, ymm3, ymm7		;; R4 + R12 (last R4)
	vsubpd	ymm3, ymm3, ymm7		;; R4 - R12 (last R12)

	;; Swizzle using 4 shufpds and 4 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	L1prefetch srcreg+d4+d1+L1pd, L1pt

	vshufpd	ymm7, ymm4, ymm2, 0		;; Shuffle R1 and R2 to create R1/R2 low
	vshufpd	ymm4, ymm4, ymm2, 15		;; Shuffle R1 and R2 to create R1/R2 hi

	vshufpd	ymm2, ymm0, ymm5, 0		;; Shuffle R3 and R4 to create R3/R4 low
	vshufpd	ymm0, ymm0, ymm5, 15		;; Shuffle R3 and R4 to create R3/R4 hi

	vperm2f128 ymm5, ymm7, ymm2, 32		;; Shuffle R1/R2 low and R3/R4 low (final R1)
	vperm2f128 ymm7, ymm7, ymm2, 49		;; Shuffle R1/R2 low and R3/R4 low (final R3)

	vperm2f128 ymm2, ymm4, ymm0, 32		;; Shuffle R1/R2 hi and R3/R4 hi (final R2)
	vperm2f128 ymm4, ymm4, ymm0, 49		;; Shuffle R1/R2 hi and R3/R4 hi (final R4)

	;; Swizzle using 4 shufpds and 4 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm0, [dstreg+32]		;; Reload last R9
	vmovapd	[dstreg], ymm5			;; Save R1

	vshufpd	ymm5, ymm0, ymm1, 0		;; Shuffle R9 and R10 to create R9/R10 low
	vshufpd	ymm0, ymm0, ymm1, 15		;; Shuffle R9 and R10 to create R9/R10 hi

	vshufpd	ymm1, ymm6, ymm3, 0		;; Shuffle R11 and R12 to create R11/R12 low
	vshufpd	ymm6, ymm6, ymm3, 15		;; Shuffle R11 and R12 to create R11/R12 hi

	L1prefetch srcreg+d4+d2+L1pd, L1pt

	vperm2f128 ymm3, ymm5, ymm1, 32		;; Shuffle R9/R10 low and R11/R12 low (final R9)
	vperm2f128 ymm5, ymm5, ymm1, 49		;; Shuffle R9/R10 low and R11/R12 low (final R11)

	vperm2f128 ymm1, ymm0, ymm6, 32		;; Shuffle R9/R10 hi and R11/R12 hi (final R10)
	vperm2f128 ymm0, ymm0, ymm6, 49		;; Shuffle R9/R10 hi and R11/R12 hi (final R12)

	vmovapd	ymm6, [dstreg+e4]		;; Reload newer R5
	vmovapd	[dstreg+e2], ymm7		;; Save R3
	vmovapd	ymm7, [dstreg+e4+e2]		;; Reload newer R13
	vmovapd	[dstreg+e1], ymm2		;; Save R2
	vaddpd	ymm2, ymm6, ymm7		;; R5 + R13 (last R5)
	vsubpd	ymm6, ymm6, ymm7		;; R5 - R13 (last R13)

	vmovapd	ymm7, [dstreg+e4+32]		;; Reload newer R6
	vmovapd	[dstreg+e2+e1], ymm4		;; Save R4
	vmovapd	ymm4, [dstreg+e4+e2+32]		;; Reload newer R14
	vmovapd	[dstreg+32], ymm3		;; Save R9
	vaddpd	ymm3, ymm7, ymm4		;; R6 + R14 (last R6)
	vsubpd	ymm7, ymm7, ymm4		;; R6 - R14 (last R14)

	vmovapd	ymm4, [dstreg+e4+e1]		;; Reload newer R7
	vmovapd	[dstreg+e2+32], ymm5		;; Save R11
	vmovapd	ymm5, [dstreg+e4+e2+e1]		;; Reload newer R15
	vmovapd	[dstreg+e1+32], ymm1		;; Save R10
	vaddpd	ymm1, ymm4, ymm5		;; R7 + R15 (last R7)
	vsubpd	ymm4, ymm4, ymm5		;; R7 - R15 (last R15)

	vmovapd	ymm5, [dstreg+e4+e1+32]		;; Reload newer R8
	vmovapd	[dstreg+e2+e1+32], ymm0		;; Save R12
	vmovapd	ymm0, [dstreg+e4+e2+e1+32]	;; Reload newer R16
	vmovapd	[dstreg+e4], ymm6		;; Save last R13
	vaddpd	ymm6, ymm5, ymm0		;; R8 + R16 (last R8)
	vsubpd	ymm5, ymm5, ymm0		;; R8 - R16 (last R16)

	;; Swizzle using 4 shufpds and 4 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vshufpd	ymm0, ymm2, ymm3, 0		;; Shuffle R5 and R6 to create R5/R6 low
	vshufpd	ymm2, ymm2, ymm3, 15		;; Shuffle R5 and R6 to create R5/R6 hi

	vshufpd	ymm3, ymm1, ymm6, 0		;; Shuffle R7 and R8 to create R7/R8 low
	vshufpd	ymm1, ymm1, ymm6, 15		;; Shuffle R7 and R8 to create R7/R8 hi

	L1prefetch srcreg+d4+d2+d1+L1pd, L1pt

	vperm2f128 ymm6, ymm0, ymm3, 32		;; Shuffle R5/R6 low and R7/R8 low (final R5)
	vperm2f128 ymm0, ymm0, ymm3, 49		;; Shuffle R5/R6 low and R7/R8 low (final R7)

	vperm2f128 ymm3, ymm2, ymm1, 32		;; Shuffle R5/R6 hi and R7/R8 hi (final R6)
	vperm2f128 ymm2, ymm2, ymm1, 49		;; Shuffle R5/R6 hi and R7/R8 hi (final R8)

	;; Swizzle using 4 shufpds and 4 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm1, [dstreg+e4]		;; Reload last R13
	vmovapd	[dstreg+e4], ymm6		;; Save R5
	vshufpd	ymm6, ymm1, ymm7, 0		;; Shuffle R13 and R14 to create R13/R14 low
	vshufpd	ymm1, ymm1, ymm7, 15		;; Shuffle R13 and R14 to create R13/R14 hi

	vshufpd	ymm7, ymm4, ymm5, 0		;; Shuffle R15 and R16 to create R15/R16 low
	vshufpd	ymm4, ymm4, ymm5, 15		;; Shuffle R15 and R16 to create R15/R16 hi

	vperm2f128 ymm5, ymm6, ymm7, 32		;; Shuffle R13/R14 low and R15/R16 low (final R13)
	vperm2f128 ymm6, ymm6, ymm7, 49		;; Shuffle R13/R14 low and R15/R16 low (final R15)

	vperm2f128 ymm7, ymm1, ymm4, 32		;; Shuffle R13/R14 hi and R15/R16 hi (final R14)
	vperm2f128 ymm1, ymm1, ymm4, 49		;; Shuffle R13/R14 hi and R15/R16 hi (final R16)

	vmovapd	[dstreg+e4+e2], ymm0		;; Save R7
	vmovapd	[dstreg+e4+e1], ymm3		;; Save R6
	vmovapd	[dstreg+e4+e2+e1], ymm2		;; Save R8
	vmovapd	[dstreg+e4+32], ymm5		;; Save R13
	vmovapd	[dstreg+e4+e2+32], ymm6		;; Save R15
	vmovapd	[dstreg+e4+e1+32], ymm7		;; Save R14
	vmovapd	[dstreg+e4+e2+e1+32], ymm1	;; Save R16

	bump	srcreg, srcinc
	bump	dstreg, dstinc
	bump	screg1, scinc1
	bump	screg2, scinc2
	ENDM


;;
;; ********************************* sixteen-reals-eight-complex-fft-with-square variants ***************************************
;;

;; Macro to do a sixteen_reals_fft and three eight_complex_fft in the final levels.
;; The sixteen-reals macro uses the lower double in a YMM register and the three eight-complex
;; use the upper 3 doubles in a YMM  register.  This isn't very efficient, but this macro is called only once.

yr8_8cl_sixteen_reals_eight_complex_fft_final MACRO srcreg,srcinc,d1,d2,d4
	;; Do the sixteen reals part 1
	yr8_16r_simple_fft_part1 srcreg,d1,d2,d4
	;; Do the three eight-complex part
	yr8_8c_simple_fft_part1 srcreg,d1,d2,d4
	yr8_8c_simple_fft_part2 srcreg,d1,d2,d4
	;; Do the sixteen reals part 2
	yr8_16r_simple_fft_part2 srcreg,d1,d2,d4
	bump	srcreg, srcinc
	ENDM

;; Do part 1 of the sixteen_reals_fft writing the results to YMM_TMP memory
;; R1,R2,R3,R4 are written to YMM_TMP1
;; R5,I5,R6,I6 are written to YMM_TMP2
;; R9,I9,R10,I10 are written to YMM_TMP3
;; R11,I11,R12,I12 are written to YMM_TMP4
yr8_16r_simple_fft_part1 MACRO srcreg,d1,d2,d4

	;; Odd levels 1 & 2

	vmovsd	xmm0, Q [srcreg]		;; R1
	vmovsd	xmm7, Q [srcreg+32]		;; R9
	vaddsd	xmm1, xmm0, xmm7		;; R1 + R9 (new R1)
	vsubsd	xmm0, xmm0, xmm7		;; R1 - R9 (new R9)

	vmovsd	xmm2, Q [srcreg+d4]		;; R5
	vmovsd	xmm7, Q [srcreg+d4+32]		;; R13
	vaddsd	xmm3, xmm2, xmm7		;; R5 + R13 (new R5)
	vsubsd	xmm2, xmm2, xmm7		;; R5 - R13 (new R13)

	vmovsd	xmm4, Q [srcreg+d2]		;; R3
	vmovsd	xmm7, Q [srcreg+d2+32]		;; R11
	vaddsd	xmm5, xmm4, xmm7		;; R3 + R11 (new R3)
	vsubsd	xmm4, xmm4, xmm7		;; R3 - R11 (new R11)

	vsubsd	xmm6, xmm1, xmm3		;; R1 - R5 (newer R5 & final R5)
	vaddsd	xmm1, xmm1, xmm3		;; R1 + R5 (newer R1)
	vmovsd	YMM_TMP2[0], xmm6		;; Save R5

	vmovsd	xmm7, Q [srcreg+d4+d2]		;; R7
	vmovsd	xmm6, Q [srcreg+d4+d2+32]	;; R15
	vaddsd	xmm3, xmm7, xmm6		;; R7 + R15 (new R7)
	vsubsd	xmm7, xmm7, xmm6		;; R7 - R15 (new R15)

	vsubsd	xmm6, xmm5, xmm3		;; R3 - R7 (newer R7 & final I5)
	vaddsd	xmm5, xmm5, xmm3		;; R3 + R7 (newer R3)
	vmovsd	YMM_TMP2[8], xmm6		;; Save I5

						;; R9/R13 morphs into newer R9/I9
						;; R11/R15 morphs into newer R11/I11

	;; Premultipliers for odd level 3

						;; mul R11/I11 by SQRTHALF + i*SQRTHALF
	vsubsd	xmm3, xmm4, xmm7		;; R11 = R11 - I11
	vaddsd	xmm4, xmm4, xmm7		;; I11 = R11 + I11
	vmulsd	xmm3, xmm3, YMM_SQRTHALF	;; R11 = R11 * SQRTHALF
	vmulsd	xmm4, xmm4, YMM_SQRTHALF	;; I11 = I11 * SQRTHALF

	;; Odd level 3

	vaddsd	xmm7, xmm1, xmm5		;; R1 + R3 (final R1)
	vsubsd	xmm1, xmm1, xmm5		;; R1 - R3 (final R3)
	vmovsd	YMM_TMP1[0], xmm7		;; Save R1
	vmovsd	YMM_TMP1[16], xmm1		;; Save R3

						;; R5/R7 morphed into final R5/I5

	vaddsd	xmm5, xmm0, xmm3		;; R9 + R11 (final R9)
	vsubsd	xmm0, xmm0, xmm3		;; R9 - R11 (final R11)
	vmovsd	YMM_TMP3[0], xmm5		;; Save R9
	vmovsd	YMM_TMP4[0], xmm0		;; Save R11

	vaddsd	xmm3, xmm2, xmm4		;; I9 + I11 (final I9)
	vsubsd	xmm2, xmm2, xmm4		;; I9 - I11 (final I11)
	vmovsd	YMM_TMP3[8], xmm3		;; Save I9
	vmovsd	YMM_TMP4[8], xmm2		;; Save I11

	;; Even levels 1 & 2

	vmovsd	xmm4, Q [srcreg+d2+d1]		;; R4
	vmovsd	xmm7, Q [srcreg+d2+d1+32]	;; R12
	vaddsd	xmm5, xmm4, xmm7		;; R4 + R12 (new R4)
	vsubsd	xmm4, xmm4, xmm7		;; R4 - R12 (new R12)

	vmovsd	xmm6, Q [srcreg+d4+d2+d1]	;; R8
	vmovsd	xmm3, Q [srcreg+d4+d2+d1+32]	;; R16
	vaddsd	xmm7, xmm6, xmm3		;; R8 + R16 (new R8)
	vsubsd	xmm6, xmm6, xmm3		;; R8 - R16 (new R16)

	vaddsd	xmm3, xmm5, xmm7		;; R4 + R8 (newer R4)
	vsubsd	xmm5, xmm5, xmm7		;; R4 - R8 (newer R8)
	vmovsd	YMM_TMP8, xmm3			;; Temporarily save newer R4

	vmovsd	xmm0, Q [srcreg+d1]		;; R2
	vmovsd	xmm7, Q [srcreg+d1+32]		;; R10
	vaddsd	xmm1, xmm0, xmm7		;; R2 + R10 (new R2)
	vsubsd	xmm0, xmm0, xmm7		;; R2 - R10 (new R10)

	vmovsd	xmm2, Q [srcreg+d4+d1]		;; R6
	vmovsd	xmm7, Q [srcreg+d4+d1+32]	;; R14
	vaddsd	xmm3, xmm2, xmm7		;; R6 + R14 (new R6)
	vsubsd	xmm2, xmm2, xmm7		;; R6 - R14 (new R14)

	vaddsd	xmm7, xmm1, xmm3		;; R2 + R6 (newer R2)
	vsubsd	xmm1, xmm1, xmm3		;; R2 - R6 (newer R6)

						;; R10/R14 morphs into newer R10/I10
						;; R12/R16 morphs into newer R12/I12

	;; Even level 3

	vaddsd	xmm3, xmm7, YMM_TMP8		;; R2 + R4 (final R2)
	vsubsd	xmm7, xmm7, YMM_TMP8		;; R2 - R4 (final R4)
	vmovsd	YMM_TMP1[8], xmm3		;; Save R2
	vmovsd	YMM_TMP1[24], xmm7		;; Save R4

	;; Premultipliers for even level 3

						;; mul R10/I10 by w^1 = .924 + .383i
	vmulsd	xmm3, xmm0, YMM_P924
	vmulsd	xmm7, xmm2, YMM_P383
	vsubsd	xmm3, xmm3, xmm7		;; Twiddled R10
	vmulsd	xmm0, xmm0, YMM_P383
	vmulsd	xmm2, xmm2, YMM_P924
	vaddsd	xmm0, xmm0, xmm2		;; Twiddled I10

						;; mul R12/I12 by w^3 = .383 + .924i
	vmulsd	xmm2, xmm4, YMM_P383
	vmulsd	xmm7, xmm6, YMM_P924
	vsubsd	xmm2, xmm2, xmm7		;; Twiddled R12
	vmulsd	xmm4, xmm4, YMM_P924
	vmulsd	xmm6, xmm6, YMM_P383
	vaddsd	xmm4, xmm4, xmm6		;; Twiddled I12

	;; More even level 3

						;; R6/R8 morph into newer R6/I6

	vaddsd	xmm6, xmm3, xmm2		;; R10 + R12 (final R10)
	vsubsd	xmm3, xmm3, xmm2		;; R10 - R12 (final R12)
	vmovsd	YMM_TMP3[16], xmm6		;; Save R10
	vmovsd	YMM_TMP4[16], xmm3		;; Save R12

	vaddsd	xmm2, xmm0, xmm4		;; I10 + I12 (final I10)
	vsubsd	xmm0, xmm0, xmm4		;; I10 - I12 (final I12)
	vmovsd	YMM_TMP3[24], xmm2		;; Save I10
	vmovsd	YMM_TMP4[24], xmm0		;; Save I12

	;; Premultipliers for even level 4

						;; mul R6/I6 by w^2 = .707 + .707i
	vsubsd	xmm0, xmm1, xmm5		;; R6 = R6 - I6
	vaddsd	xmm1, xmm1, xmm5		;; I6 = R6 + I6
	vmulsd	xmm0, xmm0, YMM_SQRTHALF	;; R6 = R6 * SQRTHALF (final R6)
	vmulsd	xmm1, xmm1, YMM_SQRTHALF	;; I6 = I6 * SQRTHALF (final I6)
	vmovsd	YMM_TMP2[16], xmm0		;; Save R6
	vmovsd	YMM_TMP2[24], xmm1		;; Save I6
	ENDM

yr8_16r_simple_fft_part2 MACRO srcreg,d1,d2,d4

	vmovsd	xmm0, YMM_TMP1[0]		;; R1
	vmovsd	xmm7, YMM_TMP1[8] 		;; R2
	vsubsd	xmm1, xmm0, xmm7		;; R1 - R2 (new R2)
	vaddsd	xmm0, xmm0, xmm7		;; R1 + R2 (new R1)

	vmovsd	xmm2, YMM_TMP1[16]		;; R3/R4 morphs into R3/I3
	vmovsd	xmm3, YMM_TMP1[24]

	vmovsd	Q [srcreg], xmm0		;; Save R1
	vmovsd	Q [srcreg+32], xmm1		;; Save R2
	vmovsd	Q [srcreg+d1], xmm2		;; Save R3
	vmovsd	Q [srcreg+d1+32], xmm3		;; Save I3

	vmovsd	xmm0, YMM_TMP2[0]		;; R5
	vmovsd	xmm7, YMM_TMP2[16] 		;; R6
	vsubsd	xmm1, xmm0, xmm7		;; R5 - R6 (new R6)
	vaddsd	xmm0, xmm0, xmm7		;; R5 + R6 (new R5)

	vmovsd	xmm2, YMM_TMP2[8]		;; I5
	vmovsd	xmm7, YMM_TMP2[24] 		;; I6
	vsubsd	xmm3, xmm2, xmm7		;; I5 - I6 (new I6)
	vaddsd	xmm2, xmm2, xmm7		;; I5 + I6 (new I5)

	vmovsd	Q [srcreg+d2+d1], xmm1		;; Save R6
	vmovsd	Q [srcreg+d2], xmm0		;; Save R5
	vmovsd	Q [srcreg+d2+d1+32], xmm3	;; Save I6
	vmovsd	Q [srcreg+d2+32], xmm2		;; Save I5

	vmovsd	xmm0, YMM_TMP3[0]		;; R9
	vmovsd	xmm7, YMM_TMP3[16] 		;; R10
	vsubsd	xmm1, xmm0, xmm7		;; R9 - R10 (new R10)
	vaddsd	xmm0, xmm0, xmm7		;; R9 + R10 (new R9)

	vmovsd	xmm2, YMM_TMP3[8]		;; I9
	vmovsd	xmm7, YMM_TMP3[24] 		;; I10
	vsubsd	xmm3, xmm2, xmm7		;; I9 - I10 (new I10)
	vaddsd	xmm2, xmm2, xmm7		;; I9 + I10 (new I9)

	vmovsd	Q [srcreg+d4+d1], xmm1		;; Save R10
	vmovsd	Q [srcreg+d4], xmm0		;; Save R9
	vmovsd	Q [srcreg+d4+d1+32], xmm3	;; Save I10
	vmovsd	Q [srcreg+d4+32], xmm2		;; Save I9

	vmovsd	xmm0, YMM_TMP4[0]		;; R11
	vmovsd	xmm7, YMM_TMP4[24] 		;; I12
	vsubsd	xmm1, xmm0, xmm7		;; R11 - I12 (new R11)
	vaddsd	xmm0, xmm0, xmm7		;; R11 + I12 (new R12)

	vmovsd	xmm2, YMM_TMP4[8]		;; I11
	vmovsd	xmm7, YMM_TMP4[16] 		;; R12
	vsubsd	xmm3, xmm2, xmm7		;; I11 - R12 (new I12)
	vaddsd	xmm2, xmm2, xmm7		;; I11 + R12 (new I11)

	vmovsd	Q [srcreg+d4+d2], xmm1		;; Save R11
	vmovsd	Q [srcreg+d4+d2+d1], xmm0	;; Save R12
	vmovsd	Q [srcreg+d4+d2+d1+32], xmm3	;; Save I12
	vmovsd	Q [srcreg+d4+d2+32], xmm2	;; Save I11
	ENDM


yr8_8cl_sixteen_reals_eight_complex_with_square MACRO srcreg,srcinc,d1,d2,d4
	ysquare7 srcreg
	;; Do the sixteen reals part 1
	yr8_16r_simple_fft_part1 srcreg,d1,d2,d4
	;; Do the three eight-complex part
	yr8_8c_simple_fft_part1 srcreg,d1,d2,d4
	yr8_8c_simple_fft_with_square srcreg,d1,d2,d4
	yr8_8c_simple_unfft srcreg,d1,d2,d4
	;; Do the remaining sixteen reals work
	yr8_16r_simple_fft_with_square srcreg,d1,d2,d4
	yr8_16r_simple_unfft srcreg,d1,d2,d4
	bump	srcreg, srcinc
	ENDM

yr8_16r_simple_fft_with_square MACRO srcreg,d1,d2,d4
	vmovsd	xmm0, YMM_TMP1[0]		;; R1
	vmovsd	xmm7, YMM_TMP1[8] 		;; R2
	vsubsd	xmm1, xmm0, xmm7		;; R1 - R2 (new R2)
	vaddsd	xmm0, xmm0, xmm7		;; R1 + R2 (new R1)

	vmulsd	xmm0, xmm0, xmm0		;; Square R1
	vmulsd	xmm1, xmm1, xmm1		;; Square R2
	vmovsd	Q [srcreg-16], xmm0		;; Save square of sum of FFT values
	vsubsd	xmm0, xmm0, xmm1		;; R1 - R2 (final R2)
	vmulsd	xmm0, xmm0, YMM_HALF		;; Mul R2 by HALF
	vaddsd	xmm1, xmm1, xmm0		;; R1 + R2 (final R1)

	vmovsd	xmm2, YMM_TMP1[16]		;; R3/R4 morphs into R3/I3
	vmovsd	xmm3, YMM_TMP1[24]

	ys_complex_square xmm2, xmm3, xmm7	;; Square R3/I3

						;; R3/I3 morphs into R3/R4

	vmovsd	YMM_TMP1[0], xmm1		;; Save R1
	vmovsd	YMM_TMP1[8], xmm0		;; Save R2
	vmovsd	YMM_TMP1[16], xmm2		;; Save R3
	vmovsd	YMM_TMP1[24], xmm3		;; Save R4

	vmovsd	xmm0, YMM_TMP2[0]		;; R5
	vmovsd	xmm7, YMM_TMP2[16] 		;; R6
	vsubsd	xmm1, xmm0, xmm7		;; R5 - R6 (new R6)
	vaddsd	xmm0, xmm0, xmm7		;; R5 + R6 (new R5)

	vmovsd	xmm2, YMM_TMP2[8]		;; I5
	vmovsd	xmm7, YMM_TMP2[24] 		;; I6
	vsubsd	xmm3, xmm2, xmm7		;; I5 - I6 (new I6)
	vaddsd	xmm2, xmm2, xmm7		;; I5 + I6 (new I5)

	ys_complex_square xmm0, xmm2, xmm7	;; Square R5/I5
	ys_complex_square xmm1, xmm3, xmm7	;; Square R6/I6

	vaddsd	xmm4, xmm0, xmm1		;; R5 + R6 (new R5)
	vsubsd	xmm0, xmm0, xmm1		;; R5 - R6 (new R6)

	vaddsd	xmm5, xmm2, xmm3		;; I5 + I6 (new I5)
	vsubsd	xmm2, xmm2, xmm3		;; I5 - I6 (new I6)

	vmovsd	YMM_TMP2[0], xmm4		;; Save R5
	vmovsd	YMM_TMP2[16], xmm0		;; Save R6
	vmovsd	YMM_TMP2[8], xmm5		;; Save I5
	vmovsd	YMM_TMP2[24], xmm2		;; Save I6

	vmovsd	xmm0, YMM_TMP3[0]		;; R9
	vmovsd	xmm7, YMM_TMP3[16] 		;; R10
	vsubsd	xmm1, xmm0, xmm7		;; R9 - R10 (new R10)
	vaddsd	xmm0, xmm0, xmm7		;; R9 + R10 (new R9)

	vmovsd	xmm2, YMM_TMP3[8]		;; I9
	vmovsd	xmm7, YMM_TMP3[24] 		;; I10
	vsubsd	xmm3, xmm2, xmm7		;; I9 - I10 (new I10)
	vaddsd	xmm2, xmm2, xmm7		;; I9 + I10 (new I9)

	ys_complex_square xmm0, xmm2, xmm7	;; Square R9/I9
	ys_complex_square xmm1, xmm3, xmm7	;; Square R10/I10

	vaddsd	xmm4, xmm0, xmm1		;; R9 + R10 (new R9)
	vsubsd	xmm0, xmm0, xmm1		;; R9 - R10 (new R10)

	vaddsd	xmm5, xmm2, xmm3		;; I9 + I10 (new I9)
	vsubsd	xmm2, xmm2, xmm3		;; I9 - I10 (new I10)

	vmovsd	YMM_TMP3[0], xmm4		;; Save R9
	vmovsd	YMM_TMP3[16], xmm0		;; Save R10
	vmovsd	YMM_TMP3[8], xmm5		;; Save I9
	vmovsd	YMM_TMP3[24], xmm2		;; Save I10

	vmovsd	xmm0, YMM_TMP4[0]		;; R11
	vmovsd	xmm7, YMM_TMP4[24] 		;; I12
	vsubsd	xmm1, xmm0, xmm7		;; R11 - I12 (new R11)
	vaddsd	xmm0, xmm0, xmm7		;; R11 + I12 (new R12)

	vmovsd	xmm2, YMM_TMP4[8]		;; I11
	vmovsd	xmm7, YMM_TMP4[16] 		;; R12
	vsubsd	xmm3, xmm2, xmm7		;; I11 - R12 (new I12)
	vaddsd	xmm2, xmm2, xmm7		;; I11 + R12 (new I11)

	ys_complex_square xmm1, xmm2, xmm7	;; Square R11/I11
	ys_complex_square xmm0, xmm3, xmm7	;; Square R12/I12

	vaddsd	xmm4, xmm0, xmm1		;; R12 + R11 (new R11)
	vsubsd	xmm0, xmm0, xmm1		;; R12 - R11 (new I12)

	vaddsd	xmm5, xmm2, xmm3		;; I11 + I12 (new I11)
	vsubsd	xmm2, xmm2, xmm3		;; I11 - I12 (new R12)

	vmovsd	YMM_TMP4[0], xmm4		;; Save R11
	vmovsd	YMM_TMP4[24], xmm0		;; Save I12
	vmovsd	YMM_TMP4[8], xmm5		;; Save I11
	vmovsd	YMM_TMP4[16], xmm2		;; Save R12
	ENDM

yr8_16r_simple_unfft MACRO srcreg,d1,d2,d4

	;; Even level 3

	vmovsd	xmm4, YMM_TMP3[16] 		;; R10
	vmovsd	xmm7, YMM_TMP4[16]		;; R12
	vaddsd	xmm0, xmm4, xmm7		;; R10 + R12 (new R10)
	vsubsd	xmm4, xmm4, xmm7		;; R10 - R12 (new R12)

	vmovsd	xmm6, YMM_TMP3[24]		;; I10
	vmovsd	xmm7, YMM_TMP4[24]		;; I12
	vaddsd	xmm2, xmm6, xmm7		;; I10 + I12 (new I10)
	vsubsd	xmm6, xmm6, xmm7		;; I10 - I12 (new I12)

	vmovsd	xmm7, YMM_TMP1[8]		;; R2
	vmovsd	xmm5, YMM_TMP1[24] 		;; R4
	vaddsd	xmm3, xmm7, xmm5		;; R2 + R4 (new R2)
	vsubsd	xmm7, xmm7, xmm5		;; R2 - R4 (new R4)

	;; Premultipliers for even level 3

						;; mul R10/I10 by w^1 = .924 - .383i
	vmulsd	xmm1, xmm0, YMM_P924
	vmulsd	xmm5, xmm2, YMM_P383
	vaddsd	xmm1, xmm1, xmm5		;; Twiddled R10
	vmulsd	xmm0, xmm0, YMM_P383
	vmulsd	xmm2, xmm2, YMM_P924
	vsubsd	xmm0, xmm2, xmm0		;; Twiddled I10

						;; mul R12/I12 by w^3 = .383 - .924i
	vmulsd	xmm5, xmm4, YMM_P383
	vmulsd	xmm2, xmm6, YMM_P924
	vaddsd	xmm5, xmm5, xmm2		;; Twiddled R12
	vmulsd	xmm4, xmm4, YMM_P924
	vmulsd	xmm6, xmm6, YMM_P383
	vsubsd	xmm4, xmm6, xmm4		;; Twiddled I12

	vmovsd	YMM_TMP8, xmm5			;; Temporarily save R12

	;; Premultipliers for even level 4

						;; mul R6/I6 by w^2 = .707 - .707i
	vmovsd	xmm5, YMM_TMP2[24]		;; I6
	vmovsd	xmm2, YMM_TMP2[16] 		;; R6
	vaddsd	xmm6, xmm5, xmm2		;; R6 = I6 + R6
	vsubsd	xmm5, xmm5, xmm2		;; I6 = I6 - R6
	vmulsd	xmm6, xmm6, YMM_SQRTHALF	;; R6 = R6 * SQRTHALF
	vmulsd	xmm5, xmm5, YMM_SQRTHALF	;; I6 = I6 * SQRTHALF

	;; Even level 3

						;; R6/I6 morph into new R6/R8

	;; Even level 2

	vaddsd	xmm2, xmm3, xmm6		;; R2 + R6 (newer R2)
	vsubsd	xmm3, xmm3, xmm6		;; R2 - R6 (newer R6)

	vaddsd	xmm6, xmm7, xmm5		;; R4 + R8 (newer R4)
	vsubsd	xmm7, xmm7, xmm5		;; R4 - R8 (newer R8)

						;; R10/I10 morphs into newer R10/R14
						;; R12/I12 morphs into newer R12/R16

	;; Even level 1

	vaddsd	xmm5, xmm2, xmm1		;; R2 + R10 (new R2)
	vsubsd	xmm2, xmm2, xmm1		;; R2 - R10 (new R10)

	vaddsd	xmm1, xmm3, xmm0		;; R6 + R14 (new R6)
	vsubsd	xmm3, xmm3, xmm0		;; R6 - R14 (new R14)

	vmovsd	Q [srcreg+d1], xmm5		;; R2
	vmovsd	Q [srcreg+d1+32], xmm2		;; R10
	vmovsd	Q [srcreg+d4+d1], xmm1		;; R6
	vmovsd	Q [srcreg+d4+d1+32], xmm3	;; R14

	vmovsd	xmm1, YMM_TMP8			;; Reload saved R12

	vaddsd	xmm2, xmm6, xmm1		;; R4 + R12 (new R4)
	vsubsd	xmm6, xmm6, xmm1		;; R4 - R12 (new R12)

	vaddsd	xmm3, xmm7, xmm4		;; R8 + R16 (new R8)
	vsubsd	xmm7, xmm7, xmm4		;; R8 - R16 (new R16)

	vmovsd	Q [srcreg+d2+d1], xmm2		;; R4
	vmovsd	Q [srcreg+d2+d1+32], xmm6	;; R12
	vmovsd	Q [srcreg+d4+d2+d1], xmm3	;; R8
	vmovsd	Q [srcreg+d4+d2+d1+32], xmm7	;; R16

	;; Odd level 3

	vmovsd	xmm4, YMM_TMP3[0]		;; R9
	vmovsd	xmm7, YMM_TMP4[0] 		;; R11
	vaddsd	xmm0, xmm4, xmm7		;; R9 + R11 (new R9)
	vsubsd	xmm4, xmm4, xmm7		;; R9 - R11 (new R11)

	vmovsd	xmm6, YMM_TMP3[8] 		;; I9
	vmovsd	xmm7, YMM_TMP4[8]		;; I11
	vaddsd	xmm2, xmm6, xmm7		;; I9 + I11 (new I9)
	vsubsd	xmm6, xmm6, xmm7		;; I9 - I11 (new I11)

	vmovsd	xmm7, YMM_TMP1[0]		;; R1
	vmovsd	xmm1, YMM_TMP1[16] 		;; R3
	vaddsd	xmm3, xmm7, xmm1		;; R1 + R3 (new R1)
	vsubsd	xmm7, xmm7, xmm1		;; R1 - R3 (new R3)

						;; R5/I5 morphs into new R5/R7

	;; Premultipliers for odd level 3

						;; mul R11/I11 by SQRTHALF - i*SQRTHALF
	vaddsd	xmm1, xmm6, xmm4		;; R11 = I11 + R11
	vsubsd	xmm6, xmm6, xmm4		;; I11 = I11 - R11
	vmulsd	xmm1, xmm1, YMM_SQRTHALF	;; R11 = R11 * SQRTHALF
	vmulsd	xmm6, xmm6, YMM_SQRTHALF	;; I11 = I11 * SQRTHALF

	;; Odd level 2

	vmovsd	xmm4, YMM_TMP2[0] 		;; R5 (a.k.a. new R5)
	vaddsd	xmm5, xmm3, xmm4		;; R1 + R5 (newer R1)
	vsubsd	xmm3, xmm3, xmm4		;; R1 - R5 (newer R5)

						;; R9/I9 morphs into newer R9/R13
						;; R11/I11 morphs into newer R11/R15

	;; Odd level 1

	vaddsd	xmm4, xmm5, xmm0		;; R1 + R9 (new R1)
	vsubsd	xmm5, xmm5, xmm0		;; R1 - R9 (new R9)

	vaddsd	xmm0, xmm3, xmm2		;; R5 + R13 (new R5)
	vsubsd	xmm3, xmm3, xmm2		;; R5 - R13 (new R13)

	vmovsd	Q [srcreg], xmm4		;; R1
	vmovsd	Q [srcreg+32], xmm5		;; R9
	vmovsd	Q [srcreg+d4], xmm0		;; R5
	vmovsd	Q [srcreg+d4+32], xmm3		;; R13

	;; Odd level 2

	vmovsd	xmm5, YMM_TMP2[8] 		;; I5 (a.k.a new R7)
	vaddsd	xmm0, xmm7, xmm5		;; R3 + R7 (newer R3)
	vsubsd	xmm7, xmm7, xmm5		;; R3 - R7 (newer R7)

	;; Odd level 1

	vaddsd	xmm5, xmm0, xmm1		;; R3 + R11 (new R3)
	vsubsd	xmm0, xmm0, xmm1		;; R3 - R11 (new R11)

	vaddsd	xmm1, xmm7, xmm6		;; R7 + R15 (new R7)
	vsubsd	xmm7, xmm7, xmm6		;; R7 - R15 (new R15)

	vmovsd	Q [srcreg+d2], xmm5		;; R3
	vmovsd	Q [srcreg+d2+32], xmm0		;; R11
	vmovsd	Q [srcreg+d4+d2], xmm1		;; R7
	vmovsd	Q [srcreg+d4+d2+32], xmm7	;; R15
	ENDM


yr8_8cl_sixteen_reals_eight_complex_with_mult MACRO srcreg,srcinc,d1,d2,d4
	ymult7	srcreg, srcreg+rbp
	;; Do the sixteen reals part 1
	yr8_16r_simple_fft_part1 srcreg,d1,d2,d4
	;; Do the three eight-complex part
	yr8_8c_simple_fft_part1 srcreg,d1,d2,d4
	yr8_8c_simple_fft_with_mult srcreg,srcreg+rbp,d1,d2,d4
	yr8_8c_simple_unfft srcreg,d1,d2,d4
	;; Do the remaining sixteen reals work
	yr8_16r_simple_fft_with_mult srcreg,srcreg+rbp,d1,d2,d4
	yr8_16r_simple_unfft srcreg,d1,d2,d4
	bump	srcreg, srcinc
	ENDM

yr8_16r_simple_fft_with_mult MACRO srcreg,altsrc,d1,d2,d4
	vmovsd	xmm0, YMM_TMP1[0]		;; R1
	vmovsd	xmm7, YMM_TMP1[8] 		;; R2
	vsubsd	xmm1, xmm0, xmm7		;; R1 - R2 (new R2)
	vaddsd	xmm0, xmm0, xmm7		;; R1 + R2 (new R1)

	vmulsd	xmm0, xmm0, Q [altsrc]		;; R1 * R1-from-mem
	vmulsd	xmm1, xmm1, Q [altsrc+32]	;; R2 * R2-from-mem
	vmovsd	Q [srcreg-16], xmm0		;; Save product of sum of FFT values
	vsubsd	xmm0, xmm0, xmm1		;; R1 - R2 (final R2)
	vmulsd	xmm0, xmm0, YMM_HALF		;; Mul R2 by HALF
	vaddsd	xmm1, xmm1, xmm0		;; R1 + R2 (final R1)

	vmovsd	xmm2, YMM_TMP1[16]		;; R3/R4 morphs into R3/I3
	vmovsd	xmm3, YMM_TMP1[24]

	ys_complex_mult xmm2, xmm3, Q [altsrc+d1], Q [altsrc+d1+32], xmm6, xmm7 ;; Mult R3/I3

						;; R3/I3 morphs into R3/R4

	vmovsd	YMM_TMP1[0], xmm1		;; Save R1
	vmovsd	YMM_TMP1[8], xmm0		;; Save R2
	vmovsd	YMM_TMP1[16], xmm2		;; Save R3
	vmovsd	YMM_TMP1[24], xmm3		;; Save R4

	vmovsd	xmm0, YMM_TMP2[0]		;; R5
	vmovsd	xmm7, YMM_TMP2[16] 		;; R6
	vsubsd	xmm1, xmm0, xmm7		;; R5 - R6 (new R6)
	vaddsd	xmm0, xmm0, xmm7		;; R5 + R6 (new R5)

	vmovsd	xmm2, YMM_TMP2[8]		;; I5
	vmovsd	xmm7, YMM_TMP2[24] 		;; I6
	vsubsd	xmm3, xmm2, xmm7		;; I5 - I6 (new I6)
	vaddsd	xmm2, xmm2, xmm7		;; I5 + I6 (new I5)

	ys_complex_mult xmm0, xmm2, Q [altsrc+d2], Q [altsrc+d2+32], xmm6, xmm7 ;; Mult R5/I5
	ys_complex_mult xmm1, xmm3, Q [altsrc+d2+d1], Q [altsrc+d2+d1+32], xmm6, xmm7 ;; Mult R6/I6

	vaddsd	xmm4, xmm0, xmm1		;; R5 + R6 (new R5)
	vsubsd	xmm0, xmm0, xmm1		;; R5 - R6 (new R6)

	vaddsd	xmm5, xmm2, xmm3		;; I5 + I6 (new I5)
	vsubsd	xmm2, xmm2, xmm3		;; I5 - I6 (new I6)

	vmovsd	YMM_TMP2[0], xmm4		;; Save R5
	vmovsd	YMM_TMP2[16], xmm0		;; Save R6
	vmovsd	YMM_TMP2[8], xmm5		;; Save I5
	vmovsd	YMM_TMP2[24], xmm2		;; Save I6

	vmovsd	xmm0, YMM_TMP3[0]		;; R9
	vmovsd	xmm7, YMM_TMP3[16] 		;; R10
	vsubsd	xmm1, xmm0, xmm7		;; R9 - R10 (new R10)
	vaddsd	xmm0, xmm0, xmm7		;; R9 + R10 (new R9)

	vmovsd	xmm2, YMM_TMP3[8]		;; I9
	vmovsd	xmm7, YMM_TMP3[24] 		;; I10
	vsubsd	xmm3, xmm2, xmm7		;; I9 - I10 (new I10)
	vaddsd	xmm2, xmm2, xmm7		;; I9 + I10 (new I9)

	ys_complex_mult xmm0, xmm2, Q [altsrc+d4], Q [altsrc+d4+32], xmm6, xmm7 ;; Mult R9/I9
	ys_complex_mult xmm1, xmm3, Q [altsrc+d4+d1], Q [altsrc+d4+d1+32], xmm6, xmm7 ;; Mult R10/I10

	vaddsd	xmm4, xmm0, xmm1		;; R9 + R10 (new R9)
	vsubsd	xmm0, xmm0, xmm1		;; R9 - R10 (new R10)

	vaddsd	xmm5, xmm2, xmm3		;; I9 + I10 (new I9)
	vsubsd	xmm2, xmm2, xmm3		;; I9 - I10 (new I10)

	vmovsd	YMM_TMP3[0], xmm4		;; Save R9
	vmovsd	YMM_TMP3[16], xmm0		;; Save R10
	vmovsd	YMM_TMP3[8], xmm5		;; Save I9
	vmovsd	YMM_TMP3[24], xmm2		;; Save I10

	vmovsd	xmm0, YMM_TMP4[0]		;; R11
	vmovsd	xmm7, YMM_TMP4[24] 		;; I12
	vsubsd	xmm1, xmm0, xmm7		;; R11 - I12 (new R11)
	vaddsd	xmm0, xmm0, xmm7		;; R11 + I12 (new R12)

	vmovsd	xmm2, YMM_TMP4[8]		;; I11
	vmovsd	xmm7, YMM_TMP4[16] 		;; R12
	vsubsd	xmm3, xmm2, xmm7		;; I11 - R12 (new I12)
	vaddsd	xmm2, xmm2, xmm7		;; I11 + R12 (new I11)

	ys_complex_mult xmm1, xmm2, Q [altsrc+d4+d2], Q [altsrc+d4+d2+32], xmm6, xmm7 ;; Mult R11/I11
	ys_complex_mult xmm0, xmm3, Q [altsrc+d4+d2+d1], Q [altsrc+d4+d2+d1+32], xmm6, xmm7 ;; Mult R12/I12

	vaddsd	xmm4, xmm0, xmm1		;; R12 + R11 (new R11)
	vsubsd	xmm0, xmm0, xmm1		;; R12 - R11 (new I12)

	vaddsd	xmm5, xmm2, xmm3		;; I11 + I12 (new I11)
	vsubsd	xmm2, xmm2, xmm3		;; I11 - I12 (new R12)

	vmovsd	YMM_TMP4[0], xmm4		;; Save R11
	vmovsd	YMM_TMP4[24], xmm0		;; Save I12
	vmovsd	YMM_TMP4[8], xmm5		;; Save I11
	vmovsd	YMM_TMP4[16], xmm2		;; Save R12
	ENDM


yr8_8cl_sixteen_reals_eight_complex_with_mulf MACRO srcreg,srcinc,d1,d2,d4
	ymult7	srcreg+rbx, srcreg+rbp
	;; Start the sixteen reals work
	yr8_16r_simple_fft_with_mulf srcreg,d1,d2,d4
	;; Do the three eight-complex part
	yr8_8c_simple_fft_with_mulf srcreg,d1,d2,d4
	yr8_8c_simple_unfft srcreg,d1,d2,d4
	;; Finish the sixteen reals work
	yr8_16r_simple_unfft srcreg,d1,d2,d4
	bump	srcreg, srcinc
	ENDM

yr8_16r_simple_fft_with_mulf MACRO srcreg,d1,d2,d4
	vmovsd	xmm0, Q [srcreg][rbx]		;; R1
	vmovsd	xmm1, Q [srcreg+32][rbx]	;; R2

	vmulsd	xmm0, xmm0, Q [srcreg][rbp]	;; R1 * R1-from-mem
	vmulsd	xmm1, xmm1, Q [srcreg+32][rbp]	;; R2 * R2-from-mem
	vmovsd	Q [srcreg-16], xmm0		;; Save product of sum of FFT values
	vsubsd	xmm0, xmm0, xmm1		;; R1 - R2 (final R2)
	vmulsd	xmm0, xmm0, YMM_HALF		;; Mul R2 by HALF
	vaddsd	xmm1, xmm1, xmm0		;; R1 + R2 (final R1)

	vmovsd	xmm2, Q [srcreg+d1][rbx]	;; R3/R4 morphs into R3/I3
	vmovsd	xmm3, Q [srcreg+d1+32][rbx]

	ys_complex_mult xmm2, xmm3, Q [srcreg+d1][rbp], Q [srcreg+d1+32][rbp], xmm6, xmm7 ;; Mult R3/I3

						;; R3/I3 morphs into R3/R4

	vmovsd	YMM_TMP1[0], xmm1		;; Save R1
	vmovsd	YMM_TMP1[8], xmm0		;; Save R2
	vmovsd	YMM_TMP1[16], xmm2		;; Save R3
	vmovsd	YMM_TMP1[24], xmm3		;; Save R4

	vmovsd	xmm0, Q [srcreg+d2][rbx]	;; R5
	vmovsd	xmm2, Q [srcreg+d2+32][rbx]	;; I5
	vmovsd	xmm1, Q [srcreg+d2+d1][rbx]	;; R6
	vmovsd	xmm3, Q [srcreg+d2+d1+32][rbx]	;; I6

	ys_complex_mult xmm0, xmm2, Q [srcreg+d2][rbp], Q [srcreg+d2+32][rbp], xmm6, xmm7 ;; Mult R5/I5
	ys_complex_mult xmm1, xmm3, Q [srcreg+d2+d1][rbp], Q [srcreg+d2+d1+32][rbp], xmm6, xmm7 ;; Mult R6/I6

	vaddsd	xmm4, xmm0, xmm1		;; R5 + R6 (new R5)
	vsubsd	xmm0, xmm0, xmm1		;; R5 - R6 (new R6)

	vaddsd	xmm5, xmm2, xmm3		;; I5 + I6 (new I5)
	vsubsd	xmm2, xmm2, xmm3		;; I5 - I6 (new I6)

	vmovsd	YMM_TMP2[0], xmm4		;; Save R5
	vmovsd	YMM_TMP2[16], xmm0		;; Save R6
	vmovsd	YMM_TMP2[8], xmm5		;; Save I5
	vmovsd	YMM_TMP2[24], xmm2		;; Save I6

	vmovsd	xmm0, Q [srcreg+d4][rbx]	;; R9
	vmovsd	xmm2, Q [srcreg+d4+32][rbx]	;; I9
	vmovsd	xmm1, Q [srcreg+d4+d1][rbx]	;; R10
	vmovsd	xmm3, Q [srcreg+d4+d1+32][rbx]	;; I10

	ys_complex_mult xmm0, xmm2, Q [srcreg+d4][rbp], Q [srcreg+d4+32][rbp], xmm6, xmm7 ;; Mult R9/I9
	ys_complex_mult xmm1, xmm3, Q [srcreg+d4+d1][rbp], Q [srcreg+d4+d1+32][rbp], xmm6, xmm7 ;; Mult R10/I10

	vaddsd	xmm4, xmm0, xmm1		;; R9 + R10 (new R9)
	vsubsd	xmm0, xmm0, xmm1		;; R9 - R10 (new R10)

	vaddsd	xmm5, xmm2, xmm3		;; I9 + I10 (new I9)
	vsubsd	xmm2, xmm2, xmm3		;; I9 - I10 (new I10)

	vmovsd	YMM_TMP3[0], xmm4		;; Save R9
	vmovsd	YMM_TMP3[16], xmm0		;; Save R10
	vmovsd	YMM_TMP3[8], xmm5		;; Save I9
	vmovsd	YMM_TMP3[24], xmm2		;; Save I10

	vmovsd	xmm1, Q [srcreg+d4+d2][rbx]	;; R11
	vmovsd	xmm2, Q [srcreg+d4+d2+32][rbx]	;; I11
	vmovsd	xmm0, Q [srcreg+d4+d2+d1][rbx] 	;; R12
	vmovsd	xmm3, Q [srcreg+d4+d2+d1+32][rbx] ;; I12

	ys_complex_mult xmm1, xmm2, Q [srcreg+d4+d2][rbp], Q [srcreg+d4+d2+32][rbp], xmm6, xmm7 ;; Mult R11/I11
	ys_complex_mult xmm0, xmm3, Q [srcreg+d4+d2+d1][rbp], Q [srcreg+d4+d2+d1+32][rbp], xmm6, xmm7 ;; Mult R12/I12

	vaddsd	xmm4, xmm0, xmm1		;; R12 + R11 (new R11)
	vsubsd	xmm0, xmm0, xmm1		;; R12 - R11 (new I12)

	vaddsd	xmm5, xmm2, xmm3		;; I11 + I12 (new I11)
	vsubsd	xmm2, xmm2, xmm3		;; I11 - I12 (new R12)

	vmovsd	YMM_TMP4[0], xmm4		;; Save R11
	vmovsd	YMM_TMP4[24], xmm0		;; Save I12
	vmovsd	YMM_TMP4[8], xmm5		;; Save I11
	vmovsd	YMM_TMP4[16], xmm2		;; Save R12
	ENDM

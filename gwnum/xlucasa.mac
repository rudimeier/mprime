; Copyright 2005-2009 - Mersenne Research, Inc.  All rights reserved
; Author:  George Woltman
; Email: woltman@alum.mit.edu
;
; These macros implement AMD64 optimized versions of macros found
; in xlucas.mac.
;

;; On AMD64 (K8 architecture, not the Phenom K10 architecture), punpckhqdq is grossly slow.
unpckhi MACRO dest, src
	unpckhpd dest, src
	ENDM

unpcklo MACRO dest, src
	unpcklpd dest, src
	ENDM

;; Macros that load or store register(s) shuffling the data

low_load MACRO reg, mem1, mem2
	movhpd	reg, Q mem2
	movlpd	reg, Q mem1
	ENDM

high_load MACRO reg, mem1, mem2
	movhpd	reg, Q mem2[8]
	movlpd	reg, Q mem1[8]
	ENDM

shuffle_load MACRO reglo, reghi, mem1, mem2
	movhpd	reglo, Q mem2
	movlpd	reglo, Q mem1
	movhpd	reghi, Q mem2[8]
	movlpd	reghi, Q mem1[8]
	ENDM

shuffle_store MACRO mem1, mem2, reglo, reghi
	movsd	Q mem1, reglo
	movsd	Q mem1[8], reghi
	unpckhi reglo, reghi
	movapd	mem2, reglo
	ENDM

x4c_fft_screg MACRO r1, r2, r3, r4, r5, r6, r7, r8, mem8, off, screg, pre1, pre2, dst1, dst2
	movapd	r8, [screg+off+32+16]	;; cosine/sine
	mulpd	r8, r3			;; A3 = R3 * cosine/sine	;1-6
	subpd	r8, r7			;; A3 = A3 - I3			;8-11
	mulpd	r7, [screg+off+32+16]	;; B3 = I3 * cosine/sine	;3-8
	addpd	r7, r3			;; B3 = B3 + R3			;10-13
	movapd	r3, [screg+off+0+16]	;; cosine/sine
	mulpd	r3, r2			;; A2 = R2 * cosine/sine	;5-10
	subpd	r3, r6			;; A2 = A2 - I2			;12-15
	mulpd	r6, [screg+off+0+16]	;; B2 = I2 * cosine/sine	;9-14
	addpd	r6, r2			;; B2 = B2 + R2			;16-19
	movapd	r2, [screg+off+64+16]	;; cosine/sine
	mulpd	r2, mem8		;; B4 = I4 * cosine/sine	;11-16
	addpd	r2, r4			;; B4 = B4 + R4			;18-21
	mulpd	r4, [screg+off+64+16]	;; A4 = R4 * cosine/sine	;7-12
	subpd	r4, mem8		;; A4 = A4 - I4			;14-17
	mulpd	r8, [screg+off+32]	;; A3 = A3 * sine (new R3)	;13-18
	mulpd	r7, [screg+off+32]	;; B3 = B3 * sine (new I3)	;15-20
	mulpd	r3, [screg+off+0]	;; A2 = A2 * sine (new R2)	;17-22
	mulpd	r4, [screg+off+64]	;; A4 = A4 * sine (new R4)	;19-24
	xprefetchw [pre1]
	 subpd	r1, r8			;; R1 = R1 - R3 (new R3)	;20-23
	 multwo	r8
	mulpd	r6, [screg+off+0]	;; B2 = B2 * sine (new I2)	;21-26
	 subpd	r5, r7			;; I1 = I1 - I3 (new I3)	;22-25
	 multwo	r7
	mulpd	r2, [screg+off+64]	;; B4 = B4 * sine (new I4)	;23-28
	xprefetchw [pre1][pre2]
	 addpd	r8, r1			;; R3 = R1 + R3 (new R1)	;24-27
	 subpd	r3, r4			;; R2 = R2 - R4 (new R4)	;26-29
	 multwo	r4			;; R4 = R4 * 2			;27-32
	 addpd	r7, r5			;; I3 = I1 + I3 (new I1)	;28-31
	 subpd	r6, r2			;; I2 = I2 - I4 (new I4)	;30-33
	 multwo	r2			;; I4 = I4 * 2			;31-36
	subpd	r5, r3			;; I3 = I3 - R4 (final I4)	;32-35
	IFNB <dst1>
	movapd	dst1, r5
	ENDIF
	 addpd	r4, r3			;; R4 = R2 + R4 (new R2)	;34-37
	multwo	r3			;; R4 = R4 * 2			;35-40
	 addpd	r2, r6			;; I4 = I2 + I4 (new I2)	;36-39
	subpd	r1, r6			;; R3 = R3 - I4 (final R3)	;38-41
	IFNB <dst2>
	movapd	dst2, r1
	ENDIF
	multwo	r6			;; I4 = I4 * 2			;39-44
	subpd	r8, r4			;; R1 = R1 - R2 (final R2)	;40-43
	multwo	r4			;; R2 = R2 * 2			;41-46
	subpd	r7, r2			;; I1 = I1 - I2 (final I2)	;42-45
	multwo	r2			;; I2 = I2 * 2			;43-48
	addpd	r3, r5			;; R4 = I3 + R4 (final I3)	;44-47
	addpd	r6, r1			;; I4 = R3 + I4 (final R4)	;46-49
	addpd	r4, r8			;; R2 = R1 + R2 (final R1)	;48-51
	addpd	r2, r7			;; I2 = I1 + I2 (final I1)	;50-53
	ENDM
x4c_fft_mem_screg MACRO R1,R2,R3,R4,R5,R6,R7,R8,off,screg,pre1,pre2,dst1,dst2
	movapd	xmm0, R3		;; R3
	movapd	xmm1, [screg+off+32+16]	;; cosine/sine
	mulpd	xmm1, xmm0		;; A3 = R3 * cosine/sine	;1-6
	movapd	xmm2, R7		;; I3
	movapd	xmm3, [screg+off+32+16]	;; cosine/sine
	mulpd	xmm3, xmm2		;; B3 = I3 * cosine/sine	;3-8
	movapd	xmm4, R2		;; R2
	movapd	xmm6, [screg+off+0+16]	;; cosine/sine
	mulpd	xmm4, xmm6		;; A2 = R2 * cosine/sine	;5-10
	movapd	xmm5, R4		;; R4
	movapd	xmm7, [screg+off+64+16]	;; cosine/sine
	mulpd	xmm5, xmm7		;; A4 = R4 * cosine/sine	;7-12
	subpd	xmm1, xmm2		;; A3 = A3 - I3			;8-11
	movapd	xmm2, R6		;; I2
	mulpd	xmm6, xmm2		;; B2 = I2 * cosine/sine	;9-14
	addpd	xmm3, xmm0		;; B3 = B3 + R3			;10-13
	movapd	xmm0, R8		;; I4
	mulpd	xmm7, xmm0		;; B4 = I4 * cosine/sine	;11-16
	subpd	xmm4, xmm2		;; A2 = A2 - I2			;12-15
	movapd	xmm2, [screg+off+32]	;; sine
	mulpd	xmm1, xmm2		;; A3 = A3 * sine (new R3)	;13-18
	subpd	xmm5, xmm0		;; A4 = A4 - I4			;14-17
	mulpd	xmm3, xmm2		;; B3 = B3 * sine (new I3)	;15-20
	addpd	xmm6, R2		;; B2 = B2 + R2			;16-19
	movapd	xmm0, [screg+off+0]	;; sine
	mulpd	xmm4, xmm0		;; A2 = A2 * sine (new R2)	;17-22
	xprefetchw [pre1]
	addpd	xmm7, R4		;; B4 = B4 + R4			;18-21
	mulpd	xmm5, [screg+off+64]	;; A4 = A4 * sine (new R4)	;19-24
	 movapd	xmm2, R1		;; R1
	 subpd	xmm2, xmm1		;; R1 = R1 - R3 (new R3)	;20-23
	mulpd	xmm6, xmm0		;; B2 = B2 * sine (new I2)	;21-26
	 movapd	xmm0, R5		;; I1
	 subpd	xmm0, xmm3		;; I1 = I1 - I3 (new I3)	;22-25
	mulpd	xmm7, [screg+off+64]	;; B4 = B4 * sine (new I4)	;23-28
	 addpd	xmm1, R1		;; R3 = R1 + R3 (new R1)	;24-27
	xprefetchw [pre1][pre2]
	 subpd	xmm4, xmm5		;; R2 = R2 - R4 (new R4)	;26-29
	 multwo	xmm5			;; R4 = R4 * 2			;27-32
	 addpd	xmm3, R5		;; I3 = I1 + I3 (new I1)	;28-31
	 subpd	xmm6, xmm7		;; I2 = I2 - I4 (new I4)	;30-33
	 multwo	xmm7			;; I4 = I4 * 2			;31-36
	subpd	xmm0, xmm4		;; I3 = I3 - R4 (final I4)	;32-35
	IFNB <dst1>
	movapd	dst1, xmm0
	ENDIF
	 addpd	xmm5, xmm4		;; R4 = R2 + R4 (new R2)	;34-37
	multwo	xmm4			;; R4 = R4 * 2			;35-40
	 addpd	xmm7, xmm6		;; I4 = I2 + I4 (new I2)	;36-39
	subpd	xmm2, xmm6		;; R3 = R3 - I4 (final R3)	;38-41
	IFNB <dst2>
	movapd	dst2, xmm2
	ENDIF
	multwo	xmm6			;; I4 = I4 * 2			;39-44
	subpd	xmm1, xmm5		;; R1 = R1 - R2 (final R2)	;40-43
	multwo	xmm5			;; R2 = R2 * 2			;41-46
	subpd	xmm3, xmm7		;; I1 = I1 - I2 (final I2)	;42-45
	multwo	xmm7			;; I2 = I2 * 2			;43-48
	addpd	xmm4, xmm0		;; R4 = I3 + R4 (final I3)	;44-47
	addpd	xmm6, xmm2		;; I4 = R3 + I4 (final R4)	;46-49
	addpd	xmm5, xmm1		;; R2 = R1 + R2 (final R1)	;48-51
	addpd	xmm7, xmm3		;; I2 = I1 + I2 (final I1)	;50-53
	ENDM

best_x4c_unfft_screg MACRO r1, r2, r3, r4, r5, r6, r7, r8, mem7, mem8, dest1, dest2, off, screg, pre1, pre2
	subpd	r1, r3			;; new R2 = R1 - R2
	multwo	r3
	movapd	r8, mem8
	addpd	r8, r6			;; new I3 = I3 + I4
	subpd	r6, mem8		;; new R4 = I3 - I4
	subpd	r2, r4			;; new I2 = I1 - I2
	multwo	r4
	movapd	r7, mem7
	subpd	r7, r5			;; new I4 = R4 - R3
	addpd	r5, mem7		;; new R3 = R3 + R4
	addpd	r3, r1			;; new R1 = R1 + R2
	addpd	r4, r2			;; new I1 = I1 + I2
	IFNB <pre1>
	xprefetchw [pre1]
	ENDIF
	subpd	r1, r6			;; R2 = R2 - R4 (new R4)
	multwo	r6			;; R4 = R4 * 2
	addpd	r6, r1			;; R4 = R2 + R4 (new R2)
	subpd	r2, r7			;; I2 = I2 - I4 (new I4)
	multwo	r7			;; I4 = I4 * 2
	addpd	r7, r2			;; I4 = I2 + I4 (new I2)
	subpd	r3, r5			;; R1 = R1 - R3 (new R3)
	multwo	r5			;; R3 = R3 * 2
	addpd	r5, r3			;; R3 = R1 + R3 (new & final R1)
	IFNB <pre1>
	xprefetchw [pre1][pre2]
	ENDIF
	movapd	dest1, r5		;; Save final R1
	subpd	r4, r8			;; I1 = I1 - I3 (new I3)
	multwo	r8			;; I3 = I3 * 2
	addpd	r8, r4			;; I3 = I1 + I3 (new & final I1)
	movapd	dest2, r8		;; Save final I1
	movapd	r5, [screg+off+64+16]	;; cosine/sine
	mulpd	r5, r1			;; A4 = new R4 * cosine/sine
	movapd	r8, [screg+off+64+16]	;; cosine/sine
	mulpd	r8, r2			;; B4 = new I4 * cosine/sine
	addpd	r5, r2			;; A4 = A4 + new I4
	subpd	r8, r1			;; B4 = B4 - new R4
	mulpd	r5, [screg+off+64]	;; A4 = A4 * sine (final R4)
	mulpd	r8, [screg+off+64]	;; B4 = A4 * sine (final I4)
	movapd	r2, [screg+off+0+16]	;; cosine/sine
	mulpd	r2, r6			;; A2 = new R2 * cosine/sine
	movapd	r1, [screg+off+0+16]	;; cosine/sine
	mulpd	r1, r7			;; B2 = new I2 * cosine/sine
	addpd	r2, r7			;; A2 = A2 + new I2
	subpd	r1, r6			;; B2 = B2 - new R2
	movapd	r6, [screg+off+32+16]	;; cosine/sine
	mulpd	r6, r3			;; A3 = new R3 * cosine/sine
	movapd	r7, [screg+off+32+16]	;; cosine/sine
	mulpd	r7, r4			;; B3 = new I3 * cosine/sine
	addpd	r6, r4			;; A3 = A3 + new I3
	subpd	r7, r3 			;; B3 = B3 - new R3
	mulpd	r2, [screg+off+0]	;; A2 = A2 * sine (final R2)
	mulpd	r1, [screg+off+0]	;; B2 = B2 * sine (final I2)
	mulpd	r6, [screg+off+32]	;; A3 = A3 * sine (final R3)
	mulpd	r7, [screg+off+32]	;; B3 = B3 * sine (final I3)
	ENDM

s2cl_four_complex_gpm_fft MACRO srcreg,srcinc,d1
	shuffle_load xmm0, xmm1, [srcreg][rbx], [srcreg+32][rbx] ;; R1,R3
	shuffle_load xmm2, xmm3, [srcreg+16][rbx], [srcreg+48][rbx] ;; R5,R7
	shuffle_load xmm4, xmm5, [srcreg+d1][rbx], [srcreg+d1+32][rbx] ;; R2,R4

	movapd	xmm6, xmm0		;; Save R1
	mulpd	xmm0, [rdi+16]		;; A1 = R1 * premul_real/premul_imag
	subpd	xmm0, xmm2		;; A1 = A1 - I1
	mulpd	xmm2, [rdi+16]		;; B1 = I1 * premul_real/premul_imag
	addpd	xmm2, xmm6		;; B1 = B1 + R1
	mulpd	xmm0, [rdi]		;; A1 = A1 * premul_imag (new R1)
	movapd	[srcreg], xmm0		;; Save new R1

	shuffle_load xmm6,xmm7,[srcreg+d1+16][rbx],[srcreg+d1+48][rbx] ;; R6,R8

	xprefetchw [srcreg+srcinc]
	movapd	xmm0, xmm1		;; Save R3
	mulpd	xmm1, [rdi+80]		;; A3 = R3 * premul_real/premul_imag
	subpd	xmm1, xmm3		;; A3 = A3 - I3
	mulpd	xmm3, [rdi+80]		;; B3 = I3 * premul_real/premul_imag
	addpd	xmm3, xmm0		;; B3 = B3 + R3
	mulpd	xmm2, [rdi]		;; B1 = B1 * premul_imag (new I1)

	xprefetchw [srcreg+srcinc+d1]
	movapd	xmm0, xmm4		;; Save R2
	mulpd	xmm4, [rdi+48]		;; A2 = R2 * premul_real/premul_imag
	subpd	xmm4, xmm6		;; A2 = A2 - I2
	mulpd	xmm6, [rdi+48]		;; B2 = I2 * premul_real/premul_imag
	addpd	xmm6, xmm0		;; B2 = B2 + R2
	mulpd	xmm1, [rdi+64]		;; A3 = A3 * premul_imag (new R3)
	mulpd	xmm3, [rdi+64]		;; B3 = B3 * premul_imag (new I3)

	movapd	xmm0, xmm5		;; Save R4
	mulpd	xmm5, [rdi+112]		;; A4 = R4 * premul_real/premul_imag
	subpd	xmm5, xmm7		;; A4 = A4 - I4
	mulpd	xmm7, [rdi+112]		;; B4 = I4 * premul_real/premul_imag
	addpd	xmm7, xmm0		;; B4 = B4 + R4
	mulpd	xmm4, [rdi+32]		;; A2 = A2 * premul_imag (new R2)
	mulpd	xmm6, [rdi+32]		;; B2 = B2 * premul_imag (new I2)
	mulpd	xmm5, [rdi+96]		;; A4 = A4 * premul_imag (new R4)
	mulpd	xmm7, [rdi+96]		;; B4 = B4 * premul_imag (new I4)

	 movapd	xmm0, [srcreg]		;; Reload new R1
	 subpd	xmm0, xmm1		;; R1 = R1 - R3 (new R3)
	 subpd	xmm2, xmm3		;; I1 = I1 - I3 (new I3)
	xprefetch [srcreg+srcinc][rbx]
	 multwo	xmm3			;; I3 = I3 * 2
	 subpd	xmm4, xmm5		;; R2 = R2 - R4 (new R4)
	 multwo	xmm5			;; R4 = R4 * 2
	xprefetch [srcreg+srcinc+d1][rbx]
	 subpd	xmm6, xmm7		;; I2 = I2 - I4 (new I4)
	 multwo	xmm7			;; I4 = I4 * 2
	 addpd	xmm1, [srcreg]		;; R3 = R1 + R3 (new R1)
	 addpd	xmm3, xmm2		;; I3 = I1 + I3 (new I1)
	 addpd	xmm5, xmm4		;; R4 = R2 + R4 (new R2)
	 addpd	xmm7, xmm6		;; I4 = I2 + I4 (new I2)
	subpd	xmm0, xmm6		;; R3 = R3 - I4 (new R3)
	movapd	[srcreg+d1], xmm0
	multwo	xmm6			;; I4 = I4 * 2
	subpd	xmm2, xmm4		;; I3 = I3 - R4 (new I4)
	movapd	[srcreg+d1+48], xmm2
	multwo	xmm4			;; R4 = R4 * 2
	subpd	xmm1, xmm5		;; R1 = R1 - R2 (new R2)
	multwo	xmm5			;; R2 = R2 * 2
	subpd	xmm3, xmm7		;; I1 = I1 - I2 (new I2)
	multwo	xmm7			;; I2 = I2 * 2
	addpd	xmm6, xmm0		;; I4 = R3 + I4 (new R4)
	addpd	xmm4, xmm2		;; R4 = I3 + R4 (new I3)
	addpd	xmm5, xmm1		;; R2 = R1 + R2 (new R1)
	addpd	xmm7, xmm3		;; I2 = I1 + I2 (new I1)
	movapd	[srcreg+d1+16], xmm4
	movapd	[srcreg+d1+32], xmm6
	movapd	[srcreg], xmm5
	movapd	[srcreg+16], xmm7
	movapd	[srcreg+32], xmm1
	movapd	[srcreg+48], xmm3
	lea	srcreg, [srcreg+srcinc]
	ENDM

x4c_fft4_cmn MACRO r1,r2,r3,r4,r5,r6,r7,r8,mem4,mem8,dest3,off1,off2,off3,off4,pre1,pre2
	movapd	r4, [rdi+off1+16]	;; premul_real/premul_imag
	mulpd	r4, r1			;; A1 = R1 * premul_real/premul_imag
	movapd	r8, [rdi+off1+16]	;; premul_real/premul_imag
	mulpd	r8, r5			;; B1 = I1 * premul_real/premul_imag
	subpd	r4, r5			;; A1 = A1 - I1
	addpd	r8, r1			;; B1 = B1 + R1

	movapd	r1, [rdi+off3+16]	;; premul_real/premul_imag
	mulpd	r1, r3			;; A3 = R3 * premul_real/premul_imag
	movapd	r5, [rdi+off3+16]	;; premul_real/premul_imag
	mulpd	r5, r7			;; B3 = I3 * premul_real/premul_imag
	subpd	r1, r7			;; A3 = A3 - I3
	addpd	r5, r3			;; B3 = B3 + R3
	mulpd	r4, [rdi+off1]		;; A1 = A1 * premul_imag (new R1)
	mulpd	r8, [rdi+off1]		;; B1 = B1 * premul_imag (new I1)

	movapd	r3, [rdi+off2+16]	;; premul_real/premul_imag
	mulpd	r3, r2		 	;; A2 = R2 * premul_real/premul_imag
	movapd	r7, [rdi+off2+16]	;; premul_real/premul_imag
	mulpd	r7, r6			;; B2 = I2 * premul_real/premul_imag
	subpd	r3, r6			;; A2 = A2 - I2
	addpd	r7, r2			;; B2 = B2 + R2
	mulpd	r1, [rdi+off3]		;; A3 = A3 * premul_imag (new R3)
	mulpd	r5, [rdi+off3]		;; B3 = B3 * premul_imag (new I3)

	movapd	r2, [rdi+off4+16]	;; premul_real/premul_imag
	mulpd	r2, mem4	 	;; A4 = R4 * premul_real/premul_imag
	movapd	r6, [rdi+off4+16]	;; premul_real/premul_imag
	mulpd	r6, mem8		;; B4 = I4 * premul_real/premul_imag
	subpd	r2, mem8		;; A4 = A4 - I4
	addpd	r6, mem4		;; B4 = B4 + R4
	mulpd	r3, [rdi+off2]		;; A2 = A2 * premul_imag (new R2)
	mulpd	r2, [rdi+off4]		;; A4 = A4 * premul_imag (new R4)
	mulpd	r7, [rdi+off2]		;; B2 = B2 * premul_imag (new I2)
	mulpd	r6, [rdi+off4]		;; B4 = B4 * premul_imag (new I4)

	subpd	r4, r1			;; R1 = R1 - R3 (new R3)
	multwo	r1			;; R3 = R3 * 2
	xprefetchw [pre1]
	subpd	r8, r5			;; I1 = I1 - I3 (new I3)
	multwo	r5			;; I3 = I3 * 2
	subpd	r3, r2			;; R2 = R2 - R4 (new R4)
	multwo	r2			;; R4 = R4 * 2
	subpd	r7, r6			;; I2 = I2 - I4 (new I4)
	multwo	r6			;; I4 = I4 * 2
	addpd	r1, r4			;; R3 = R1 + R3 (new R1)
	addpd	r2, r3			;; R4 = R2 + R4 (new R2)
	xprefetchw [pre1][pre2]
	addpd	r5, r8			;; I3 = I1 + I3 (new I1)
	addpd	r6, r7			;; I4 = I2 + I4 (new I2)

	subpd	r4, r7			;; R3 = R3 - I4 (final R3)
	movapd	dest3, r4
	multwo	r7			;; R2 = R2 * 2
	subpd	r8, r3			;; I3 = I3 - R4 (final I4)
	multwo	r3
	subpd	r1, r2			;; R1 = R1 - R2 (final R2)
	multwo	r2
	subpd	r5, r6			;; I1 = I1 - I2 (final I2)
	multwo	r6
	addpd	r7, r4			;; I4 = R3 + I4 (final R4)
	addpd	r3, r8			;; R4 = I3 + R4 (final I3)
	addpd	r2, r1			;; R2 = R1 + R2 (final R1)
	addpd	r6, r5			;; I2 = I1 + I2 (final I1)
	ENDM


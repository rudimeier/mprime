; Copyright 2011-2012 - Mersenne Research, Inc.  All rights reserved
; Author:  George Woltman
; Email: woltman@alum.mit.edu
;

;; Include other macro files needed by the AVX versions of our primarily radix-4 traditional FFTs
;; Only the 64-bit macros are heavily optimized.  We figure that anyone running AVX CPUs is likely
;; to run a 64-bit operating system.

INCLUDE yr3.mac
INCLUDE yr5.mac
INCLUDE yr7.mac
INCLUDE yr8.mac

;;
;;
;; All new macros for version 27 of gwnum where we do a very traditional, primarily
;; radix-4, FFT.  The forward FFT macros multiply by the sin/cos values at the end
;; of the macro and the inverse FFTs multiply by the sin/cos values at the start of
;; the macro.  We also implement the Daniel J. Bernstein (DJB) "exponent-1" idea to
;; save sin/cos memory.
;;
;;

;; Note that macros have funny abbreviations at the front.  Here is
;; how to decipher most of it (ex: yr4_rb4cl_four_complex_djbfft):
;;
;; yr4_	- all macros in this file start with this.  y=AVX, r4=radix-4
;; f - first levels (uses rbx as an offset to source)
;; e - load sin/cos data from an earlier bigger sin/cos table (only needed when using broadcast)
;; r - load sin/cos data from a slightly larger real sin/cos table
;; b - load compressed sin/cos data using broadcast instructions
;; s - swizzle the outputs (fft) or swizzle the inputs (unfft)
;; g - takes separate destination register
;; 4cl_ - operates on 4 cache lines
;; 2sc_ - uses two register pointers to retrieve the sin/cos data
;; csc_ - uses one combined sin/cos register pointer (ptr usable by four complex and eight reals)
;;

;; In forward FFTs,
;;    input values are R1+R5i, R2+R6i, R3+R7i, R4+R8i
;;    output values are R1+R2i, R3+R4i, R5+R6i, R7+R8i

;; In inverse FFTs,
;;    input values are R1+R2i, R3+R4i, R5+R6i, R7+R8i
;;    output values are R1+R5i, R2+R6i, R3+R7i, R4+R8i

;;
;; ************************************* four-complex-djbfft variants ******************************************
;;
;; Macros to do Daniel J. Bernstein's exponent-1 butterflies.  The
;; difference with a standard four-complex-fft is in the postmultiply step.
;; Instead of multiplying by w^2x, w^x, w^3x, we multiply by w^2x, w^x, w^-x.
;;

; Basic four-complex DJB FFT building block
yr4_4cl_four_complex_djbfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_4c_djbfft_cmn srcreg,srcinc,0,d1,d2,noexec,screg,screg+64,32,noexec,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like the 4cl version except vbroadcastsd is used to reduce sin/cos data
yr4_b4cl_four_complex_djbfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_4c_djbfft_cmn srcreg,srcinc,0,d1,d2,exec,screg,screg+16,8,noexec,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like b4cl but extracts the sin/cos data to broadcast from an earlier level's uncompressed sin/cos data
yr4_eb4cl_four_complex_djbfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_4c_djbfft_cmn srcreg,srcinc,0,d1,d2,exec,screg,screg+64,32,noexec,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like b4cl but extracts the sin/cos data to broadcast from the eight-real/four_complex sin/cos table
yr4_rb4cl_four_complex_djbfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_4c_djbfft_cmn srcreg,srcinc,0,d1,d2,exec,screg+8,screg+80,40,noexec,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like 4cl but swizzles the outputs.  Used in the middle levels of pass 2.
yr4_s4cl_four_complex_djbfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_4c_djbfft_cmn srcreg,srcinc,0,d1,d2,noexec,screg,screg+64,32,exec,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like s4cl but uses rbx to get input data.  Used in the first levels of pass 2.
yr4_fs4cl_four_complex_djbfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_4c_djbfft_cmn srcreg,srcinc,rbx,d1,d2,noexec,screg,screg+64,32,exec,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Common macro to get the four complex djbfft job done

yr4_4c_djbfft_cmn MACRO srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,swiz,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd	ymm0, [srcreg+srcoff]		;; R1
	vmovapd	ymm6, [srcreg+srcoff+d2]	;; R3
	vaddpd	ymm2, ymm0, ymm6		;; R1 + R3 (new R1)		; 1-3

	vmovapd	ymm1, [srcreg+srcoff+d1]	;; R2
	vmovapd	ymm7, [srcreg+srcoff+d2+d1]	;; R4
	vaddpd	ymm3, ymm1, ymm7		;; R2 + R4 (new R2)		; 2-4

	vsubpd	ymm0, ymm0, ymm6		;; R1 - R3 (new R3)		; 3-5
	vsubpd	ymm1, ymm1, ymm7		;; R2 - R4 (new R4)		; 4-6

	vmovapd	ymm4, [srcreg+srcoff+32]	;; I1
	vmovapd	ymm7, [srcreg+srcoff+d2+32]	;; I3
	vaddpd	ymm6, ymm4, ymm7		;; I1 + I3 (new I1)		; 5-7

	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	ymm5, ymm2, ymm3		;; R1 - R2 (final R2)		; 6-8
	vaddpd	ymm2, ymm2, ymm3		;; R1 + R2 (final R1)		; 7-9

	vmovapd	[srcreg], ymm2			;; Save R1

	vmovapd	ymm2, [srcreg+srcoff+d1+32]	;; I2
	vaddpd	ymm3, ymm2, [srcreg+srcoff+d2+d1+32] ;; I2 + I4 (new I2)	; 8-10
	vsubpd	ymm2, ymm2, [srcreg+srcoff+d2+d1+32] ;; I2 - I4 (new I4)	; 9-11

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm4, ymm4, ymm7		;; I1 - I3 (new I3)		; 10-12

	vsubpd	ymm7, ymm6, ymm3		;; I1 - I2 (final I2)		; 11-13
	vaddpd	ymm6, ymm6, ymm3		;; I1 + I2 (final I1)		; 12-14

	vmovapd	[srcreg+32], ymm6		;; Save I1

	vsubpd	ymm3, ymm0, ymm2		;; R3 - I4 (final R3)		; 13-15
	vaddpd	ymm6, ymm4, ymm1		;; I3 + R4 (final I3)		; 14-16

	L1prefetchw srcreg+d2+L1pd, L1pt

	vaddpd	ymm0, ymm0, ymm2		;; R3 + I4 (final R4)		; 15-17
	vsubpd	ymm4, ymm4, ymm1		;; I3 - R4 (final I4)		; 16-18

no bcast vmovapd ymm1, [screg2+32]		;; cosine/sine
bcast	vbroadcastsd ymm1, Q [screg2+scoff]	;; cosine/sine
	vmulpd	ymm2, ymm5, ymm1		;; A2 = R2 * cosine/sine	;  9-13
	vsubpd	ymm2, ymm2, ymm7		;; A2 = A2 - I2			; 17-19

	vmulpd	ymm7, ymm7, ymm1		;; B2 = I2 * cosine/sine	; 14-18
	vaddpd	ymm7, ymm7, ymm5		;; B2 = B2 + R2			; 19-21

no bcast vmovapd ymm1, [screg1+32]		;; cosine/sine
bcast	vbroadcastsd ymm1, Q [screg1+scoff]	;; cosine/sine
	vmulpd	ymm5, ymm3, ymm1		;; A3 = R3 * cosine/sine	; 16-20
	vsubpd	ymm5, ymm5, ymm6		;; A3 = A3 - I3			; 21-23

	vmulpd	ymm6, ymm6, ymm1		;; B3 = I3 * cosine/sine	; 17-21
	vaddpd	ymm6, ymm6, ymm3		;; B3 = B3 + R3			; 22-24

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vmulpd	ymm3, ymm0, ymm1		;; A4 = R4 * cosine/sine	; 18-22
	vaddpd	ymm3, ymm3, ymm4		;; A4 = A4 + I4			; 23-25

	vmulpd	ymm4, ymm4, ymm1		;; B4 = I4 * cosine/sine	; 19-23
	vsubpd	ymm4, ymm4, ymm0		;; B4 = B4 - R4			; 24-26

no bcast vmovapd ymm1, [screg2]			;; Sine
bcast	vbroadcastsd ymm1, Q [screg2]		;; Sine
	vmulpd	ymm2, ymm2, ymm1		;; A2 = A2 * sine (final R2)	; 20-24
	vmulpd	ymm7, ymm7, ymm1		;; B2 = B2 * sine (final I2)	; 22-26

no bcast vmovapd ymm1, [screg1]			;; Sine
bcast	vbroadcastsd ymm1, Q [screg1]		;; Sine
	vmulpd	ymm5, ymm5, ymm1		;; A3 = A3 * sine (final R3)	; 24-28
	vmulpd	ymm6, ymm6, ymm1		;; B3 = B3 * sine (final I3)	; 25-29

	vmulpd	ymm3, ymm3, ymm1		;; A4 = A4 * sine (final R4)	; 26-30
	vmulpd	ymm4, ymm4, ymm1		;; B4 = B4 * sine (final I4)	; 27-31

	;; Non-swizzling save

;;	vmovapd	[srcreg], ymm0			;; Save R1
;;	vmovapd	[srcreg+32], ymm0		;; Save I1
no swiz	vmovapd	[srcreg+d1], ymm2		;; Save R2
no swiz	vmovapd	[srcreg+d1+32], ymm7		;; Save I2
no swiz	vmovapd	[srcreg+d2], ymm5		;; Save R3
no swiz	vmovapd	[srcreg+d2+32], ymm6		;; Save I3
no swiz	vmovapd	[srcreg+d2+d1], ymm3		;; Save R4
no swiz	vmovapd	[srcreg+d2+d1+32], ymm4		;; Save I4

	;; Swizzle using 8 shufpds and 8 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

swiz	vmovapd	ymm0, [srcreg]			;; Reload saved R1
swiz	vshufpd	ymm1, ymm0, ymm2, 0		;; Shuffle R1 and R2 to create R1/R2 low
swiz	vshufpd	ymm0, ymm0, ymm2, 15		;; Shuffle R1 and R2 to create R1/R2 hi

swiz	vshufpd	ymm2, ymm5, ymm3, 0		;; Shuffle R3 and R4 to create R3/R4 low
swiz	vshufpd	ymm5, ymm5, ymm3, 15		;; Shuffle R3 and R4 to create R3/R4 hi

swiz	vperm2f128 ymm3, ymm1, ymm2, 32		;; Shuffle R1/R2 low and R3/R4 low (new R1)
swiz	vperm2f128 ymm1, ymm1, ymm2, 49		;; Shuffle R1/R2 low and R3/R4 low (new R3)

swiz	vperm2f128 ymm2, ymm0, ymm5, 32		;; Shuffle R1/R2 hi and R3/R4 hi (new R2)
swiz	vperm2f128 ymm0, ymm0, ymm5, 49		;; Shuffle R1/R2 hi and R3/R4 hi (new R4)

swiz	vmovapd	ymm5, [srcreg+32]		;; Reload saved I1

swiz	vmovapd	[srcreg], ymm3			;; Save R1
swiz	vmovapd	[srcreg+d2], ymm1		;; Save R3
swiz	vmovapd	[srcreg+d1], ymm2		;; Save R2
swiz	vmovapd	[srcreg+d2+d1], ymm0		;; Save R4

swiz	vshufpd	ymm0, ymm5, ymm7, 0		;; Shuffle I1 and I2 to create I1/I2 low
swiz	vshufpd	ymm5, ymm5, ymm7, 15		;; Shuffle I1 and I2 to create I1/I2 hi

swiz	vshufpd	ymm7, ymm6, ymm4, 0		;; Shuffle I3 and I4 to create I3/I4 low
swiz	vshufpd	ymm6, ymm6, ymm4, 15		;; Shuffle I3 and I4 to create I3/I4 hi

swiz	vperm2f128 ymm4, ymm0, ymm7, 32		;; Shuffle I1/I2 low and I3/I4 low (new I1)
swiz	vperm2f128 ymm0, ymm0, ymm7, 49		;; Shuffle I1/I2 low and I3/I4 low (new I3)

swiz	vperm2f128 ymm7, ymm5, ymm6, 32		;; Shuffle I1/I2 hi and I3/I4 hi (new I2)
swiz	vperm2f128 ymm5, ymm5, ymm6, 49		;; Shuffle I1/I2 hi and I3/I4 hi (new I4)

swiz	vmovapd	[srcreg+32], ymm4		;; Save I1
swiz	vmovapd	[srcreg+d2+32], ymm0		;; Save I3
swiz	vmovapd	[srcreg+d1+32], ymm7		;; Save I2
swiz	vmovapd	[srcreg+d2+d1+32], ymm5		;; Save I4

	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; 64-bit version

IFDEF X86_64

yr4_4c_djbfft_cmn MACRO srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,swiz,screg,scinc,maxrpt,L1pt,L1pd
no swiz yr4_4c_djbfft_noswiz_cmn srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,maxrpt,L1pt,L1pd
swiz	yr4_4c_djbfft_swiz_cmn srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

yr4_4c_djbfft_noswiz_cmn MACRO srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	yr4_4c_djbfft_noswiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 2,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 3,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 4,exec,exec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 5,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 5*srcinc
	bump	screg, 5*scinc
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	yr4_4c_djbfft_noswiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 2,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 3,exec,exec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 4,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	screg, 4*scinc
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	yr4_4c_djbfft_noswiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 2,exec,exec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 3,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	screg, 3*scinc
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	yr4_4c_djbfft_noswiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 1,exec,exec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 2,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	screg, 2*scinc
	ELSE
	yr4_4c_djbfft_noswiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 0,noexec,exec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_noswiz_unroll 1,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDIF
	ENDM

yr4_4c_djbfft_noswiz_unroll MACRO iter,prev,this,next,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<ymm0>
y1	TEXTEQU	<ymm1>
y2	TEXTEQU	<ymm2>
y3	TEXTEQU	<ymm3>
y4	TEXTEQU	<ymm4>
y5	TEXTEQU	<ymm5>
y6	TEXTEQU	<ymm6>
y7	TEXTEQU	<ymm7>
y8	TEXTEQU	<ymm8>
y9	TEXTEQU	<ymm9>
y10	TEXTEQU	<ymm10>
y11	TEXTEQU	<ymm11>
y12	TEXTEQU	<ymm12>
y13	TEXTEQU	<ymm13>
y14	TEXTEQU	<ymm14>
y15	TEXTEQU	<ymm15>
	ENDIF

;; On later calls, previous R3,R4,I3,B4,A2,B2,sine1,sine2 will be in y0-7.  This R1,R3,I2,I4,I1,I3 will be in y8-13.
;; The remaining registers are free.

this	vsubpd	y14, y8, y9				;; R1 - R3 (new R3)		; 1-3
prev	vmulpd	y3, y3, y6				;; B4 = B4 * sine (final I4)	; 1-5
this	vmovapd	y6, [srcreg+iter*srcinc+srcoff+d1]	;; R2

this	vaddpd	y8, y8, y9				;; R1 + R3 (new R1)		; 2-4
prev	vmulpd	y4, y4, y7				;; A2 = A2 * sine (final R2)	; 2-6
this	vmovapd	y9, [srcreg+iter*srcinc+srcoff+d2+d1]	;; R4

this	vsubpd	y15, y10, y11				;; I2 - I4 (new I4)		; 3-5
prev	vmulpd	y5, y5, y7				;; B2 = B2 * sine (final I2)	; 3-7
this no bcast vmovapd y7, [screg1+iter*scinc+32]	;; cosine/sine
this bcast vbroadcastsd y7, Q [screg1+iter*scinc+scoff]	;; cosine/sine
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2], y0		;; Save R3			; 3

this	vaddpd	y10, y10, y11				;; I2 + I4 (new I2)		; 4-6
this no bcast vmovapd y11, [screg2+iter*scinc+32]	;; cosine/sine
this bcast vbroadcastsd y11, Q [screg2+iter*scinc+scoff] ;; cosine/sine
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+d1], y1	;; Save R4			; 4

this	vsubpd	y1, y12, y13				;; I1 - I3 (new I3)		; 5-7
this no bcast vmovapd y0, [screg1+iter*scinc]		;; Sine
this bcast vbroadcastsd y0, Q [screg1+iter*scinc]	;; Sine
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+32], y2	;; Save I3			; 5

this	vaddpd	y12, y12, y13				;; I1 + I3 (new I1)		; 6-8
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+d1+32], y3	;; Save I4			; 6
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

this	vsubpd	y3, y6, y9				;; R2 - R4 (new R4)		; 7-9
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1], y4		;; Save R2			; 7

this	vaddpd	y6, y6, y9				;; R2 + R4 (new R2)		; 8-10
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1+32], y5	;; Save I2			; 8

this	vsubpd	y5, y14, y15				;; R3 - I4 (final R3)		; 9-11
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	vaddpd	y14, y14, y15				;; R3 + I4 (final R4)		; 10-12

this	vaddpd	y15, y1, y3				;; I3 + R4 (final I3)		; 11-13

this	vsubpd	y1, y1, y3				;; I3 - R4 (final I4)		; 12-14
this	vmulpd	y3, y5, y7				;; A3 = R3 * cosine/sine	; 12-16
this	L1prefetchw srcreg+iter*srcinc+d2+L1pd, L1pt

this	vsubpd	y9, y8, y6				;; R1 - R2 (final R2)		; 13-15
this	vmulpd	y4, y14, y7				;; A4 = R4 * cosine/sine	; 13-17

this	vsubpd	y13, y12, y10				;; I1 - I2 (final I2)		; 14-16
this	vmulpd	y2, y15, y7				;; B3 = I3 * cosine/sine	; 14-18
this next yloop_unrolled_one

this	vaddpd	y8, y8, y6				;; R1 + R2 (final R1)		; 15-17
this	vmulpd	y7, y1, y7				;; B4 = I4 * cosine/sine	; 15-19
next	vmovapd	y6, [srcreg+(iter+1)*srcinc+srcoff]	;; R1

this	vaddpd	y12, y12, y10				;; I1 + I2 (final I1)		; 16-18
this	vmulpd	y10, y9, y11				;; A2 = R2 * cosine/sine	; 16-20
this	L1prefetchw srcreg+iter*srcinc+d2+d1+L1pd, L1pt

this	vsubpd	y3, y3, y15				;; A3 = A3 - I3			; 17-19
this	vmulpd	y11, y13, y11				;; B2 = I2 * cosine/sine	; 17-21
next	vmovapd	y15, [srcreg+(iter+1)*srcinc+srcoff+d2]	;; R3

this	vaddpd	y4, y4, y1				;; A4 = A4 + I4			; 18-20
this no bcast vmovapd y1, [screg2+iter*scinc]		;; Sine
this bcast	vbroadcastsd y1, Q [screg2+iter*scinc]	;; Sine
this	vmovapd	[srcreg+iter*srcinc], y8		;; Save R1			; 18

this	vaddpd	y2, y2, y5				;; B3 = B3 + R3			; 19-21
next	vmovapd	y5, [srcreg+(iter+1)*srcinc+srcoff+d1+32] ;; I2
this	vmovapd	[srcreg+iter*srcinc+32], y12		;; Save I1			; 19

this	vsubpd	y7, y7, y14				;; B4 = B4 - R4			; 20-22
this	vmulpd	y3, y3, y0				;; A3 = A3 * sine (final R3)	; 20-24
next	vmovapd	y14, [srcreg+(iter+1)*srcinc+srcoff+d2+d1+32] ;; I4

this	vsubpd	y10, y10, y13				;; A2 = A2 - I2			; 21-23
this	vmulpd	y4, y4, y0				;; A4 = A4 * sine (final R4)	; 21-25
next	vmovapd	y13, [srcreg+(iter+1)*srcinc+srcoff+32]	;; I1

this	vaddpd	y11, y11, y9				;; B2 = B2 + R2			; 22-24
this	vmulpd	y2, y2, y0				;; B3 = B3 * sine (final I3)	; 22-26
next	vmovapd	y9, [srcreg+(iter+1)*srcinc+srcoff+d2+32] ;; I3

;; Shuffle register assignments so that next call has R3,R4,I3,B4,A2,B2,sine1,sine2 in y0-7 and next R1,R3,I2,I4,I1,I3 in y8-13.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y3
y3	TEXTEQU	y7
y7	TEXTEQU	y1
y1	TEXTEQU	y4
y4	TEXTEQU	y10
y10	TEXTEQU	y5
y5	TEXTEQU	y11
y11	TEXTEQU	y14
y14	TEXTEQU	y8
y8	TEXTEQU	y6
y6	TEXTEQU	ytmp
ytmp	TEXTEQU y9
y9	TEXTEQU	y15
y15	TEXTEQU	y12
y12	TEXTEQU	y13
y13	TEXTEQU ytmp
	ENDM


yr4_4c_djbfft_swiz_cmn MACRO srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	yr4_4c_djbfft_swiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 2,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 3,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 4,exec,exec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 5,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 5*srcinc
	bump	screg, 5*scinc
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	yr4_4c_djbfft_swiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 2,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 3,exec,exec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 4,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	screg, 4*scinc
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	yr4_4c_djbfft_swiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 2,exec,exec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 3,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	screg, 3*scinc
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	yr4_4c_djbfft_swiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 1,exec,exec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 2,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	screg, 2*scinc
	ELSE
	yr4_4c_djbfft_swiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 0,noexec,exec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbfft_swiz_unroll 1,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDIF
	ENDM

yr4_4c_djbfft_swiz_unroll MACRO iter,prev,this,next,srcreg,srcinc,srcoff,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<ymm0>
y1	TEXTEQU	<ymm1>
y2	TEXTEQU	<ymm2>
y3	TEXTEQU	<ymm3>
y4	TEXTEQU	<ymm4>
y5	TEXTEQU	<ymm5>
y6	TEXTEQU	<ymm6>
y7	TEXTEQU	<ymm7>
y8	TEXTEQU	<ymm8>
y9	TEXTEQU	<ymm9>
y10	TEXTEQU	<ymm10>
y11	TEXTEQU	<ymm11>
y12	TEXTEQU	<ymm12>
y13	TEXTEQU	<ymm13>
y14	TEXTEQU	<ymm14>
y15	TEXTEQU	<ymm15>
	ENDIF

;; On later calls, previous R1,I1,R3,R4,R2,B3,B4,B2,sine1,sine2 will be in y0-9.  This R1,R3,I2,I4 will be in y10-13.
;; The remaining registers are free.

this	vsubpd	y14, y10, y11			;; R1 - R3 (new R3)			; 1-3
prev	vmulpd	y5, y5, y8			;; B3 = B3 * sine (final I3)		; 23-27
this	vmovapd	y15, [srcreg+iter*srcinc+srcoff+d1] ;; R2

this	vaddpd	y10, y10, y11			;; R1 + R3 (new R1)			; 2-4
prev	vshufpd	y11, y2, y3, 0			;; Shuffle R3 and R4 to create R3/R4 low ; 24
prev	vmulpd	y6, y6, y8			;; B4 = B4 * sine (final I4)		; 24-28
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

this	vsubpd	y8, y12, y13			;; I2 - I4 (new I4)			; 3-5
prev	vshufpd	y2, y2, y3, 15			;; Shuffle R3 and R4 to create R3/R4 hi	; 25
prev	vmulpd	y7, y7, y9			;; B2 = B2 * sine (final I2)		; 25-29
this	vmovapd	y3, [srcreg+iter*srcinc+srcoff+d2+d1] ;; R4

this	vaddpd	y9, y15, y3			;; R2 + R4 (new R2)			; 4-6
this	vsubpd	y15, y15, y3			;; R2 - R4 (new R4)			; 5-7
prev	vshufpd	y3, y0, y4, 0			;; Shuffle R1 and R2 to create R1/R2 low ; 26
prev	vshufpd	y0, y0, y4, 15			;; Shuffle R1 and R2 to create R1/R2 hi	; 27
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

prev	vperm2f128 y4, y3, y11, 32		;; Shuffle R1/R2 low and R3/R4 low (new R1) ; 28-29
prev	vmovapd	[srcreg+(iter-1)*srcinc], y4	;; Save R1				; 30
this	vsubpd	y4, y14, y8			;; R3 - I4 (final R3)			; 6-8

this	vaddpd	y14, y14, y8			;; R3 + I4 (final R4)			; 7-9
prev	vperm2f128 y3, y3, y11, 49		;; Shuffle R1/R2 low and R3/R4 low (new R3) ; 29-30

this	vsubpd	y11, y10, y9			;; R1 - R2 (final R2)			; 8-10
prev	vperm2f128 y8, y0, y2, 32		;; Shuffle R1/R2 hi and R3/R4 hi (new R2) ; 30-31

prev	vperm2f128 y0, y0, y2, 49		;; Shuffle R1/R2 hi and R3/R4 hi (new R4) ; 31-32
this	vmovapd	y2, [srcreg+iter*srcinc+srcoff+32] ;; I1
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2], y3	;; Save R3				; 31
this	vmovapd	y3, [srcreg+iter*srcinc+srcoff+d2+32] ;; I3
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1], y8	;; Save R2				; 32
this	vsubpd	y8, y2, y3			;; I1 - I3 (new I3)			; 9-11

this	vaddpd	y12, y12, y13			;; I2 + I4 (new I2)			; 10-12
this no bcast vmovapd y13, [screg1+iter*scinc+32] ;; cosine/sine
this bcast vbroadcastsd y13, Q [screg1+iter*scinc+scoff] ;; cosine/sine
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+d1], y0 ;; Save R4				; 33
this	vmulpd	y0, y4, y13			;; A3 = R3 * cosine/sine		; 10-14

this	vaddpd	y2, y2, y3			;; I1 + I3 (new I1)			; 11-13
prev	vshufpd	y3, y5, y6, 0			;; Shuffle I3 and I4 to create I3/I4 low ; 33
prev	vshufpd	y5, y5, y6, 15			;; Shuffle I3 and I4 to create I3/I4 hi	; 34
prev	vshufpd	y6, y1, y7, 0			;; Shuffle I1 and I2 to create I1/I2 low ; 35
prev	vshufpd	y1, y1, y7, 15			;; Shuffle I1 and I2 to create I1/I2 hi	; 36
prev	vperm2f128 y7, y6, y3, 32		;; Shuffle I1/I2 low and I3/I4 low (new I1) ; 37-38
prev	vmovapd	[srcreg+(iter-1)*srcinc+32], y7	;; Save I1				; 39
this	vmulpd	y7, y14, y13			;; A4 = R4 * cosine/sine		; 11-15

prev	vperm2f128 y6, y6, y3, 49		;; Shuffle I1/I2 low and I3/I4 low (new I3) ; 38-39
this	vaddpd	y3, y8, y15			;; I3 + R4 (final I3)			; 12-14
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+32], y6 ;; Save I3				; 40
this no bcast vmovapd y6, [screg2+iter*scinc+32] ;; cosine/sine
this bcast vbroadcastsd y6, Q [screg2+iter*scinc+scoff]	;; cosine/sine
this	vsubpd	y8, y8, y15			;; I3 - R4 (final I4)			; 13-15
prev	vperm2f128 y15, y1, y5, 32		;; Shuffle I1/I2 hi and I3/I4 hi (new I2) ; 39-40
prev	vperm2f128 y1, y1, y5, 49		;; Shuffle I1/I2 hi and I3/I4 hi (new I4) ; 40-41
this	vmulpd	y5, y11, y6			;; A2 = R2 * cosine/sine		; 12-16

prev	vmovapd	[srcreg+(iter-1)*srcinc+d1+32], y15 ;; Save I2				; 41
this	vsubpd	y15, y2, y12			;; I1 - I2 (final I2)			; 14-16

this	vsubpd	y0, y0, y3			;; A3 = A3 - I3				; 15-17
this	vmulpd	y3, y3, y13			;; B3 = I3 * cosine/sine		; 15-19
this	L1prefetchw srcreg+iter*srcinc+d2+L1pd, L1pt

this	vaddpd	y7, y7, y8			;; A4 = A4 + I4				; 16-18
this	vmulpd	y8, y8, y13			;; B4 = I4 * cosine/sine		; 16-20
this no bcast vmovapd y13, [screg1+iter*scinc]	;; Sine
this bcast vbroadcastsd y13, Q [screg1+iter*scinc] ;; Sine

this	vsubpd	y5, y5, y15			;; A2 = A2 - I2				; 17-19
this	vmulpd	y15, y15, y6			;; B2 = I2 * cosine/sine		; 17-21
this no bcast vmovapd y6, [screg2+iter*scinc]	;; Sine
this bcast vbroadcastsd y6, Q [screg2+iter*scinc] ;; Sine

this	vaddpd	y10, y10, y9			;; R1 + R2 (final R1)			; 18-20
this	vmulpd	y0, y0, y13			;; A3 = A3 * sine (final R3)		; 18-22
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+d1+32], y1 ;; Save I4			; 42
this	L1prefetchw srcreg+iter*srcinc+d2+d1+L1pd, L1pt

this	vaddpd	y2, y2, y12			;; I1 + I2 (final I1)			; 19-21
this	vmulpd	y7, y7, y13			;; A4 = A4 * sine (final R4)		; 19-23
next	vmovapd	y12, [srcreg+(iter+1)*srcinc+srcoff] ;; R1

this	vaddpd	y3, y3, y4			;; B3 = B3 + R3				; 20-22
this	vmulpd	y5, y5, y6			;; A2 = A2 * sine (final R2)		; 20-24
next	vmovapd	y4, [srcreg+(iter+1)*srcinc+srcoff+d2] ;; R3

this	vsubpd	y8, y8, y14			;; B4 = B4 - R4				; 21-23
next	vmovapd	y14, [srcreg+(iter+1)*srcinc+srcoff+d1+32] ;; I2
this next yloop_unrolled_one

this	vaddpd	y15, y15, y11			;; B2 = B2 + R2				; 22-24
next	vmovapd	y11, [srcreg+(iter+1)*srcinc+srcoff+d2+d1+32] ;; I4

;; Shuffle register assignments so that next call has R1,I1,R3,R4,R2,B3,B4,B2,sine1,sine2 in y0-9 and next R1,R3,I2,I4 will be in y10-13.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y10
y10	TEXTEQU	y12
y12	TEXTEQU	y14
y14	TEXTEQU	y9
y9	TEXTEQU	y6
y6	TEXTEQU	y8
y8	TEXTEQU	y13
y13	TEXTEQU	y11
y11	TEXTEQU	y4
y4	TEXTEQU	y5
y5	TEXTEQU y3
y3	TEXTEQU	y7
y7	TEXTEQU	y15
y15	TEXTEQU	y1
y1	TEXTEQU	y2
y2	TEXTEQU ytmp
	ENDM

ENDIF


;;
;; ************************************* four-complex-djbunfft variants ******************************************
;;

; Basic four-complex DJB inverse FFT building block
yr4_4cl_four_complex_djbunfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_4c_djbunfft_cmn srcreg,srcinc,d1,d2,noexec,screg,screg+64,32,noexec,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like the 4cl version except vbroadcastsd is used to reduce sin/cos data
yr4_b4cl_four_complex_djbunfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_4c_djbunfft_cmn srcreg,srcinc,d1,d2,exec,screg,screg+16,8,noexec,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like b4cl but extracts the sin/cos data to broadcast from a earlier level's uncompressed sin/cos data
yr4_eb4cl_four_complex_djbunfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_4c_djbunfft_cmn srcreg,srcinc,d1,d2,exec,screg,screg+64,32,noexec,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like b4cl but extracts the sin/cos data to broadcast from the eight-real/four_complex sin/cos table
yr4_rb4cl_four_complex_djbunfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_4c_djbunfft_cmn srcreg,srcinc,d1,d2,exec,screg+8,screg+80,40,noexec,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like the 4cl version except the inputs are swizzled.
yr4_s4cl_four_complex_djbunfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_4c_djbunfft_cmn srcreg,srcinc,d1,d2,noexec,screg,screg+64,32,exec,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

;; Common macros to get the four complex djbunfft job done

yr4_4c_djbunfft_cmn MACRO srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,swiz,screg,scinc,maxrpt,L1pt,L1pd
no swiz	yr4_4c_djbunfft_noswiz srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,maxrpt,L1pt,L1pd
swiz	yr4_4c_djbunfft_swiz srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

yr4_4c_djbunfft_noswiz MACRO srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,maxrpt,L1pt,L1pd
no bcast vmovapd ymm3, [screg2+32]		;; cosine/sine
bcast	vbroadcastsd ymm3, Q [screg2+scoff]	;; cosine/sine
	vmovapd	ymm2, [srcreg+d1]		;; R2
	vmulpd	ymm6, ymm2, ymm3		;; A2 = R2 * cosine/sine		; 1-5
	vmovapd	ymm0, [srcreg+d1+32]		;; I2
	vmulpd	ymm3, ymm3, ymm0		;; B2 = I2 * cosine/sine		; 2-6

no bcast vmovapd ymm5, [screg1+32]		;; cosine/sine
bcast	vbroadcastsd ymm5, Q [screg1+scoff]	;; cosine/sine
	vmovapd	ymm4, [srcreg+d2]		;; R3
	vmulpd	ymm7, ymm4, ymm5		;; A3 = R3 * cosine/sine		; 3-7
	vmovapd	ymm1, [srcreg+d2+32]		;; I3

	vaddpd	ymm6, ymm6, ymm0		;; A2 = A2 + I2				; 6-8

	vmulpd	ymm0, ymm1, ymm5		;; B3 = I3 * cosine/sine		; 4-8
	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	ymm3, ymm3, ymm2		;; B2 = B2 - R2				; 7-9
	vaddpd	ymm7, ymm7, ymm1		;; A3 = A3 + I3				; 8-10

	vmovapd	ymm2, [srcreg+d2+d1]		;; R4
	vmulpd	ymm1, ymm2, ymm5		;; A4 = R4 * cosine/sine		; 5-9

	vsubpd	ymm0, ymm0, ymm4		;; B3 = B3 - R3				; 9-11

	vmovapd	ymm4, [srcreg+d2+d1+32]		;; I4
	vmulpd	ymm5, ymm4, ymm5		;; B4 = I4 * cosine/sine		; 6-10

	vsubpd	ymm1, ymm1, ymm4		;; A4 = A4 - I4				; 10-12

no bcast vmovapd ymm4, [screg2]			;; Sine
bcast	vbroadcastsd ymm4, Q [screg2]		;; Sine
	vmulpd	ymm6, ymm6, ymm4		;; A2 = A2 * sine (new R2)		; 9-13
	vmulpd	ymm3, ymm3, ymm4		;; B2 = B2 * sine (new I2)		; 10-14
	L1prefetchw srcreg+d1+L1pd, L1pt

	vaddpd	ymm5, ymm5, ymm2		;; B4 = B4 + R4				; 11-13

no bcast vmovapd ymm4, [screg1]			;; Sine
bcast	vbroadcastsd ymm4, Q [screg1]		;; Sine
	vmulpd	ymm7, ymm7, ymm4		;; A3 = A3 * sine (new R3)		; 11-15
	vmulpd	ymm0, ymm0, ymm4		;; B3 = B3 * sine (new I3)		; 12-16
	vmulpd	ymm1, ymm1, ymm4		;; A4 = A4 * sine (new R4)		; 13-17
	vmulpd	ymm5, ymm5, ymm4		;; B4 = B4 * sine (new I4)		; 14-18

	vmovapd	ymm2, [srcreg]			;; R1
	vaddpd	ymm4, ymm2, ymm6		;; R1 + R2 (new R1)			; 14-16
	vsubpd	ymm2, ymm2, ymm6		;; R1 - R2 (new R2)			; 15-17
	L1prefetchw srcreg+d2+L1pd, L1pt

	vaddpd	ymm6, ymm1, ymm7		;; R4 + R3 (new R3)			; 16-18
	vsubpd	ymm1, ymm1, ymm7		;; R4 - R3 (new I4)			; 17-19

	vsubpd	ymm7, ymm0, ymm5		;; I3 - I4 (new R4)			; 18-20
	vaddpd	ymm0, ymm0, ymm5		;; I3 + I4 (new I3)			; 19-21

	vsubpd	ymm5, ymm4, ymm6		;; R1 - R3 (final R3)			; 20-22
	vaddpd	ymm4, ymm4, ymm6		;; R1 + R3 (final R1)			; 21-23

	vmovapd	ymm6, [srcreg+32]		;; I1
	vmovapd	[srcreg], ymm4			;; Store R1

	vsubpd	ymm4, ymm6, ymm3		;; I1 - I2 (new I2)			; 22-24
	vaddpd	ymm6, ymm6, ymm3		;; I1 + I2 (new I1)			; 23-25
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm3, ymm2, ymm7		;; R2 - R4 (final R4)			; 24-26
	vaddpd	ymm2, ymm2, ymm7		;; R2 + R4 (final R2)			; 25-27

	vsubpd	ymm7, ymm4, ymm1		;; I2 - I4 (final I4)			; 26-28
	vaddpd	ymm4, ymm4, ymm1		;; I2 + I4 (final I2)			; 27-29

	vsubpd	ymm1, ymm6, ymm0		;; I1 - I3 (final I3)			; 28-30
	vaddpd	ymm6, ymm6, ymm0		;; I1 + I3 (final I1)			; 29-31

;;	vmovapd	[srcreg], ymm			;; Save R1
	vmovapd	[srcreg+32], ymm6		;; Save I1
	vmovapd	[srcreg+d1], ymm2		;; Save R2
	vmovapd	[srcreg+d1+32], ymm4		;; Save I2
	vmovapd	[srcreg+d2], ymm5		;; Save R3
	vmovapd	[srcreg+d2+32], ymm1		;; Save I3
	vmovapd	[srcreg+d2+d1], ymm3		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm7		;; Save I4
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

yr4_4c_djbunfft_swiz MACRO srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,maxrpt,L1pt,L1pd

	;; Swizzle using 8 shufpds and 8 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm1, [srcreg]			;; R1
	vmovapd	ymm7, [srcreg+d1]		;; R2
	vshufpd	ymm0, ymm1, ymm7, 15		;; Shuffle R1 and R2 to create R1/R2 hi			; 1
	vshufpd	ymm1, ymm1, ymm7, 0		;; Shuffle R1 and R2 to create R1/R2 low		; 2

	vmovapd	ymm2, [srcreg+d2]		;; R3
	vmovapd	ymm7, [srcreg+d2+d1]		;; R4
	vshufpd	ymm3, ymm2, ymm7, 15		;; Shuffle R3 and R4 to create R3/R4 hi			; 3
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle R3 and R4 to create R3/R4 low		; 4

	vperm2f128 ymm4, ymm0, ymm3, 32		;; Shuffle R1/R2 hi and R3/R4 hi (new R2)		; 5-6
	vperm2f128 ymm0, ymm0, ymm3, 49		;; Shuffle R1/R2 hi and R3/R4 hi (new R4)		; 6-7
	L1prefetchw srcreg+L1pd, L1pt

	vperm2f128 ymm3, ymm1, ymm2, 32		;; Shuffle R1/R2 low and R3/R4 low (new R1)		; 7-8
	vperm2f128 ymm1, ymm1, ymm2, 49		;; Shuffle R1/R2 low and R3/R4 low (new R3)		; 8-9

	vmovapd	ymm6, [srcreg+32]		;; I1
	vmovapd	ymm2, [srcreg+d1+32]		;; I2
	vshufpd	ymm5, ymm6, ymm2, 15		;; Shuffle I1 and I2 to create I1/I2 hi
	vshufpd	ymm6, ymm6, ymm2, 0		;; Shuffle I1 and I2 to create I1/I2 low

	vmovapd	[srcreg], ymm3			;; Temporarily save new R1

	vmovapd	ymm2, [srcreg+d2+32]		;; I3
	vmovapd	ymm7, [srcreg+d2+d1+32]		;; I4
	vshufpd	ymm3, ymm2, ymm7, 15		;; Shuffle I3 and I4 to create I3/I4 hi
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle I3 and I4 to create I3/I4 low

	vperm2f128 ymm7, ymm5, ymm3, 32		;; Shuffle I1/I2 hi and I3/I4 hi (new I2)
	vperm2f128 ymm5, ymm5, ymm3, 49		;; Shuffle I1/I2 hi and I3/I4 hi (new I4)
	L1prefetchw srcreg+d1+L1pd, L1pt

	vperm2f128 ymm3, ymm6, ymm2, 32		;; Shuffle I1/I2 low and I3/I4 low (new I1)
	vperm2f128 ymm6, ymm6, ymm2, 49		;; Shuffle I1/I2 low and I3/I4 low (new I3)

	vmovapd	[srcreg+32], ymm3		;; Temporarily save new I1

	vmovapd ymm3, [screg2+scoff]		;; cosine/sine
	vmulpd	ymm2, ymm4, ymm3		;; A2 = R2 * cosine/sine		; 1-5
	vmulpd	ymm3, ymm7, ymm3		;; B2 = I2 * cosine/sine		; 2-6
	vaddpd	ymm2, ymm2, ymm7		;; A2 = A2 + I2				; 6-8
	vsubpd	ymm3, ymm3, ymm4		;; B2 = B2 - R2				; 7-9

	vmovapd ymm7, [screg1+scoff]		;; cosine/sine
	vmulpd	ymm4, ymm1, ymm7		;; A3 = R3 * cosine/sine		; 3-7
	vaddpd	ymm4, ymm4, ymm6		;; A3 = A3 + I3				; 8-10
	vmulpd	ymm6, ymm6, ymm7		;; B3 = I3 * cosine/sine		; 4-8
	vsubpd	ymm6, ymm6, ymm1		;; B3 = B3 - R3				; 9-11
	L1prefetchw srcreg+d2+L1pd, L1pt

	vmulpd	ymm1, ymm0, ymm7		;; A4 = R4 * cosine/sine		; 5-9
	vmulpd	ymm7, ymm5, ymm7		;; B4 = I4 * cosine/sine		; 6-10
	vsubpd	ymm1, ymm1, ymm5		;; A4 = A4 - I4				; 10-12

	vmovapd ymm5, [screg2]			;; Sine
	vmulpd	ymm2, ymm2, ymm5		;; A2 = A2 * sine (new R2)		; 9-13
	vmulpd	ymm3, ymm3, ymm5		;; B2 = B2 * sine (new I2)		; 10-14

	vaddpd	ymm7, ymm7, ymm0		;; B4 = B4 + R4				; 11-13
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vmovapd ymm5, [screg1]			;; Sine
	vmulpd	ymm4, ymm4, ymm5		;; A3 = A3 * sine (new R3)		; 11-15
	vmulpd	ymm6, ymm6, ymm5		;; B3 = B3 * sine (new I3)		; 12-16
	vmulpd	ymm1, ymm1, ymm5		;; A4 = A4 * sine (new R4)		; 13-17
	vmulpd	ymm7, ymm7, ymm5		;; B4 = B4 * sine (new I4)		; 14-18

	vmovapd	ymm0, [srcreg]			;; Reload R1
	vaddpd	ymm5, ymm0, ymm2		;; R1 + R2 (newer R1)			; 14-16
	vsubpd	ymm0, ymm0, ymm2		;; R1 - R2 (newer R2)			; 15-17

	vaddpd	ymm2, ymm1, ymm4		;; R4 + R3 (newer R3)			; 16-18
	vsubpd	ymm1, ymm1, ymm4		;; R4 - R3 (newer I4)			; 17-19

	vsubpd	ymm4, ymm6, ymm7		;; I3 - I4 (newer R4)			; 18-20
	vaddpd	ymm6, ymm6, ymm7		;; I3 + I4 (newer I3)			; 19-21

	vsubpd	ymm7, ymm5, ymm2		;; R1 - R3 (final R3)			; 20-22
	vaddpd	ymm5, ymm5, ymm2		;; R1 + R3 (final R1)			; 21-23

	vmovapd	ymm2, [srcreg+32]		;; Reload I1
	vmovapd	[srcreg], ymm5			;; Store R1

	vsubpd	ymm5, ymm2, ymm3		;; I1 - I2 (newer I2)			; 22-24
	vaddpd	ymm2, ymm2, ymm3		;; I1 + I2 (newer I1)			; 23-25

	vsubpd	ymm3, ymm0, ymm4		;; R2 - R4 (final R4)			; 24-26
	vaddpd	ymm0, ymm0, ymm4		;; R2 + R4 (final R2)			; 25-27

	vsubpd	ymm4, ymm5, ymm1		;; I2 - I4 (final I4)			; 26-28
	vaddpd	ymm5, ymm5, ymm1		;; I2 + I4 (final I2)			; 27-29

	vsubpd	ymm1, ymm2, ymm6		;; I1 - I3 (final I3)			; 28-30
	vaddpd	ymm2, ymm2, ymm6		;; I1 + I3 (final I1)			; 29-31

	vmovapd	[srcreg+32], ymm2		;; Save I1
	vmovapd	[srcreg+d1], ymm0		;; Save R2
	vmovapd	[srcreg+d1+32], ymm5		;; Save I2
	vmovapd	[srcreg+d2], ymm7		;; Save R3
	vmovapd	[srcreg+d2+32], ymm1		;; Save I3
	vmovapd	[srcreg+d2+d1], ymm3		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm4		;; Save I4
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

IFDEF X86_64

yr4_4c_djbunfft_cmn MACRO srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,swiz,screg,scinc,maxrpt,L1pt,L1pd
no swiz yr4_4c_djbunfft_noswiz_cmn srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,maxrpt,L1pt,L1pd
swiz	yr4_4c_djbunfft_swiz_cmn srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

yr4_4c_djbunfft_noswiz_cmn MACRO srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	yr4_4c_djbunfft_noswiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 3,exec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 4,exec,exec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 5,exec,noexec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 5*srcinc
	bump	screg, 5*scinc
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	yr4_4c_djbunfft_noswiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 3,exec,exec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 4,exec,noexec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	screg, 4*scinc
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	yr4_4c_djbunfft_noswiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 2,exec,exec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 3,exec,noexec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	screg, 3*scinc
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	yr4_4c_djbunfft_noswiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 1,exec,exec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 2,exec,noexec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	screg, 2*scinc
	ELSE
	yr4_4c_djbunfft_noswiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 0,noexec,exec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_noswiz_unroll 1,exec,noexec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDIF
	ENDM

yr4_4c_djbunfft_noswiz_unroll MACRO iter,prev,this,next,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<ymm0>
y1	TEXTEQU	<ymm1>
y2	TEXTEQU	<ymm2>
y3	TEXTEQU	<ymm3>
y4	TEXTEQU	<ymm4>
y5	TEXTEQU	<ymm5>
y6	TEXTEQU	<ymm6>
y7	TEXTEQU	<ymm7>
y8	TEXTEQU	<ymm8>
y9	TEXTEQU	<ymm9>
y10	TEXTEQU	<ymm10>
y11	TEXTEQU	<ymm11>
y12	TEXTEQU	<ymm12>
y13	TEXTEQU	<ymm13>
y14	TEXTEQU	<ymm14>
y15	TEXTEQU	<ymm15>
	ENDIF

;; On later calls, previous R2,I3,I1 will be in y0-2.  This R4,R3,R2,I4,I3,I2,A4,A3,A2,B4,B3,c/s2 will be in y3-14.
;; The remaining register is free.

this	vsubpd	y9, y9, y6			;; A4 = A4 - I4 (new R4/sine)		; 1-3
this	vmulpd	y14, y8, y14			;; B2 = I2 * cosine/sine		; 1-5
this no bcast vmovapd y6, [screg2+iter*scinc]	;; Sine
this bcast vbroadcastsd y6, Q [screg2+iter*scinc] ;; Sine

this	vaddpd	y10, y10, y7			;; A3 = A3 + I3 (new R3/sine)		; 2-4
this no bcast vmovapd y7, [screg1+iter*scinc]	;; Sine
this bcast vbroadcastsd y7, Q [screg1+iter*scinc] ;; Sine

this	vaddpd	y11, y11, y8			;; A2 = A2 + I2				; 3-5
this	vmovapd	y8, [srcreg+iter*srcinc]	;; R1

this	vaddpd	y12, y12, y3			;; B4 = B4 + R4 (new I4/sine)		; 4-6
this	vmovapd	y3, [srcreg+iter*srcinc+32]	;; I1

this	vsubpd	y13, y13, y4			;; B3 = B3 - R3 (new I3/sine)		; 5-7
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1], y0	;; Save R2				; 1
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

this	vsubpd	y14, y14, y5			;; B2 = B2 - R2				; 6-8
this	vmulpd	y11, y11, y6			;; A2 = A2 * sine (new R2)		; 6-10
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+32], y1 ;; Save I3				; 2

this	vaddpd	y1, y9, y10			;; C3 = R4/sine + R3/sine (newer R3/sine) ; 7-9
prev	vmovapd	[srcreg+(iter-1)*srcinc+32], y2	;; Save I1				; 3

this	vsubpd	y9, y9, y10			;; D4 = R4/sine - R3/sine (newer I4/sine) ; 8-10
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	vsubpd	y10, y13, y12			;; C4 = I3/sine - I4/sine (newer R4/sine) ; 9-11
this	vmulpd	y14, y14, y6			;; B2 = B2 * sine (new I2)		; 9-13
next no bcast vmovapd y15, [screg1+(iter+1)*scinc+32] ;; cosine/sine
next bcast	vbroadcastsd y15, Q [screg1+(iter+1)*scinc+scoff] ;; cosine/sine

this	vaddpd	y13, y13, y12			;; D3 = I3/sine + I4/sine (newer I3/sine) ; 10-12
this	vmulpd	y1, y1, y7			;; C3 * sine (newer R3)			; 10-14
next	vmovapd	y4, [srcreg+(iter+1)*srcinc+d2+d1] ;; R4

this	vaddpd	y12, y8, y11			;; R1 + R2 (newer R1)			; 11-13
this	vmulpd	y9, y9, y7			;; D4 * sine (newer I4)			; 11-15
next	vmovapd	y0, [srcreg+(iter+1)*srcinc+d2]	;; R3

this	vsubpd	y8, y8, y11			;; R1 - R2 (newer R2)			; 12-14
this	vmulpd	y10, y10, y7			;; C4 * sine (newer R4)			; 12-16
next no bcast vmovapd y5, [screg2+(iter+1)*scinc+32] ;; cosine/sine
next bcast vbroadcastsd y5, Q [screg2+(iter+1)*scinc+scoff] ;; cosine/sine

this	vsubpd	y11, y3, y14			;; I1 - I2 (newer I2)			; 13-15
this	vmulpd	y13, y13, y7			;; D3 * sine (newer I3)			; 13-17
next	vmovapd	y2, [srcreg+(iter+1)*srcinc+d1]	;; R2

this	vaddpd	y3, y3, y14			;; I1 + I2 (newer I1)			; 14-16
this	L1prefetchw srcreg+iter*srcinc+d2+L1pd, L1pt

this	vsubpd	y14, y12, y1			;; R1 - R3 (final R3)			; 15-17
next	vmovapd	y6, [srcreg+(iter+1)*srcinc+d2+d1+32] ;; I4

this	vaddpd	y12, y12, y1			;; R1 + R3 (final R1)			; 16-18
next	vmovapd	y7, [srcreg+(iter+1)*srcinc+d2+32] ;; I3

this	vmovapd	[srcreg+iter*srcinc+d2], y14	;; Save R3				; 18
this	vsubpd	y14, y11, y9			;; I2 - I4 (final I4)			; 17-19
next	vmovapd	y1, [srcreg+(iter+1)*srcinc+d1+32] ;; I2

this	vaddpd	y11, y11, y9			;; I2 + I4 (final I2)			; 18-20
next	vmulpd	y9, y4, y15			;; A4 = R4 * cosine/sine		; 18-22
this	L1prefetchw srcreg+iter*srcinc+d2+d1+L1pd, L1pt

this	vmovapd	[srcreg+iter*srcinc], y12	;; Save R1				; 19
this	vsubpd	y12, y8, y10			;; R2 - R4 (final R4)			; 19-21
this	vmovapd	[srcreg+iter*srcinc+d2+d1+32], y14 ;; Save I4				; 20
next	vmulpd	y14, y0, y15			;; A3 = R3 * cosine/sine		; 19-23

this	vaddpd	y8, y8, y10			;; R2 + R4 (final R2)			; 20-22
next	vmulpd	y10, y2, y5			;; A2 = R2 * cosine/sine		; 20-24

this	vmovapd	[srcreg+iter*srcinc+d1+32], y11	;; Save I2				; 21
this	vsubpd	y11, y3, y13			;; I1 - I3 (final I3)			; 21-23
this	vmovapd	[srcreg+iter*srcinc+d2+d1], y12	;; Save R4				; 22
next	vmulpd	y12, y6, y15			;; B4 = I4 * cosine/sine		; 21-25

this	vaddpd	y3, y3, y13			;; I1 + I3 (final I1)			; 22-24
next	vmulpd	y13, y7, y15			;; B3 = I3 * cosine/sine		; 22-26
this next yloop_unrolled_one

;; Shuffle register assignments so that next call has R2,I3,I1 in y0-2 and next R4,R3,R2,I4,I3,I2,A4,A3,A2,B4,B3,c/s2 in y3-14.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y8
y8	TEXTEQU	y1
y1	TEXTEQU	y11
y11	TEXTEQU	y10
y10	TEXTEQU	y14
y14	TEXTEQU	y5
y5	TEXTEQU	y2
y2	TEXTEQU	y3
y3	TEXTEQU	y4
y4	TEXTEQU	ytmp
	ENDM


yr4_4c_djbunfft_swiz_cmn MACRO srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	yr4_4c_djbunfft_swiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 3,exec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 4,exec,exec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 5,exec,noexec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 5*srcinc
	bump	screg, 5*scinc
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	yr4_4c_djbunfft_swiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 3,exec,exec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 4,exec,noexec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	screg, 4*scinc
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	yr4_4c_djbunfft_swiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 2,exec,exec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 3,exec,noexec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	screg, 3*scinc
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	yr4_4c_djbunfft_swiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 1,exec,exec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 2,exec,noexec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	screg, 2*scinc
	ELSE
	yr4_4c_djbunfft_swiz_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 0,noexec,exec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	yr4_4c_djbunfft_swiz_unroll 1,exec,noexec,noexec,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDIF
	ENDM

yr4_4c_djbunfft_swiz_unroll MACRO iter,prev,this,next,srcreg,srcinc,d1,d2,bcast,screg1,screg2,scoff,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<ymm0>
y1	TEXTEQU	<ymm1>
y2	TEXTEQU	<ymm2>
y3	TEXTEQU	<ymm3>
y4	TEXTEQU	<ymm4>
y5	TEXTEQU	<ymm5>
y6	TEXTEQU	<ymm6>
y7	TEXTEQU	<ymm7>
y8	TEXTEQU	<ymm8>
y9	TEXTEQU	<ymm9>
y10	TEXTEQU	<ymm10>
y11	TEXTEQU	<ymm11>
y12	TEXTEQU	<ymm12>
y13	TEXTEQU	<ymm13>
y14	TEXTEQU	<ymm14>
y15	TEXTEQU	<ymm15>
	ENDIF

;; On later calls,  This R4,R3,R2,R1,I4,I3,I2,I1,A4,A3,A2,B4,B3,c/s2 will be in y0-13.
;; The remaining registers are free.

this	vsubpd	y8, y8, y4			;; A4 = A4 - I4 (new R4/sine)		; 1-3
this	vmulpd	y13, y6, y13			;; B2 = I2 * cosine/sine		; 1-5
this no bcast vmovapd y14, [screg2+iter*scinc]	;; Sine
this bcast vbroadcastsd y14, Q [screg2+iter*scinc] ;; Sine

this	vaddpd	y9, y9, y5			;; A3 = A3 + I3 (new R3/sine)		; 2-4
next	vmovapd	y15, [srcreg+(iter+1)*srcinc]	;; R1

this	vaddpd	y10, y10, y6			;; A2 = A2 + I2				; 3-5
next	vmovapd	y4, [srcreg+(iter+1)*srcinc+d1] ;; R2

this	vaddpd	y11, y11, y0			;; B4 = B4 + R4 (new I4/sine)		; 4-6
next	vmovapd	y5, [srcreg+(iter+1)*srcinc+d2]	;; R3

this	vsubpd	y12, y12, y1			;; B3 = B3 - R3 (new I3/sine)		; 5-7
next	vmovapd	y6, [srcreg+(iter+1)*srcinc+d2+d1] ;; R4

this	vsubpd	y13, y13, y2			;; B2 = B2 - R2				; 6-8
this	vmulpd	y10, y10, y14			;; A2 = A2 * sine (new R2)		; 6-10
this no bcast vmovapd y0, [screg1+iter*scinc]	;; Sine
this bcast vbroadcastsd y0, Q [screg1+iter*scinc] ;; Sine
next	vshufpd	y2, y15, y4, 15			;; Shuffle R1 and R2 to create R1/R2 hi	; 6

this	vaddpd	y1, y8, y9			;; C3 = R4/sine + R3/sine (newer R3/sine) ; 7-9
next	vshufpd	y15, y15, y4, 0			;; Shuffle R1 and R2 to create R1/R2 low ; 7
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

this	vsubpd	y8, y8, y9			;; D4 = R4/sine - R3/sine (newer I4/sine) ; 8-10
next	vshufpd	y9, y5, y6, 15			;; Shuffle R3 and R4 to create R3/R4 hi	; 8

this	vsubpd	y4, y12, y11			;; C4 = I3/sine - I4/sine (newer R4/sine) ; 9-11
this	vmulpd	y13, y13, y14			;; B2 = B2 * sine (new I2)		; 9-13
next	vshufpd	y5, y5, y6, 0			;; Shuffle R3 and R4 to create R3/R4 low ; 9

this	vaddpd	y12, y12, y11			;; D3 = I3/sine + I4/sine (newer I3/sine) ; 10-12
this	vmulpd	y1, y1, y0			;; C3 * sine (newer R3)			; 10-14
next	vperm2f128 y11, y2, y9, 49		;; Shuffle R1/R2 hi and R3/R4 hi (new R4) ; 10-11
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	vaddpd	y6, y3, y10			;; R1 + R2 (newer R1)			; 11-13
this	vmulpd	y8, y8, y0			;; D4 * sine (newer I4)			; 11-15
next	vperm2f128 y14, y15, y5, 49		;; Shuffle R1/R2 low and R3/R4 low (new R3) ; 11-12

this	vsubpd	y3, y3, y10			;; R1 - R2 (newer R2)			; 12-14
this	vmulpd	y4, y4, y0			;; C4 * sine (newer R4)			; 12-16
next	vperm2f128 y2, y2, y9, 32		;; Shuffle R1/R2 hi and R3/R4 hi (new R2) ; 12-13
next	vmovapd	y10, [srcreg+(iter+1)*srcinc+d2+32] ;; I3

this	vsubpd	y9, y7, y13			;; I1 - I2 (newer I2)			; 13-15
this	vmulpd	y12, y12, y0			;; D3 * sine (newer I3)			; 13-17
next	vperm2f128 y15, y15, y5, 32		;; Shuffle R1/R2 low and R3/R4 low (new R1) ; 13-14
next	vmovapd	y5, [srcreg+(iter+1)*srcinc+d2+d1+32] ;; I4

this	vaddpd	y7, y7, y13			;; I1 + I2 (newer I1)			; 14-16
next	vmovapd	y0, [srcreg+(iter+1)*srcinc+32]	;; I1

this	vsubpd	y13, y6, y1			;; R1 - R3 (final R3)			; 15-17
this	vmovapd	[srcreg+iter*srcinc+d2], y13	;; Save R3				; 18
next	vshufpd	y13, y10, y5, 15		;; Shuffle I3 and I4 to create I3/I4 hi	; 15
this	L1prefetchw srcreg+iter*srcinc+d2+L1pd, L1pt

this	vaddpd	y6, y6, y1			;; R1 + R3 (final R1)			; 16-18
next	vmovapd	y1, [srcreg+(iter+1)*srcinc+d1+32] ;; I2
next	vshufpd	y10, y10, y5, 0			;; Shuffle I3 and I4 to create I3/I4 low ; 16

this	vsubpd	y5, y9, y8			;; I2 - I4 (final I4)			; 17-19
this	vmovapd	[srcreg+iter*srcinc], y6	;; Save R1				; 19
next no bcast vmovapd y6, [screg1+(iter+1)*scinc+32] ;; cosine/sine
next bcast vbroadcastsd y6, Q [screg1+(iter+1)*scinc+scoff] ;; cosine/sine
this	vmovapd	[srcreg+iter*srcinc+d2+d1+32], y5 ;; Save I4				; 20
next	vshufpd	y5, y0, y1, 15			;; Shuffle I1 and I2 to create I1/I2 hi	; 17

this	vaddpd	y9, y9, y8			;; I2 + I4 (final I2)			; 18-20
next	vmulpd	y8, y11, y6			;; A4 = R4 * cosine/sine		; 18-22
next	vshufpd	y0, y0, y1, 0			;; Shuffle I1 and I2 to create I1/I2 low ; 18
this	L1prefetchw srcreg+iter*srcinc+d2+d1+L1pd, L1pt

this	vsubpd	y1, y3, y4			;; R2 - R4 (final R4)			; 19-21
this	vmovapd	[srcreg+iter*srcinc+d1+32], y9	;; Save I2				; 21
next	vmulpd	y9, y14, y6			;; A3 = R3 * cosine/sine		; 19-23
this	vmovapd	[srcreg+iter*srcinc+d2+d1], y1	;; Save R4				; 22
next	vperm2f128 y1, y5, y13, 49		;; Shuffle I1/I2 hi and I3/I4 hi (new I4) ; 19-20

this	vaddpd	y3, y3, y4			;; R2 + R4 (final R2)			; 20-22
next	vperm2f128 y4, y0, y10, 49		;; Shuffle I1/I2 low and I3/I4 low (new I3) ; 20-21
this	vmovapd	[srcreg+iter*srcinc+d1], y3	;; Save R2				; 23
next no bcast vmovapd y3, [screg2+(iter+1)*scinc+32] ;; cosine/sine
next bcast vbroadcastsd y3, Q [screg2+(iter+1)*scinc+scoff] ;; cosine/sine
next	vperm2f128 y5, y5, y13, 32		;; Shuffle I1/I2 hi and I3/I4 hi (new I2) ; 21-22
next	vmulpd	y13, y2, y3			;; A2 = R2 * cosine/sine		; 20-24

next	vperm2f128 y0, y0, y10, 32		;; Shuffle I1/I2 low and I3/I4 low (new I1) ; 22-23
this	vsubpd	y10, y7, y12			;; I1 - I3 (final I3)			; 21-23
this	vmovapd	[srcreg+iter*srcinc+d2+32], y10	;; Save I3				; 2
next	vmulpd	y10, y1, y6			;; B4 = I4 * cosine/sine		; 21-25

this	vaddpd	y7, y7, y12			;; I1 + I3 (final I1)			; 22-24
next	vmulpd	y6, y4, y6			;; B3 = I3 * cosine/sine		; 22-26
this	vmovapd	[srcreg+iter*srcinc+32], y7	;; Save I1				; 3
this next yloop_unrolled_one

;; Shuffle register assignments so that next call has next R4,R3,R2,R1,I4,I3,I2,I1,A4,A3,A2,B4,B3,c/s2 in y0-13.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y11
y11	TEXTEQU	y10
y10	TEXTEQU	y13
y13	TEXTEQU	y3
y3	TEXTEQU	y15
y15	TEXTEQU	y7
y7	TEXTEQU	ytmp
ytmp	TEXTEQU	y1
y1	TEXTEQU	y14
y14	TEXTEQU	y12
y12	TEXTEQU	y6
y6	TEXTEQU	y5
y5	TEXTEQU	y4
y4	TEXTEQU	ytmp
	ENDM

ENDIF


;;
;; ************************************* four-complex-first-fft variants ******************************************
;;

;; This code applies the roots-of-minus-1 premultipliers in an all-complex one-pass FFT.
;; It also applies the three sin/cos multipliers after a radix-4 butterfly.  We save memory
;; by splitting the roots-of-minus-1 premultipliers such that every macro uses the 
;; same premultiplier data and we have 4 sin/cos postmultipliers.  Every sin/cos postmultiplier,
;; a very big table anyway, is multiplied by the other part of the split roots-of-minus-1.
;; The common premuliplier data is 1, .924+.383i, SQRTHALF+SQRTHALFi, .383+.924i.
;; This scheme is used by our simple radix-4 DJB "r4" FFTs.

yr4_fs4cl_four_complex_first_djbfft_preload MACRO
	ENDM
yr4_fs4cl_four_complex_first_djbfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd	ymm7, YMM_P924
	vmovapd	ymm6, [srcreg+d1][rbx]		;; R2
	vmulpd	ymm4, ymm6, ymm7		;; A2 = R2 * .924		;  1-5
	vmovapd	ymm1, YMM_P383
	vmovapd	ymm2, [srcreg+d1+32][rbx]	;; R6 (I2)
	vmulpd	ymm0, ymm2, ymm1		;; C2 = I2 * .383		;  2-6
	vmulpd	ymm6, ymm6, ymm1		;; B2 = R2 * .383		;  3-7
	vmulpd	ymm2, ymm2, ymm7		;; D2 = I2 * .924		;  4-8
	vmovapd	ymm3, [srcreg+d2+d1][rbx]	;; R4
	vmulpd	ymm5, ymm3, ymm1		;; A4 = R4 * .383		;  5-9
	vsubpd	ymm4, ymm4, ymm0		;; A2 = A2 - C2 (new R2)	; 7-9
	vaddpd	ymm6, ymm6, ymm2		;; B2 = B2 + D2 (new I2)	; 9-11
	vmovapd	ymm2, [srcreg+d2+d1+32][rbx]	;; R8 (I4)
	vmulpd	ymm0, ymm2, ymm7		;; C4 = I4 * .924		;  6-10
	vmulpd	ymm7, ymm3, ymm7		;; B4 = R4 * .924		;  7-11
	vmulpd	ymm1, ymm2, ymm1		;; D4 = I4 * .383		;  8-12
	vsubpd	ymm5, ymm5, ymm0		;; A4 = A4 - C4 (new R4)	; 11-13

	vmovapd	ymm2, [srcreg+d2][rbx]		;; R3
	vmovapd	ymm3, [srcreg+d2+32][rbx]	;; R7 (I3)
	vsubpd	ymm0, ymm2, ymm3		;; A3 = R3 - I3			;  1-3  (move upward in 64-bit version)
	vaddpd	ymm2, ymm2, ymm3		;; B3 = R3 + I3			;  2-4

	L1prefetchw srcreg+L1pd, L1pt

	vaddpd	ymm7, ymm7, ymm1		;; B4 = B4 + D4 (new I4)	; 13-15

	vmovapd	ymm1, YMM_SQRTHALF
	vmulpd	ymm0, ymm0, ymm1		;; A3 = A3 * SQRTHALF (new R3)	;  9-13
	vmulpd	ymm2, ymm2, ymm1		;; B3 = B3 * SQRTHALF (new I3)	;  10-14

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm3, ymm4, ymm5		;; R2 - R4 (new R4)		; 14-16
	vaddpd	ymm4, ymm4, ymm5		;; R2 + R4 (new R2)		; 15-17

	vsubpd	ymm1, ymm6, ymm7		;; I2 - I4 (new I4)		; 16-18
	vaddpd	ymm6, ymm6, ymm7		;; I2 + I4 (new I2)		; 17-19

	vmovapd	ymm5, [srcreg][rbx]		;; R1
	vsubpd	ymm7, ymm5, ymm0		;; R1 - R3 (new R3)		; 18-20
	vaddpd	ymm5, ymm5, ymm0		;; R1 + R3 (new R1)		; 19-21

	L1prefetchw srcreg+d2+L1pd, L1pt

	vsubpd	ymm0, ymm7, ymm1		;; R3 - I4 (new R3)		; 21-23
	vaddpd	ymm7, ymm7, ymm1		;; R3 + I4 (new R4)		; 22-24
	vsubpd	ymm1, ymm5, ymm4		;; R1 - R2 (new R2)		; 23-25
	vaddpd	ymm5, ymm5, ymm4		;; R1 + R2 (new R1)		; 24-26

	vmovapd	[srcreg], ymm5			;; Save R1			; eliminate in 64-bit version

	vmovapd	ymm4, [srcreg+32][rbx]		;; R5 (I1)
	vsubpd	ymm5, ymm4, ymm2		;; I1 - I3 (new I3)		; 20-22	move forward in 64-bit
	vaddpd	ymm4, ymm4, ymm2		;; I1 + I3 (new I1)		; 25-27

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm2, ymm5, ymm3		;; I3 - R4 (new I4)		; 26-28
	vaddpd	ymm5, ymm5, ymm3		;; I3 + R4 (new I3)		; 27-29
	vsubpd	ymm3, ymm4, ymm6		;; I1 - I2 (new I2)		; 28-30
	vaddpd	ymm4, ymm4, ymm6		;; I1 + I2 (new I1)		; 29-31

	vmovapd	[srcreg+32], ymm4		;; Save I1			; eliminate in 64-bit version

	vmovapd	ymm4, [screg+64+32]		;; cosine/sine
	vmulpd	ymm6, ymm0, ymm4		;; A3 = R3 * cosine/sine
	vsubpd	ymm6, ymm6, ymm5		;; A3 = A3 - I3
	vmulpd	ymm5, ymm5, ymm4		;; B3 = I3 * cosine/sine
	vaddpd	ymm5, ymm5, ymm0		;; B3 = B3 + R3

	vmovapd	ymm4, [screg+128+32]		;; cosine/sine
	vmulpd	ymm0, ymm1, ymm4		;; A2 = R2 * cosine/sine
	vsubpd	ymm0, ymm0, ymm3		;; A2 = A2 - I2
	vmulpd	ymm3, ymm3, ymm4		;; B2 = I2 * cosine/sine
	vaddpd	ymm3, ymm3, ymm1		;; B2 = B2 + R2

	vmovapd	ymm4, [screg+192+32]		;; cosine/sine
	vmulpd	ymm1, ymm7, ymm4		;; A4 = R4 * cosine/sine
	vsubpd	ymm1, ymm1, ymm2		;; A4 = A4 - I4
	vmulpd	ymm2, ymm2, ymm4		;; B4 = I4 * cosine/sine
	vaddpd	ymm2, ymm2, ymm7		;; B4 = B4 + R4

	vmovapd	ymm4, [srcreg]			;; Restore R1
	vmulpd	ymm4, ymm4, [screg+0+32]	;; A1 = R1 * cosine/sine
	vmovapd	ymm7, [srcreg+32]		;; Restore I1
	vsubpd	ymm4, ymm4, ymm7		;; A1 = A1 - I1
	vmulpd	ymm7, ymm7, [screg+0+32]	;; B1 = I1 * cosine/sine
	vaddpd	ymm7, ymm7, [srcreg]		;; B1 = B1 + R1

	vmulpd	ymm6, ymm6, [screg+64]		;; A3 = A3 * sine (final R3)
	vmulpd	ymm5, ymm5, [screg+64]		;; B3 = B3 * sine (final I3)
	vmulpd	ymm0, ymm0, [screg+128]		;; A2 = A2 * sine (final R2)
	vmulpd	ymm3, ymm3, [screg+128]		;; B2 = B2 * sine (final I2)
	vmulpd	ymm1, ymm1, [screg+192]		;; A4 = A4 * sine (final R4)
	vmulpd	ymm2, ymm2, [screg+192]		;; B4 = B4 * sine (final I4)
	vmulpd	ymm4, ymm4, [screg+0]		;; A1 = A1 * sine (final R1)
	vmulpd	ymm7, ymm7, [screg+0]		;; B1 = B1 * sine (final I1)

	vmovapd	[srcreg+32], ymm7		;; Save I1			; eliminate in 64-bit version

	;; R1 (0 1 2 3) is ymm4, R2 (8 9 10 11) is ymm0, R3 is ymm6, R4 is ymm1, I1 is in srcreg+32, I2 is ymm3, I3 is ymm5, I4 is ymm2

	;; Swizzle the 64-byte cache lines to hold these data values:
	;;	0	8	16	24	4	12	20	28
	;;	1	9	...
	;;	2	10	...
	;;	3	11	...

	;; Swizzle using 8 shufpds and 8 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vshufpd	ymm7, ymm4, ymm0, 0		;; Shuffle R1 and R2 to create 0 8 2 10
	vshufpd	ymm4, ymm4, ymm0, 15		;; Shuffle R1 and R2 to create 1 9 3 11

	vshufpd	ymm0, ymm6, ymm1, 0		;; Shuffle R3 and R4 to create 16 24 18 26
	vshufpd	ymm6, ymm6, ymm1, 15		;; Shuffle R3 and R4 to create 17 25 19 27

	vperm2f128 ymm1, ymm7, ymm0, 32		;; Shuffle R1/R2 low and R3/R4 low (0 8 16 24)
	vperm2f128 ymm7, ymm7, ymm0, 49		;; Shuffle R1/R2 low and R3/R4 low (2 10 18 26)

	vperm2f128 ymm0, ymm4, ymm6, 32		;; Shuffle R1/R2 hi and R3/R4 hi (1 9 17 25)
	vperm2f128 ymm4, ymm4, ymm6, 49		;; Shuffle R1/R2 hi and R3/R4 hi (3 11 19 27)

	vmovapd	[srcreg], ymm1			;; Save (0 8 16 24)
	vmovapd	[srcreg+d2], ymm7		;; Save (2 10 18 26)
	vmovapd	[srcreg+d1], ymm0		;; Save (1 9 17 25)
	vmovapd	[srcreg+d2+d1], ymm4		;; Save (3 11 19 27)

	vmovapd	ymm1, [srcreg+32]		;; Reload saved I1

	vshufpd	ymm7, ymm1, ymm3, 0		;; Shuffle I1 and I2 to create 4 12 6 14
	vshufpd	ymm1, ymm1, ymm3, 15		;; Shuffle I1 and I2 to create 5 13 7 15

	vshufpd	ymm3, ymm5, ymm2, 0		;; Shuffle I3 and I4 to create 16 28 22 30
	vshufpd	ymm5, ymm5, ymm2, 15		;; Shuffle I3 and I4 to create 21 29 23 31

	vperm2f128 ymm2, ymm7, ymm3, 32		;; Shuffle I1/I2 low and I3/I4 low (4 12 20 28)
	vperm2f128 ymm7, ymm7, ymm3, 49		;; Shuffle I1/I2 low and I3/I4 low (6 14 22 30)

	vperm2f128 ymm3, ymm1, ymm5, 32		;; Shuffle I1/I2 hi and I3/I4 hi (5 13 21 29)
	vperm2f128 ymm1, ymm1, ymm5, 49		;; Shuffle I1/I2 hi and I3/I4 hi (7 15 23 31)

	vmovapd	[srcreg+32], ymm2		;; Save (4 12 20 28)
	vmovapd	[srcreg+d2+32], ymm7		;; Save (6 14 22 30)
	vmovapd	[srcreg+d1+32], ymm3		;; Save (5 13 21 29)
	vmovapd	[srcreg+d2+d1+32], ymm1		;; Save (7 15 23 31)

	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; This code applies the roots-of-minus-1 premultipliers in an all-complex FFT.
;; It also applies the three sin/cos multipliers after the first radix-4 butterfly.
;; We save memory by splitting the roots-of-minus-1 premultipliers such that every
;; macro uses the same premultiplier data.  The rest of the premultiplier is applied
;; at the end of pass 1.  This scheme is used by the r4delay and r4dwpn FFTs.

; The sin/cos and premultiplier data is combined in one table.  The premultiplier data
; is at screg the sin/cos data is at screg+256.
yr4_4cl_csc_four_complex_first_djbfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd ymm6, [screg+0+32]		;; cosine/sine
	vmovapd	ymm1, [srcreg]			;; R1
	vmulpd	ymm7, ymm1, ymm6		;; A1 = R1 * cosine/sine
	vmovapd	ymm2, [srcreg+32]		;; R5 (I1)
	vsubpd	ymm7, ymm7, ymm2		;; A1 = A1 - I1
	vmulpd	ymm2, ymm2, ymm6		;; B1 = I1 * cosine/sine
	vaddpd	ymm2, ymm2, ymm1		;; B1 = B1 + R1

	vmovapd ymm3, [screg+128+32]		;; cosine/sine
	vmovapd	ymm0, [srcreg+d2]		;; R3
	vmulpd	ymm5, ymm0, ymm3		;; A3 = R3 * cosine/sine
	vmovapd	ymm1, [srcreg+d2+32]		;; R7 (I3)
	vsubpd	ymm5, ymm5, ymm1		;; A3 = A3 - I3
	vmulpd	ymm1, ymm1, ymm3		;; B3 = I3 * cosine/sine
	vaddpd	ymm1, ymm1, ymm0		;; B3 = B3 + R3

	vmovapd ymm0, [screg+0]			;; sine
	vmulpd	ymm7, ymm7, ymm0		;; A1 = A1 * sine
	vmulpd	ymm2, ymm2, ymm0		;; B1 = B1 * sine
	vmovapd ymm0, [screg+128]		;; sine
	vmulpd	ymm5, ymm5, ymm0		;; A3 = A3 * sine
	vmulpd	ymm1, ymm1, ymm0		;; B3 = B3 * sine

	vsubpd	ymm0, ymm7, ymm5		;; R1 - R3 (new R3)
	vaddpd	ymm7, ymm7, ymm5		;; R1 + R3 (new R1)

	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	ymm5, ymm2, ymm1		;; I1 - I3 (new I3)
	vaddpd	ymm2, ymm2, ymm1		;; I1 + I3 (new I1)

	vmovapd	[srcreg], ymm7			;; Temporarily save new R1
	vmovapd	[srcreg+32], ymm2		;; Temporarily save new I1

	vmovapd ymm7, [screg+64+32]		;; cosine/sine
	vmovapd	ymm3, [srcreg+d1]		;; R2
	vmulpd	ymm6, ymm3, ymm7		;; A2 = R2 * cosine/sine
	vmovapd	ymm4, [srcreg+d1+32]		;; R6 (I2)
	vsubpd	ymm6, ymm6, ymm4		;; A2 = A2 - I2
	vmulpd	ymm4, ymm4, ymm7		;; B2 = I2 * cosine/sine
	vaddpd	ymm4, ymm4, ymm3		;; B2 = B2 + R2

	vmovapd ymm7, [screg+192+32]		;; cosine/sine
	vmovapd	ymm1, [srcreg+d2+d1]		;; R4
	vmulpd	ymm3, ymm1, ymm7		;; A4 = R4 * cosine/sine
	vmovapd	ymm2, [srcreg+d2+d1+32]		;; R8 (I4)
	vsubpd	ymm3, ymm3, ymm2		;; A4 = A4 - I4
	vmulpd	ymm2, ymm2, ymm7		;; B4 = I4 * cosine/sine
	vaddpd	ymm2, ymm2, ymm1		;; B4 = B4 + R4

	L1prefetchw srcreg+d1+L1pd, L1pt

	vmovapd ymm1, [screg+64]		;; sine
	vmulpd	ymm6, ymm6, ymm1		;; A2 = A2 * sine
	vmulpd	ymm4, ymm4, ymm1		;; B2 = B2 * sine
	vmovapd ymm1, [screg+192]		;; sine
	vmulpd	ymm3, ymm3, ymm1		;; A4 = A4 * sine
	vmulpd	ymm2, ymm2, ymm1		;; B4 = B4 * sine

	vsubpd	ymm1, ymm6, ymm3		;; R2 - R4 (new R4)
	vaddpd	ymm6, ymm6, ymm3		;; R2 + R4 (new R2)

	vsubpd	ymm3, ymm4, ymm2		;; I2 - I4 (new I4)
	vaddpd	ymm4, ymm4, ymm2		;; I2 + I4 (new I2)

	L1prefetchw srcreg+d2+L1pd, L1pt

	vsubpd	ymm2, ymm0, ymm3		;; R3 - I4 (newer R3)
	vaddpd	ymm0, ymm0, ymm3		;; R3 + I4 (newer R4)

	vsubpd	ymm3, ymm5, ymm1		;; I3 - R4 (newer I4)
	vaddpd	ymm5, ymm5, ymm1		;; I3 + R4 (newer I3)

	vmovapd	ymm1, [screg+256+0+32]		;; cosine/sine
	vmulpd	ymm7, ymm2, ymm1		;; A3 = R3 * cosine/sine
	vsubpd	ymm7, ymm7, ymm5		;; A3 = A3 - I3
	vmulpd	ymm5, ymm5, ymm1		;; B3 = I3 * cosine/sine
	vaddpd	ymm5, ymm5, ymm2		;; B3 = B3 + R3

	vmulpd	ymm2, ymm0, ymm1		;; A4 = R4 * cosine/sine
	vaddpd	ymm2, ymm2, ymm3		;; A4 = A4 + I4
	vmulpd	ymm3, ymm3, ymm1		;; B4 = I4 * cosine/sine
	vsubpd	ymm3, ymm3, ymm0		;; B4 = B4 - R4

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vmovapd	ymm0, [screg+256+0]
	vmulpd	ymm7, ymm7, ymm0		;; A3 = A3 * sine (final R3)
	vmulpd	ymm5, ymm5, ymm0		;; B3 = B3 * sine (final I3)
	vmulpd	ymm2, ymm2, ymm0		;; A4 = A4 * sine (final R4)
	vmulpd	ymm3, ymm3, ymm0		;; B4 = B4 * sine (final I4)

	vmovapd	[srcreg+d2], ymm7		;; Save R3
	vmovapd	[srcreg+d2+32], ymm5		;; Save I3
	vmovapd	[srcreg+d2+d1], ymm2		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm3		;; Save I4

	vmovapd	ymm0, [srcreg] 			;; Reload new R1
	vmovapd	ymm1, [srcreg+32] 		;; Reload new I1

	vsubpd	ymm2, ymm0, ymm6		;; R1 - R2 (newer R2)
	vaddpd	ymm0, ymm0, ymm6		;; R1 + R2 (newer R1)

	vsubpd	ymm3, ymm1, ymm4		;; I1 - I2 (newer I2)
	vaddpd	ymm1, ymm1, ymm4		;; I1 + I2 (newer I1)

	vmovapd	[srcreg], ymm0			;; Save R1
	vmovapd	[srcreg+32], ymm1		;; Save I1

	vmovapd	ymm7, [screg+256+64+32]		;; cosine/sine
	vmulpd	ymm4, ymm2, ymm7		;; A2 = R2 * cosine/sine
	vsubpd	ymm4, ymm4, ymm3		;; A2 = A2 - I2
	vmulpd	ymm3, ymm3, ymm7		;; B2 = I2 * cosine/sine
	vaddpd	ymm3, ymm3, ymm2		;; B2 = B2 + R2

	vmulpd	ymm4, ymm4, [screg+256+64]	;; A2 = A2 * sine (final R2)
	vmulpd	ymm3, ymm3, [screg+256+64]	;; B2 = B2 * sine (final I2)

	vmovapd	[srcreg+d1], ymm4		;; Save R2
	vmovapd	[srcreg+d1+32], ymm3		;; Save I2
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; 64-bit version

IFDEF X86_64

yr4_4cl_csc_four_complex_first_djbfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	yr4_4c_first_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 3,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 4,exec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 5,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, 5*srcinc
	bump	screg, 5*scinc
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	yr4_4c_first_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 3,exec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 4,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	screg, 4*scinc
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	yr4_4c_first_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 2,exec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 3,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	screg, 3*scinc
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	yr4_4c_first_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 1,exec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 2,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	screg, 2*scinc
	ELSE
	yr4_4c_first_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 0,noexec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_first_djbfft_unroll 1,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDIF
	ENDM

yr4_4c_first_djbfft_unroll MACRO iter,prev,this,next,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<ymm0>
y1	TEXTEQU	<ymm1>
y2	TEXTEQU	<ymm2>
y3	TEXTEQU	<ymm3>
y4	TEXTEQU	<ymm4>
y5	TEXTEQU	<ymm5>
y6	TEXTEQU	<ymm6>
y7	TEXTEQU	<ymm7>
y8	TEXTEQU	<ymm8>
y9	TEXTEQU	<ymm9>
y10	TEXTEQU	<ymm10>
y11	TEXTEQU	<ymm11>
y12	TEXTEQU	<ymm12>
y13	TEXTEQU	<ymm13>
y14	TEXTEQU	<ymm14>
y15	TEXTEQU	<ymm15>
	ENDIF

;; On later calls, previous A2,B2,A3,B3,A4,B4 will be in y0-5.  This R1,R3,I1,I2,I3,A1,A3,B1,B3,c/s2 will be in y6-15.

this	vsubpd	y11, y11, y8			;; A1 = A1 - I1					; 1-3
this	vmulpd	y8, y9, y15			;; B2 = I2 * cosine/sine			;	0-4

this	vsubpd	y12, y12, y10			;; A3 = A3 - I3					; 2-4
this	vmovapd	y10, [srcreg+iter*srcinc+d2+d1+32] ;; R8 (I4)

this	vaddpd	y13, y13, y6			;; B1 = B1 + R1					; 3-5
this	vmulpd	y6, y10, [screg+iter*scinc+192+32] ;; B4 = I4 * cosine/sine			;	1-5

this	vaddpd	y14, y14, y7			;; B3 = B3 + R3					; 4-6
this	vmovapd	y7, [srcreg+iter*srcinc+d1]	;; R2
this	vmulpd	y15, y7, y15			;; A2 = R2 * cosine/sine			;	2-6

this	vaddpd	y8, y8, y7			;; B2 = B2 + R2					; 5-7
this	vmovapd	y7, [srcreg+iter*srcinc+d2+d1]	;; R4

this	vaddpd	y6, y6, y7			;; B4 = B4 + R4					; 6-8
this	vmulpd	y7, y7, [screg+iter*scinc+192+32] ;; A4 = R4 * cosine/sine			;	3-7

this	vsubpd	y15, y15, y9			;; A2 = A2 - I2					; 7-9
this	vmovapd y9, [screg+iter*scinc+0]	;; sine
this	vmulpd	y11, y11, y9			;; A1 = A1 * sine				;	4-8

this	vsubpd	y7, y7, y10			;; A4 = A4 - I4					; 8-10
this	vmovapd y10, [screg+iter*scinc+128]	;; sine
this	vmulpd	y12, y12, y10			;; A3 = A3 * sine				;	5-9
this	vmulpd	y13, y13, y9			;; B1 = B1 * sine				;	6-10
this	vmulpd	y14, y14, y10			;; B3 = B3 * sine				;	7-11
this	vmovapd y9, [screg+iter*scinc+64]	;; sine
this	vmulpd	y8, y8, y9			;; B2 = B2 * sine				;	8-12

this	vmovapd y10, [screg+iter*scinc+192]	;; sine
this	vmulpd	y6, y6, y10			;; B4 = B4 * sine				;	9-13

this	vmulpd	y15, y15, y9			;; A2 = A2 * sine				;	10-14
this	vsubpd	y9, y11, y12			;; R1 - R3 (new R3)				; 10-12

this	vaddpd	y11, y11, y12			;; R1 + R3 (new R1)				; 11-13
this	vmulpd	y7, y7, y10			;; A4 = A4 * sine				;	11-15
prev	vmovapd	y12, [screg+(iter-1)*scinc+256+0]

this	vsubpd	y10, y13, y14			;; I1 - I3 (new I3)				; 12-14
prev	vmulpd	y2, y2, y12			;; A3 = A3 * sine (final R3)			;	12-16
this next yloop_unrolled_one

this	vaddpd	y13, y13, y14			;; I1 + I3 (new I1)				; 13-15
prev	vmulpd	y3, y3, y12			;; B3 = B3 * sine (final I3)			;	13-17

this	vsubpd	y14, y8, y6			;; I2 - I4 (new I4)				; 14-16
prev	vmulpd	y4, y4, y12			;; A4 = A4 * sine (final R4)			;	14-18
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

this	vaddpd	y8, y8, y6			;; I2 + I4 (new I2)				; 15-17
prev	vmulpd	y5, y5, y12			;; B4 = B4 * sine (final I4)			;	15-19
prev	vmovapd	y6, [screg+(iter-1)*scinc+256+64] ;; sine

this	vsubpd	y12, y15, y7			;; R2 - R4 (new R4)				; 16-18
prev	vmulpd	y0, y0, y6			;; A2 = A2 * sine (final R2)			;	16-20
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	vaddpd	y15, y15, y7			;; R2 + R4 (new R2)				; 17-19
prev	vmulpd	y1, y1, y6			;; B2 = B2 * sine (final I2)			;	17-21
this	vmovapd	y7, [screg+iter*scinc+256+0+32]	;; cosine/sine
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2], y2	;; Save R3					; 17

this	vsubpd	y6, y9, y14			;; R3 - I4 (newer R3)				; 18-20
this	vmovapd	y2, [screg+iter*scinc+256+64+32] ;; cosine/sine
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+32], y3 ;; Save I3					; 18

this	vaddpd	y3, y10, y12			;; I3 + R4 (newer I3)				; 19-21
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+d1], y4 ;; Save R4					; 19

this	vaddpd	y9, y9, y14			;; R3 + I4 (newer R4)				; 20-22
next	vmovapd y4, [screg+(iter+1)*scinc+0+32]	;; cosine/sine
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+d1+32], y5 ;; Save I4				; 20

this	vsubpd	y10, y10, y12			;; I3 - R4 (newer I4)				; 21-23
this	vmulpd	y12, y6, y7			;; A3 = R3 * cosine/sine			;	21-25
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1], y0 ;; Save R2					; 21

this	vsubpd	y0, y11, y15			;; R1 - R2 (newer R2)				; 22-24
this	vmulpd	y5, y3, y7			;; B3 = I3 * cosine/sine			;	22-26
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1+32], y1 ;; Save I2					; 22

this	vsubpd	y1, y13, y8			;; I1 - I2 (newer I2)				; 23-25		
this	vmulpd	y14, y9, y7			;; A4 = R4 * cosine/sine			;	23-27
this	L1prefetchw srcreg+iter*srcinc+d2+L1pd, L1pt

this	vaddpd	y11, y11, y15			;; R1 + R2 (newer R1)				; 24-26
this	vmulpd	y7, y10, y7			;; B4 = I4 * cosine/sine			;	24-28
next	vmovapd	y15, [srcreg+(iter+1)*srcinc]	;; R1

this	vaddpd	y13, y13, y8			;; I1 + I2 (newer I1)				; 25-27
this	vmulpd	y8, y0, y2			;; A2 = R2 * cosine/sine			;	25-29
this	L1prefetchw srcreg+iter*srcinc+d2+d1+L1pd, L1pt

this	vsubpd	y12, y12, y3			;; A3 = A3 - I3					; 26-28
this	vmulpd	y2, y1, y2			;; B2 = I2 * cosine/sine			;	26-30
next	vmovapd y3, [screg+(iter+1)*scinc+128+32] ;; cosine/sine

this	vaddpd	y5, y5, y6			;; B3 = B3 + R3					; 27-29
next	vmulpd	y6, y15, y4			;; A1 = R1 * cosine/sine			;	27-31
this	vmovapd	[srcreg+iter*srcinc], y11	;; Save R1					; 27
next	vmovapd	y11, [srcreg+(iter+1)*srcinc+d2] ;; R3

this	vaddpd	y14, y14, y10			;; A4 = A4 + I4					; 28-30
next	vmulpd	y10, y11, y3			;; A3 = R3 * cosine/sine			;	28-32
this	vmovapd	[srcreg+iter*srcinc+32], y13	;; Save I1					; 28
next	vmovapd	y13, [srcreg+(iter+1)*srcinc+32] ;; R5 (I1)

this	vsubpd	y7, y7, y9			;; B4 = B4 - R4					; 29-31
next	vmulpd	y4, y13, y4			;; B1 = I1 * cosine/sine			;	29-33
next	vmovapd	y9, [srcreg+(iter+1)*srcinc+d2+32] ;; R7 (I3)

this	vsubpd	y8, y8, y1			;; A2 = A2 - I2					; 30-32
next	vmulpd	y3, y9, y3			;; B3 = I3 * cosine/sine			;	30-34
next	vmovapd y1, [screg+(iter+1)*scinc+64+32] ;; cosine/sine

this	vaddpd	y2, y2, y0			;; B2 = B2 + R2					; 31-33
next	vmovapd	y0, [srcreg+(iter+1)*srcinc+d1+32] ;; R6 (I2)

;; Shuffle register assignments so that next call has A2,B2,A3,B3,A4,B4 in y0-5 and next R1,R3,I1,I2,I3,A1,A3,B1,B3,c/s2 will be in y6-15.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y8
y8	TEXTEQU	y13
y13	TEXTEQU	y4
y4	TEXTEQU	y14
y14	TEXTEQU	y3
y3	TEXTEQU	y5
y5	TEXTEQU	y7
y7	TEXTEQU	y11
y11	TEXTEQU	y6
y6	TEXTEQU	y15
y15	TEXTEQU	y1
y1	TEXTEQU y2
y2	TEXTEQU	y12
y12	TEXTEQU	y10
y10	TEXTEQU	y9
y9	TEXTEQU	ytmp
	ENDM

ENDIF


;;
;; ************************************* four-complex-last-unfft variants ******************************************
;;

;; This code applies the sin/cos multipliers before a radix-4 butterfly.
;; Then it applies the premultipliers since the all-complex inverse FFT is complete.
;; We save memory by splitting the roots-of-minus-1 premultipliers such that every
;; macro uses the same premultiplier data.  Every sin/cos postmultiplier, a very big
;; table anyway, are then multiplied by the other part of the split roots-of-minus-1.
;; The common postmuliplier data is 1, .924-.383i, SQRTHALF-SQRTHALFi, .383-.924i.

yr4_s4cl_four_complex_last_unfft_preload MACRO
	ENDM
yr4_s4cl_four_complex_last_unfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd

	;; On input the 64-byte cache lines hold these data values:
	;;	0	8	16	24	4	12	20	28
	;;	1	...
	;;	2	...
	;;	3	...

	;; These are swizzled to:
	;;	0	1	2	3	4	5	6	7
	;;	8	...
	;;	16	...
	;;	24	...

	;; Swizzle using 8 shufpds and 8 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm1, [srcreg]			;; R1
	vmovapd	ymm7, [srcreg+d1]		;; R2
	vshufpd	ymm0, ymm1, ymm7, 15		;; Shuffle R1 and R2 to create 8 9 24 25		; 1
	vshufpd	ymm1, ymm1, ymm7, 0		;; Shuffle R1 and R2 to create 0 1 16 17		; 2

	vmovapd	ymm2, [srcreg+d2]		;; R3
	vmovapd	ymm7, [srcreg+d2+d1]		;; R4
	vshufpd	ymm3, ymm2, ymm7, 15		;; Shuffle R3 and R4 to create 10 11 26 27		; 3
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle R3 and R4 to create 2 3 18 19		; 4

	vperm2f128 ymm4, ymm0, ymm3, 32		;; Shuffle R1/R2 hi and R3/R4 hi (8 9 10 11)		; 5-6
	vperm2f128 ymm0, ymm0, ymm3, 49		;; Shuffle R1/R2 hi and R3/R4 hi (24 25 26 27)		; 6-7

	vperm2f128 ymm3, ymm1, ymm2, 32		;; Shuffle R1/R2 low and R3/R4 low (0 1 2 3)		; 7-8
	vperm2f128 ymm1, ymm1, ymm2, 49		;; Shuffle R1/R2 low and R3/R4 low (16 17 18 19)	; 8-9

	vmovapd	ymm6, [srcreg+32]		;; I1
	vmovapd	ymm2, [srcreg+d1+32]		;; I2
	vshufpd	ymm5, ymm6, ymm2, 15		;; Shuffle I1 and I2 to create 12 13 28 29
	vshufpd	ymm6, ymm6, ymm2, 0		;; Shuffle I1 and I2 to create 4 5 20 21

	vmovapd	[srcreg], ymm3			;; Temporarily save R1 (0 1 2 3)

	vmovapd	ymm2, [srcreg+d2+32]		;; I3
	vmovapd	ymm7, [srcreg+d2+d1+32]		;; I4
	vshufpd	ymm3, ymm2, ymm7, 15		;; Shuffle I3 and I4 to create 14 15 30 31
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle I3 and I4 to create 6 7 22 23

	vperm2f128 ymm7, ymm5, ymm3, 32		;; Shuffle I1/I2 hi and I3/I4 hi (12 13 14 15)
	vperm2f128 ymm5, ymm5, ymm3, 49		;; Shuffle I1/I2 hi and I3/I4 hi (28 29 30 31)

	vperm2f128 ymm3, ymm6, ymm2, 32		;; Shuffle I1/I2 low and I3/I4 low (4 5 6 7)
	vperm2f128 ymm6, ymm6, ymm2, 49		;; Shuffle I1/I2 low and I3/I4 low (20 21 22 23)

	vmovapd	[srcreg+32], ymm3		;; Temporarily save I1 (4 5 6 7)

	vmovapd	ymm3, [screg+128+32]		;; cosine/sine
	vmulpd	ymm2, ymm4, ymm3		;; A2 = R2 * cosine/sine
	vaddpd	ymm2, ymm2, ymm7		;; A2 = A2 + I2
	vmulpd	ymm7, ymm7, ymm3		;; B2 = I2 * cosine/sine
	vsubpd	ymm7, ymm7, ymm4		;; B2 = B2 - R2

	vmovapd	ymm3, [screg+192+32]		;; cosine/sine
	vmulpd	ymm4, ymm0, ymm3		;; A4 = R4 * cosine/sine
	vaddpd	ymm4, ymm4, ymm5		;; A4 = A4 + I4
	vmulpd	ymm5, ymm5, ymm3		;; B4 = I4 * cosine/sine
	vsubpd	ymm5, ymm5, ymm0		;; B4 = B4 - R4

	vmovapd	ymm3, [screg+64+32]		;; cosine/sine
	vmulpd	ymm0, ymm1, ymm3		;; A3 = R3 * cosine/sine
	vaddpd	ymm0, ymm0, ymm6		;; A3 = A3 + I3
	vmulpd	ymm6, ymm6, ymm3		;; B3 = I3 * cosine/sine
	vsubpd	ymm6, ymm6, ymm1		;; B3 = B3 - R3

	L1prefetchw srcreg+L1pd, L1pt

	vmovapd	ymm3, [screg+128]		;; sine
	vmulpd	ymm2, ymm2, ymm3		;; A2 = A2 * sine (new R2)
	vmulpd	ymm7, ymm7, ymm3		;; B2 = B2 * sine (new I2)

	vmovapd	ymm3, [screg+192]		;; sine
	vmulpd	ymm4, ymm4, ymm3		;; A4 = A4 * sine (new R4)
	vmulpd	ymm5, ymm5, ymm3		;; B4 = B4 * sine (new I4)

	vmovapd	ymm3, [screg+64]		;; sine
	vmulpd	ymm0, ymm0, ymm3		;; A3 = A3 * sine (new R3)
	vmulpd	ymm6, ymm6, ymm3		;; B3 = B3 * sine (new I3)

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm1, ymm4, ymm0		;; R4 = R4 - R3 (new I4)
	vaddpd	ymm4, ymm4, ymm0		;; R3 = R4 + R3 (new R3)

	vsubpd	ymm0, ymm6, ymm5		;; I3 = I3 - I4 (new R4)
	vaddpd	ymm6, ymm6, ymm5		;; I4 = I3 + I4 (new I3)

	vmovapd	[srcreg+d1], ymm0		;; Temporarily save R4
	vmovapd	[srcreg+d1+32], ymm1		;; Temporarily save I4

	vmovapd	ymm5, [srcreg]			;; Reload R1
	vmovapd	ymm3, [screg+0+32]		;; cosine/sine
	vmulpd	ymm0, ymm5, ymm3		;; A1 = R1 * cosine/sine
	vmovapd	ymm1, [srcreg+32]		;; Reload I1
	vaddpd	ymm0, ymm0, ymm1		;; A1 = A1 + I1
	vmulpd	ymm1, ymm1, ymm3		;; B1 = I1 * cosine/sine
	vsubpd	ymm1, ymm1, ymm5		;; B1 = B1 - R1
	vmovapd	ymm3, [screg+0]			;; Load sine
	vmulpd	ymm0, ymm0, ymm3		;; A1 = A1 * sine (new R1)
	vmulpd	ymm1, ymm1, ymm3		;; B1 = B1 * sine (new I1)

	L1prefetchw srcreg+d2+L1pd, L1pt

	vsubpd	ymm5, ymm0, ymm2		;; R1 - R2 (new R2)
	vaddpd	ymm0, ymm0, ymm2		;; R1 + R2 (new R1)

	vsubpd	ymm2, ymm1, ymm7		;; I1 - I2 (new I2)
	vaddpd	ymm1, ymm1, ymm7		;; I1 + I2 (new I1)

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm7, ymm0, ymm4		;; R1 - R3 (new R3)
	vaddpd	ymm0, ymm0, ymm4		;; R1 + R3 (final R1)

	vsubpd	ymm4, ymm1, ymm6		;; I1 - I3 (new I3)
	vaddpd	ymm1, ymm1, ymm6		;; I1 + I3 (final I1)

	vmovapd	[srcreg], ymm0			;; Save R1
	vmovapd	[srcreg+32], ymm1		;; Save I1

	vmovapd	ymm3, [srcreg+d1]		;; Reload R4
	vsubpd	ymm0, ymm5, ymm3		;; R2 - R4 (new R4)
	vaddpd	ymm5, ymm5, ymm3		;; R2 + R4 (new R2)

	vmovapd	ymm3, [srcreg+d1+32]		;; Reload I4
	vsubpd	ymm1, ymm2, ymm3		;; I2 - I4 (new I4)
	vaddpd	ymm2, ymm2, ymm3		;; I2 + I4 (new I2)

	vmulpd	ymm7, ymm7, YMM_SQRTHALF	;; A3 = R3 * SQRTHALF
	vmulpd	ymm4, ymm4, YMM_SQRTHALF	;; B3 = I3 * SQRTHALF
	vsubpd	ymm6, ymm4, ymm7		;; B3 = B3 - A3 (final I3)
	vaddpd	ymm4, ymm4, ymm7		;; A3 = A3 + B3	(final R3)

	vmovapd	[srcreg+d2], ymm4		;; Save R3
	vmovapd	[srcreg+d2+32], ymm6		;; Save I3

	vmovapd	ymm3, YMM_P924
	vmulpd	ymm7, ymm5, ymm3		;; A2 = R2 * .924
	vmovapd	ymm4, YMM_P383
	vmulpd	ymm6, ymm2, ymm4		;; C2 = I2 * .383
	vaddpd	ymm7, ymm7, ymm6		;; A2 = A2 + C2 (final R2)
	vmulpd	ymm5, ymm5, ymm4		;; B2 = R2 * .383
	vmulpd	ymm2, ymm2, ymm3		;; D2 = I2 * .924
	vsubpd	ymm2, ymm2, ymm5		;; D2 = D2 - B2 (final I2)

	vmulpd	ymm6, ymm0, ymm4		;; A4 = R4 * .383
	vmulpd	ymm5, ymm1, ymm3		;; C4 = I4 * .924
	vaddpd	ymm6, ymm6, ymm5		;; A4 = A4 + C4	(final R4)
	vmulpd	ymm0, ymm0, ymm3		;; B4 = R4 * .924
	vmulpd	ymm1, ymm1, ymm4		;; D4 = I4 * .383
	vsubpd	ymm1, ymm1, ymm0		;; D4 = D4 - B4	(final I4)

	vmovapd	[srcreg+d1], ymm7		;; Save R2
	vmovapd	[srcreg+d1+32], ymm2		;; Save I2
	vmovapd	[srcreg+d2+d1], ymm6		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm1		;; Save I4
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; This code applies the sin/cos multipliers before a radix-4 butterfly.
;; After the butterfly, it applies the all-complex premultipliers since the inverse FFT is complete.
;; The screg and pmreg data is combined
yr4_4cl_csc_four_complex_last_djbunfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd	ymm0, [screg+256+64+32]		;; cosine/sine
	vmovapd	ymm1, [srcreg+d1]		;; R2
	vmulpd	ymm5, ymm1, ymm0		;; A2 = R2 * cosine/sine
	vmovapd	ymm2, [srcreg+d1+32]		;; I2
	vaddpd	ymm5, ymm5, ymm2		;; A2 = A2 + I2
	vmulpd	ymm2, ymm2, ymm0		;; B2 = I2 * cosine/sine
	vsubpd	ymm2, ymm2, ymm1		;; B2 = B2 - R2

	vmovapd	ymm0, [screg+256+0+32]		;; cosine/sine
	vmovapd	ymm1, [srcreg+d2+d1]		;; R4
	vmulpd	ymm7, ymm1, ymm0		;; A4 = R4 * cosine/sine
	vmovapd	ymm3, [srcreg+d2+d1+32]		;; I4
	vsubpd	ymm7, ymm7, ymm3		;; A4 = A4 - I4
	vmulpd	ymm3, ymm3, ymm0		;; B4 = I4 * cosine/sine
	vaddpd	ymm3, ymm3, ymm1		;; B4 = B4 + R4

	vmovapd	ymm1, [srcreg+d2]		;; R3
	vmulpd	ymm6, ymm1, ymm0		;; A3 = R3 * cosine/sine
	vmovapd	ymm4, [srcreg+d2+32]		;; I3
	vaddpd	ymm6, ymm6, ymm4		;; A3 = A3 + I3
	vmulpd	ymm4, ymm4, ymm0		;; B3 = I3 * cosine/sine
	vsubpd	ymm4, ymm4, ymm1		;; B3 = B3 - R3

	vmulpd	ymm5, ymm5, [screg+256+64]	;; A2 = A2 * sine (new R2)
	vmulpd	ymm2, ymm2, [screg+256+64]	;; B2 = B2 * sine (new I2)

	vmovapd	ymm0, [screg+256+0]		;; sine
	vmulpd	ymm7, ymm7, ymm0		;; A4 = A4 * sine (new R4)
	vmulpd	ymm3, ymm3, ymm0		;; B4 = B4 * sine (new I4)
	vmulpd	ymm6, ymm6, ymm0		;; A3 = A3 * sine (new R3)
	vmulpd	ymm4, ymm4, ymm0		;; B3 = B3 * sine (new I3)

	L1prefetchw srcreg+L1pd, L1pt

	vmovapd	ymm0, [srcreg]			;; R1
	vsubpd	ymm1, ymm0, ymm5		;; R1 - R2 (new R2)
	vaddpd	ymm0, ymm0, ymm5		;; R1 + R2 (new R1)
	vsubpd	ymm5, ymm7, ymm6		;; R4 - R3 (new I4)
	vaddpd	ymm7, ymm7, ymm6		;; R4 + R3 (new R3)

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm6, ymm4, ymm3		;; I3 - I4 (new R4)
	vaddpd	ymm4, ymm4, ymm3		;; I3 + I4 (new I3)

	vsubpd	ymm3, ymm1, ymm6		;; R2 - R4 (newer R4)
	vaddpd	ymm1, ymm1, ymm6		;; R2 + R4 (newer R2)

	L1prefetchw srcreg+d2+L1pd, L1pt

	vmovapd	[srcreg], ymm3			;; Temporarily save R4

	vmovapd	ymm6, [srcreg+32]		;; I1
	vsubpd	ymm3, ymm6, ymm2		;; I1 - I2 (new I2)
	vaddpd	ymm6, ymm6, ymm2		;; I1 + I2 (new I1)

	vsubpd	ymm2, ymm0, ymm7		;; R1 - R3 (newer R3)
	vaddpd	ymm0, ymm0, ymm7		;; R1 + R3 (newer R1)

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vaddpd	ymm7, ymm3, ymm5		;; I2 + I4 (newer I2)
	vsubpd	ymm3, ymm3, ymm5		;; I2 - I4 (newer I4)

	vsubpd	ymm5, ymm6, ymm4		;; I1 - I3 (newer I3)
	vaddpd	ymm6, ymm6, ymm4		;; I1 + I3 (newer I1)

	vmovapd	[srcreg+32], ymm3		;; Temporarily save I4

	vmovapd ymm3, [screg+0+32]		;; cosine/sine
	vmulpd	ymm4, ymm0, ymm3		;; A1 = R1 * cosine/sine
	vaddpd	ymm4, ymm4, ymm6		;; A1 = A1 + I1
	vmulpd	ymm6, ymm6, ymm3		;; B1 = I1 * cosine/sine
	vsubpd	ymm6, ymm6, ymm0		;; B1 = B1 - R1

	vmovapd ymm3, [screg+128+32]		;; cosine/sine
	vmulpd	ymm0, ymm2, ymm3		;; A3 = R3 * cosine/sine
	vaddpd	ymm0, ymm0, ymm5		;; A3 = A3 + I3
	vmulpd	ymm5, ymm5, ymm3		;; B3 = I3 * cosine/sine
	vsubpd	ymm5, ymm5, ymm2		;; B3 = B3 - R3

	vmovapd ymm3, [screg+64+32]		;; cosine/sine
	vmulpd	ymm2, ymm1, ymm3		;; A2 = R2 * cosine/sine
	vaddpd	ymm2, ymm2, ymm7		;; A2 = A2 + I2
	vmulpd	ymm7, ymm7, ymm3		;; B2 = I2 * cosine/sine
	vsubpd	ymm7, ymm7, ymm1		;; B2 = B2 - R2

	vmovapd ymm1, [screg+0]			;; sine
	vmulpd	ymm4, ymm4, ymm1		;; A1 = A1 * sine
	vmulpd	ymm6, ymm6, ymm1		;; B1 = B1 * sine

	vmovapd ymm3, [screg+192+32]		;; cosine/sine
	vmulpd	ymm1, ymm3, [srcreg]		;; A4 = R4 * cosine/sine
	vmulpd	ymm3, ymm3, [srcreg+32]		;; B4 = I4 * cosine/sine
	vaddpd	ymm1, ymm1, [srcreg+32]		;; A4 = A4 + I4
	vsubpd	ymm3, ymm3, [srcreg]		;; B4 = B4 - R4

	vmovapd	[srcreg], ymm4			;; Save R1
	vmovapd	[srcreg+32], ymm6		;; Save I1

	vmovapd ymm4, [screg+128]		;; sine
	vmulpd	ymm0, ymm0, ymm4		;; A3 = A3 * sine
	vmulpd	ymm5, ymm5, ymm4		;; B3 = B3 * sine
	vmovapd ymm4, [screg+64]		;; sine
	vmulpd	ymm2, ymm2, ymm4		;; A2 = A2 * sine
	vmulpd	ymm7, ymm7, ymm4		;; B2 = B2 * sine
	vmovapd ymm4, [screg+192]		;; sine
	vmulpd	ymm1, ymm1, ymm4		;; A4 = A4 * sine
	vmulpd	ymm3, ymm3, ymm4		;; B4 = B4 * sine

	vmovapd	[srcreg+d1], ymm2		;; Save R2
	vmovapd	[srcreg+d1+32], ymm7		;; Save I2
	vmovapd	[srcreg+d2], ymm0		;; Save R3
	vmovapd	[srcreg+d2+32], ymm5		;; Save I3
	vmovapd	[srcreg+d2+d1], ymm1		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm3		;; Save I4
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; 64-bit version

IFDEF X86_64

yr4_4cl_csc_four_complex_last_djbunfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	yr4_4c_last_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 3,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 4,exec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 5,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, 5*srcinc
	bump	screg, 5*scinc
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	yr4_4c_last_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 3,exec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 4,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	screg, 4*scinc
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	yr4_4c_last_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 2,exec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 3,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	screg, 3*scinc
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	yr4_4c_last_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 1,exec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 2,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	screg, 2*scinc
	ELSE
	yr4_4c_last_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 0,noexec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_last_djbunfft_unroll 1,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDIF
	ENDM

yr4_4c_last_djbunfft_unroll MACRO iter,prev,this,next,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<ymm0>
y1	TEXTEQU	<ymm1>
y2	TEXTEQU	<ymm2>
y3	TEXTEQU	<ymm3>
y4	TEXTEQU	<ymm4>
y5	TEXTEQU	<ymm5>
y6	TEXTEQU	<ymm6>
y7	TEXTEQU	<ymm7>
y8	TEXTEQU	<ymm8>
y9	TEXTEQU	<ymm9>
y10	TEXTEQU	<ymm10>
y11	TEXTEQU	<ymm11>
y12	TEXTEQU	<ymm12>
y13	TEXTEQU	<ymm13>
y14	TEXTEQU	<ymm14>
y15	TEXTEQU	<ymm15>
	ENDIF

;; On later calls, previous A2,B2,A3,B3,A1,B1 will be in y0-5.  This R3,I3,R4,I4,A2,B2,A3,B3,A4,B4 will be in y6-15.

this	vsubpd	y14, y14, y9		;; A4 = A4 - I4 (R4/sine)		; 1-3
prev	vmovapd y9, [screg+(iter-1)*scinc+64] ;; sine
prev	vmulpd	y1, y1, y9		;; B2 = B2 * sine			;	0-4
prev	vmulpd	y0, y0, y9		;; A2 = A2 * sine			;	1-5

this	vaddpd	y12, y12, y7		;; A3 = A3 + I3 (R3/sine)		; 2-4
this	vmovapd	y9, [screg+iter*scinc+256+64] ;; sine
this	vmulpd	y11, y11, y9		;; B2 = B2 * sine (new I2)		;	2-6

this	vaddpd	y15, y15, y8		;; B4 = B4 + R4 (I4/sine)		; 3-5
this	vmulpd	y10, y10, y9		;; A2 = A2 * sine (new R2)		;	3-7
prev	vmovapd y7, [screg+(iter-1)*scinc+128] ;; sine

this	vsubpd	y13, y13, y6		;; B3 = B3 - R3 (I3/sine)		; 4-6
prev	vmulpd	y2, y2, y7		;; A3 = A3 * sine			;	4-8
prev	vmovapd y8, [screg+(iter-1)*scinc+0] ;; sine

this	vsubpd	y6, y14, y12		;; R4/sine - R3/sine (new I4/sine)	; 5-7
prev	vmulpd	y3, y3, y7		;; B3 = B3 * sine			;	5-9
this	vmovapd	y9, [screg+iter*scinc+256+0] ;; sine
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1+32], y1 ;; Save I2			; 5

this	vaddpd	y14, y14, y12		;; R4/sine + R3/sine (new R3/sine)	; 6-8
prev	vmulpd	y4, y4, y8		;; A1 = A1 * sine			;	6-10
this	vmovapd	y7, [srcreg+iter*srcinc+32] ;; I1
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1], y0 ;; Save R2			; 6

this	vsubpd	y0, y13, y15		;; I3/sine - I4/sine (new R4/sine)	; 7-9
prev	vmulpd	y5, y5, y8		;; B1 = B1 * sine			;	7-11
this	vmovapd	y1, [srcreg+iter*srcinc] ;; R1

this	vaddpd	y13, y13, y15		;; I3/sine + I4/sine (new I3/sine)	; 8-10
this	vmulpd	y6, y6, y9		;; new I4/sine * sine			;	8-12
next	vmovapd	y12, [screg+(iter+1)*scinc+256+64+32] ;; cosine/sine

this	vsubpd	y15, y7, y11		;; I1 - I2 (new I2)			; 9-11
this	vmulpd	y14, y14, y9		;; new R3/sine * sine			;	9-13
next	vmovapd	y8, [srcreg+(iter+1)*srcinc+d1+32] ;; I2
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2], y2 ;; Save R3			; 9

this	vaddpd	y7, y7, y11		;; I1 + I2 (new I1)			; 10-12
this	vmulpd	y0, y0, y9		;; new R4/sine * sine			;	10-14
next	vmovapd	y2, [srcreg+(iter+1)*srcinc+d1] ;; R2
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+32], y3 ;; Save I3			; 10

this	vsubpd	y3, y1, y10		;; R1 - R2 (new R2)			; 11-13
this	vmulpd	y13, y13, y9		;; new I3/sine * sine			;	11-15
this	vmovapd y11, [screg+iter*scinc+192+32]	;; cosine/sine
prev	vmovapd	[srcreg+(iter-1)*srcinc], y4 ;; Save R1				; 11

this	vaddpd	y1, y1, y10		;; R1 + R2 (new R1)			; 12-14
prev	vmovapd	[srcreg+(iter-1)*srcinc+32], y5 ;; Save I1			; 12

this	vsubpd	y5, y15, y6		;; I2 - I4 (newer I4)			; 13-15
this next yloop_unrolled_one

this	vaddpd	y15, y15, y6		;; I2 + I4 (newer I2)			; 14-16
next	vmulpd	y6, y8, y12		;; B2 = I2 * cosine/sine		;	14-18
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

this	vsubpd	y10, y3, y0		;; R2 - R4 (newer R4)			; 15-17
next	vmulpd	y12, y2, y12		;; A2 = R2 * cosine/sine		;	15-19

this	vaddpd	y3, y3, y0		;; R2 + R4 (newer R2)			; 16-18
this	vmulpd	y0, y5, y11		;; B4 = I4 * cosine/sine		;	16-20
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	vsubpd	y4, y1, y14		;; R1 - R3 (newer R3)			; 17-19

this	vsubpd	y9, y7, y13		;; I1 - I3 (newer I3)			; 18-20
this	L1prefetchw srcreg+iter*srcinc+d2+L1pd, L1pt

this	vaddpd	y1, y1, y14		;; R1 + R3 (newer R1)			; 19-21
this	vmovapd y14, [screg+iter*scinc+64+32] ;; cosine/sine

this	vaddpd	y7, y7, y13		;; I1 + I3 (newer I1)			; 20-22
this	vmulpd	y13, y15, y14		;; B2 = I2 * cosine/sine		;	17-21
this	vmulpd	y11, y10, y11		;; A4 = R4 * cosine/sine		;	18-22
this	vmulpd	y14, y3, y14		;; A2 = R2 * cosine/sine		;	19-23

this	vsubpd	y0, y0, y10		;; B4 = B4 - R4				; 21-23
this	vmovapd y10, [screg+iter*scinc+128+32] ;; cosine/sine

this	vsubpd	y13, y13, y3		;; B2 = B2 - R2				; 22-24
this	vmulpd	y3, y4, y10		;; A3 = R3 * cosine/sine		;	20-24
this	vmulpd	y10, y9, y10		;; B3 = I3 * cosine/sine		;	21-25
this	L1prefetchw srcreg+iter*srcinc+d2+d1+L1pd, L1pt

this	vaddpd	y11, y11, y5		;; A4 = A4 + I4				; 23-25
this	vmovapd y5, [screg+iter*scinc+0+32] ;; cosine/sine

this	vaddpd	y14, y14, y15		;; A2 = A2 + I2				; 24-26
this	vmulpd	y15, y1, y5		;; A1 = R1 * cosine/sine		;	22-26
this	vmulpd	y5, y7, y5		;; B1 = I1 * cosine/sine		;	23-27

next	vsubpd	y6, y6, y2		;; B2 = B2 - R2				; 25-27
this	vmovapd y2, [screg+iter*scinc+192] ;; sine
this	vmulpd	y0, y0, y2		;; B4 = B4 * sine			;	24-28
this	vmovapd	[srcreg+iter*srcinc+d2+d1+32], y0 ;; Save I4			; 29
next	vmovapd	y0, [screg+(iter+1)*scinc+256+0+32] ;; cosine/sine

next	vaddpd	y12, y12, y8		;; A2 = A2 + I2				; 26-28
next	vmovapd	y8, [srcreg+(iter+1)*srcinc+d2+d1] ;; R4

this	vaddpd	y3, y3, y9		;; A3 = A3 + I3				; 27-29
next	vmulpd	y9, y8, y0		;; A4 = R4 * cosine/sine		;	25-29
this	vmulpd	y11, y11, y2		;; A4 = A4 * sine			;	26-30
next	vmovapd	y2, [srcreg+(iter+1)*srcinc+d2] ;; R3
this	vmovapd	[srcreg+iter*srcinc+d2+d1], y11 ;; Save R4			; 31
next	vmulpd	y11, y2, y0		;; A3 = R3 * cosine/sine		;	27-31

this	vsubpd	y10, y10, y4		;; B3 = B3 - R3				; 28-30
next	vmovapd	y4, [srcreg+(iter+1)*srcinc+d2+d1+32] ;; I4

this	vaddpd	y15, y15, y7		;; A1 = A1 + I1				; 29-31
next	vmulpd	y7, y4, y0		;; B4 = I4 * cosine/sine		;	28-32

this	vsubpd	y5, y5, y1		;; B1 = B1 - R1				; 30-32
next	vmovapd	y1, [srcreg+(iter+1)*srcinc+d2+32] ;; I3
next	vmulpd	y0, y1, y0		;; B3 = I3 * cosine/sine		;	29-33

;; Shuffle register assignments so that next call has A2,B2,A3,B3,A1,B1 in y0-5 and next R3,I3,R4,I4,A2,B2,A3,B3,A4,B4 will be in y6-15.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y14
y14	TEXTEQU	y9
y9	TEXTEQU	y4
y4	TEXTEQU	y15
y15	TEXTEQU	y7
y7	TEXTEQU	y1
y1	TEXTEQU	y13
y13	TEXTEQU	ytmp
ytmp	TEXTEQU	y2
y2	TEXTEQU	y3
y3	TEXTEQU	y10
y10	TEXTEQU	y12
y12	TEXTEQU y11
y11	TEXTEQU	y6
y6	TEXTEQU	ytmp
	ENDM

ENDIF


;;
;; ******************************* four-complex-with-partial-normalization variants *************************************
;;
;; These macros are used in pass 1 of r4dwpn two pass FFTs.  They are like the standard four-complex
;; DJBFFT macros except that a normalization multiplier has been pre-applied to the sine multiplier.
;; Consequently, the forward FFT and inverse FFT use different sine multipliers.
;; Also, a normalization multiplier must be applied to the final R1/I1 value.
;;

old_yr4_b4cl_wpn_four_complex_djbfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd	ymm0, [srcreg]		;; R1
	vmovapd	ymm6, [srcreg+d2]	;; R3
	vaddpd	ymm2, ymm0, ymm6	;; R1 + R3 (new R1)		; 1-3

	vmovapd	ymm1, [srcreg+d1]	;; R2
	vmovapd	ymm7, [srcreg+d2+d1]	;; R4
	vaddpd	ymm3, ymm1, ymm7	;; R2 + R4 (new R2)		; 2-4

	vsubpd	ymm0, ymm0, ymm6	;; R1 - R3 (new R3)		; 3-5
	vsubpd	ymm1, ymm1, ymm7	;; R2 - R4 (new R4)		; 4-6

	vmovapd	ymm4, [srcreg+32]	;; I1
	vmovapd	ymm7, [srcreg+d2+32]	;; I3
	vaddpd	ymm6, ymm4, ymm7	;; I1 + I3 (new I1)		; 5-7

	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	ymm5, ymm2, ymm3	;; R1 - R2 (final R2)		; 6-8
	vaddpd	ymm2, ymm2, ymm3	;; R1 + R2 (final R1)		; 7-9

	vbroadcastsd ymm3, Q [screg]	;; Load normalization multiplier for R1
	vmulpd	ymm2, ymm2, ymm3	;; Apply normalization multiplier to R1
	vmovapd	[srcreg], ymm2		;; Save R1

	vmovapd	ymm2, [srcreg+d1+32]	;; I2
	vaddpd	ymm3, ymm2, [srcreg+d2+d1+32] ;; I2 + I4 (new I2)	; 8-10
	vsubpd	ymm2, ymm2, [srcreg+d2+d1+32] ;; I2 - I4 (new I4)	; 9-11

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm4, ymm4, ymm7	;; I1 - I3 (new I3)		; 10-12

	vsubpd	ymm7, ymm6, ymm3	;; I1 - I2 (final I2)		; 11-13
	vaddpd	ymm6, ymm6, ymm3	;; I1 + I2 (final I1)		; 12-14

	vbroadcastsd ymm3, Q [screg]	;; Load normalization multiplier for I1
	vmulpd	ymm6, ymm6, ymm3	;; Apply normalization multiplier to I1
	vmovapd	[srcreg+32], ymm6	;; Save I1

	vsubpd	ymm3, ymm0, ymm2	;; R3 - I4 (final R3)		; 13-15
	vaddpd	ymm6, ymm4, ymm1	;; I3 + R4 (final I3)		; 14-16

	L1prefetchw srcreg+d2+L1pd, L1pt

	vaddpd	ymm0, ymm0, ymm2	;; R3 + I4 (final R4)		; 15-17
	vsubpd	ymm4, ymm4, ymm1	;; I3 - R4 (final I4)		; 16-18

	vmovapd ymm1, [screg+128+32]	;; cosine/sine
	vmulpd	ymm2, ymm5, ymm1	;; A2 = R2 * cosine/sine	;  9-13
	vsubpd	ymm2, ymm2, ymm7	;; A2 = A2 - I2			; 17-19

	vmulpd	ymm7, ymm7, ymm1	;; B2 = I2 * cosine/sine	; 14-18
	vaddpd	ymm7, ymm7, ymm5	;; B2 = B2 + R2			; 19-21

	vmovapd ymm1, [screg+32+32]	;; cosine/sine
	vmulpd	ymm5, ymm3, ymm1	;; A3 = R3 * cosine/sine	; 16-20
	vsubpd	ymm5, ymm5, ymm6	;; A3 = A3 - I3			; 21-23

	vmulpd	ymm6, ymm6, ymm1	;; B3 = I3 * cosine/sine	; 17-21
	vaddpd	ymm6, ymm6, ymm3	;; B3 = B3 + R3			; 22-24

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vmulpd	ymm3, ymm0, ymm1	;; A4 = R4 * cosine/sine	; 18-22
	vaddpd	ymm3, ymm3, ymm4	;; A4 = A4 + I4			; 23-25

	vmulpd	ymm4, ymm4, ymm1	;; B4 = I4 * cosine/sine	; 19-23
	vsubpd	ymm4, ymm4, ymm0	;; B4 = B4 - R4			; 24-26

	vmovapd ymm1, [screg+128]	;; Sine
	vmulpd	ymm2, ymm2, ymm1	;; A2 = A2 * sine (final R2)	; 20-24
	vmulpd	ymm7, ymm7, ymm1	;; B2 = B2 * sine (final I2)	; 22-26

	vmovapd ymm1, [screg+32]	;; Sine
	vmulpd	ymm5, ymm5, ymm1	;; A3 = A3 * sine (final R3)	; 24-28
	vmulpd	ymm6, ymm6, ymm1	;; B3 = B3 * sine (final I3)	; 25-29

	vmulpd	ymm3, ymm3, ymm1	;; A4 = A4 * sine (final R4)	; 26-30
	vmulpd	ymm4, ymm4, ymm1	;; B4 = B4 * sine (final I4)	; 27-31

;;	vmovapd	[srcreg], ymm0		;; Save R1
;;	vmovapd	[srcreg+32], ymm0	;; Save I1
	vmovapd	[srcreg+d1], ymm2	;; Save R2
	vmovapd	[srcreg+d1+32], ymm7	;; Save I2
	vmovapd	[srcreg+d2], ymm5	;; Save R3
	vmovapd	[srcreg+d2+32], ymm6	;; Save I3
	vmovapd	[srcreg+d2+d1], ymm3	;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm4	;; Save I4
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM


old_yr4_b4cl_wpn_four_complex_djbunfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd ymm3, [screg+128+32]	;; cosine/sine
	vmovapd	ymm2, [srcreg+d1]	;; R2
	vmulpd	ymm6, ymm2, ymm3	;; A2 = R2 * cosine/sine		; 1-5
	vmovapd	ymm0, [srcreg+d1+32]	;; I2
	vmulpd	ymm3, ymm3, ymm0	;; B2 = I2 * cosine/sine		; 2-6

	vmovapd ymm5, [screg+32+32]	;; cosine/sine
	vmovapd	ymm4, [srcreg+d2]	;; R3
	vmulpd	ymm7, ymm4, ymm5	;; A3 = R3 * cosine/sine		; 3-7
	vmovapd	ymm1, [srcreg+d2+32]	;; I3

	vaddpd	ymm6, ymm6, ymm0	;; A2 = A2 + I2				; 6-8

	vmulpd	ymm0, ymm1, ymm5	;; B3 = I3 * cosine/sine		; 4-8

	vsubpd	ymm3, ymm3, ymm2	;; B2 = B2 - R2				; 7-9
	vaddpd	ymm7, ymm7, ymm1	;; A3 = A3 + I3				; 8-10

	vmovapd	ymm2, [srcreg+d2+d1]	;; R4
	vmulpd	ymm1, ymm2, ymm5	;; A4 = R4 * cosine/sine		; 5-9

	vsubpd	ymm0, ymm0, ymm4	;; B3 = B3 - R3				; 9-11

	vmovapd	ymm4, [srcreg+d2+d1+32]	;; I4
	vmulpd	ymm5, ymm4, ymm5	;; B4 = I4 * cosine/sine		; 6-10

	vsubpd	ymm1, ymm1, ymm4	;; A4 = A4 - I4				; 10-12
	vaddpd	ymm5, ymm5, ymm2	;; B4 = B4 + R4				; 11-13

	vbroadcastsd ymm2, Q [screg+8]	;; normalization_inverse
	vmulpd	ymm2, ymm2, [srcreg]	;; R1 * normalization_inverse		; 8-12

	vmovapd ymm4, [screg+128+64]	;; Sine * normalization_inverse
	vmulpd	ymm6, ymm6, ymm4	;; A2 = A2 * sine (new R2)		; 9-13
	vmulpd	ymm3, ymm3, ymm4	;; B2 = B2 * sine (new I2)		; 10-14

	L1prefetchw srcreg+L1pd, L1pt

	vmovapd ymm4, [screg+32+64]	;; Sine * normalization_inverse
	vmulpd	ymm7, ymm7, ymm4	;; A3 = A3 * sine (new R3)		; 11-15
	vmulpd	ymm0, ymm0, ymm4	;; B3 = B3 * sine (new I3)		; 12-16
	vmulpd	ymm1, ymm1, ymm4	;; A4 = A4 * sine (new R4)		; 13-17
	vmulpd	ymm5, ymm5, ymm4	;; B4 = B4 * sine (new I4)		; 14-18

	vaddpd	ymm4, ymm2, ymm6	;; R1 + R2 (new R1)			; 14-16
	vsubpd	ymm2, ymm2, ymm6	;; R1 - R2 (new R2)			; 15-17

	L1prefetchw srcreg+d1+L1pd, L1pt

	vaddpd	ymm6, ymm1, ymm7	;; R4 + R3 (new R3)			; 16-18
	vsubpd	ymm1, ymm1, ymm7	;; R4 - R3 (new I4)			; 17-19

	vsubpd	ymm7, ymm0, ymm5	;; I3 - I4 (new R4)			; 18-20
	vaddpd	ymm0, ymm0, ymm5	;; I3 + I4 (new I3)			; 19-21

	L1prefetchw srcreg+d2+L1pd, L1pt

	vsubpd	ymm5, ymm4, ymm6	;; R1 - R3 (final R3)			; 20-22
	vaddpd	ymm4, ymm4, ymm6	;; R1 + R3 (final R1)			; 21-23

	vbroadcastsd ymm6, Q [screg+8]	;; normalization_inverse
	vmulpd	ymm6, ymm6, [srcreg+32]	;; I1 * normalization_inverse		; 15-19
	vmovapd	[srcreg], ymm4		;; Store R1

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm4, ymm6, ymm3	;; I1 - I2 (new I2)			; 22-24
	vaddpd	ymm6, ymm6, ymm3	;; I1 + I2 (new I1)			; 23-25

	vsubpd	ymm3, ymm2, ymm7	;; R2 - R4 (final R4)			; 24-26
	vaddpd	ymm2, ymm2, ymm7	;; R2 + R4 (final R2)			; 25-27

	vsubpd	ymm7, ymm4, ymm1	;; I2 - I4 (final I4)			; 26-28
	vaddpd	ymm4, ymm4, ymm1	;; I2 + I4 (final I2)			; 27-29

	vsubpd	ymm1, ymm6, ymm0	;; I1 - I3 (final I3)			; 28-30
	vaddpd	ymm6, ymm6, ymm0	;; I1 + I3 (final I1)			; 29-31

;;	vmovapd	[srcreg], ymm0		;; Save R1
	vmovapd	[srcreg+32], ymm6	;; Save I1
	vmovapd	[srcreg+d1], ymm2	;; Save R2
	vmovapd	[srcreg+d1+32], ymm4	;; Save I2
	vmovapd	[srcreg+d2], ymm5	;; Save R3
	vmovapd	[srcreg+d2+32], ymm1	;; Save I3
	vmovapd	[srcreg+d2+d1], ymm3	;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm7	;; Save I4
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM


;; 64-bit version

IFDEF X86_64

old_yr4_b4cl_wpn_four_complex_djbfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	yr4_4c_wpn_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 3,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 4,exec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 5,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, 5*srcinc
	bump	screg, 5*scinc
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	yr4_4c_wpn_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 3,exec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 4,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	screg, 4*scinc
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	yr4_4c_wpn_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 2,exec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 3,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	screg, 3*scinc
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	yr4_4c_wpn_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 1,exec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 2,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	screg, 2*scinc
	ELSE
	yr4_4c_wpn_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 0,noexec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 1,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDIF
	ENDM

old_yr4_4c_wpn_djbfft_unroll MACRO iter,prev,this,next,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<ymm0>
y1	TEXTEQU	<ymm1>
y2	TEXTEQU	<ymm2>
y3	TEXTEQU	<ymm3>
y4	TEXTEQU	<ymm4>
y5	TEXTEQU	<ymm5>
y6	TEXTEQU	<ymm6>
y7	TEXTEQU	<ymm7>
y8	TEXTEQU	<ymm8>
y9	TEXTEQU	<ymm9>
y10	TEXTEQU	<ymm10>
y11	TEXTEQU	<ymm11>
y12	TEXTEQU	<ymm12>
y13	TEXTEQU	<ymm13>
y14	TEXTEQU	<ymm14>
y15	TEXTEQU	<ymm15>
	ENDIF

;; On later calls, previous R3,R4,I3,B4,A2,B2,sine1,sine2 will be in y0-7.  This R1,R3,I2,I4,I1,I3 will be in y8-13.
;; The remaining registers are free.

this	vsubpd	y14, y8, y9				;; R1 - R3 (new R3)		; 1-3
prev	vmulpd	y3, y3, y6				;; B4 = B4 * sine (final I4)	; 1-5
this	vmovapd	y6, [srcreg+iter*srcinc+d1]		;; R2

this	vaddpd	y8, y8, y9				;; R1 + R3 (new R1)		; 2-4
prev	vmulpd	y4, y4, y7				;; A2 = A2 * sine (final R2)	; 2-6
this	vmovapd	y9, [srcreg+iter*srcinc+d2+d1]		;; R4

this	vsubpd	y15, y10, y11				;; I2 - I4 (new I4)		; 3-5
prev	vmulpd	y5, y5, y7				;; B2 = B2 * sine (final I2)	; 3-7
this	vmovapd y7, [screg+iter*scinc+32+32]		;; cosine/sine
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2], y0		;; Save R3			; 3

this	vaddpd	y10, y10, y11				;; I2 + I4 (new I2)		; 4-6
this	vmovapd y11, [screg+iter*scinc+128+32]		;; cosine/sine
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+d1], y1	;; Save R4			; 4

this	vsubpd	y1, y12, y13				;; I1 - I3 (new I3)		; 5-7
this	vmovapd y0, [screg+iter*scinc+32]		;; Sine
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+32], y2	;; Save I3			; 5

this	vaddpd	y12, y12, y13				;; I1 + I3 (new I1)		; 6-8
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+d1+32], y3	;; Save I4			; 6
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

this	vsubpd	y3, y6, y9				;; R2 - R4 (new R4)		; 7-9
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1], y4		;; Save R2			; 7

this	vaddpd	y6, y6, y9				;; R2 + R4 (new R2)		; 8-10
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1+32], y5	;; Save I2			; 8

this	vsubpd	y5, y14, y15				;; R3 - I4 (final R3)		; 9-11
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	vaddpd	y14, y14, y15				;; R3 + I4 (final R4)		; 10-12

this	vaddpd	y15, y1, y3				;; I3 + R4 (final I3)		; 11-13

this	vsubpd	y1, y1, y3				;; I3 - R4 (final I4)		; 12-14
this	vmulpd	y3, y5, y7				;; A3 = R3 * cosine/sine	; 12-16
this	L1prefetchw srcreg+iter*srcinc+d2+L1pd, L1pt

this	vsubpd	y9, y8, y6				;; R1 - R2 (final R2)		; 13-15
this	vmulpd	y4, y14, y7				;; A4 = R4 * cosine/sine	; 13-17

this	vsubpd	y13, y12, y10				;; I1 - I2 (final I2)		; 14-16
this	vmulpd	y2, y15, y7				;; B3 = I3 * cosine/sine	; 14-18
this next yloop_unrolled_one

this	vaddpd	y8, y8, y6				;; R1 + R2 (final R1)		; 15-17
this	vmulpd	y7, y1, y7				;; B4 = I4 * cosine/sine	; 15-19
this	vbroadcastsd y6, Q [screg+iter*scinc]		;; Normalization multiplier for R1 & I1

this	vaddpd	y12, y12, y10				;; I1 + I2 (final I1)		; 16-18
this	vmulpd	y10, y9, y11				;; A2 = R2 * cosine/sine	; 16-20
this	L1prefetchw srcreg+iter*srcinc+d2+d1+L1pd, L1pt

this	vsubpd	y3, y3, y15				;; A3 = A3 - I3			; 17-19
this	vmulpd	y11, y13, y11				;; B2 = I2 * cosine/sine	; 17-21
next	vmovapd	y15, [srcreg+(iter+1)*srcinc]		;; R1

this	vaddpd	y4, y4, y1				;; A4 = A4 + I4			; 18-20
this	vmulpd	y8, y8, y6				;; Apply normalization multiplier to R1	; 18-22
next	vmovapd	y1, [srcreg+(iter+1)*srcinc+d2]		;; R3

this	vaddpd	y2, y2, y5				;; B3 = B3 + R3			; 19-21
this	vmulpd	y12, y12, y6				;; Apply normalization multiplier to I1 ; 19-23
this	vmovapd y5, [screg+iter*scinc+128]		;; Sine
next	vmovapd	y6, [srcreg+(iter+1)*srcinc+d1+32]	;; I2

this	vsubpd	y7, y7, y14				;; B4 = B4 - R4			; 20-22
this	vmulpd	y3, y3, y0				;; A3 = A3 * sine (final R3)	; 20-24
next	vmovapd	y14, [srcreg+(iter+1)*srcinc+d2+d1+32]	;; I4

this	vsubpd	y10, y10, y13				;; A2 = A2 - I2			; 21-23
this	vmulpd	y4, y4, y0				;; A4 = A4 * sine (final R4)	; 21-25
next	vmovapd	y13, [srcreg+(iter+1)*srcinc+32]	;; I1
this	vmovapd	[srcreg+iter*srcinc], y8		;; Save R1			; 23

this	vaddpd	y11, y11, y9				;; B2 = B2 + R2			; 22-24
this	vmulpd	y2, y2, y0				;; B3 = B3 * sine (final I3)	; 22-26
next	vmovapd	y9, [srcreg+(iter+1)*srcinc+d2+32]	;; I3
this	vmovapd	[srcreg+iter*srcinc+32], y12		;; Save I1			; 24

;; Shuffle register assignments so that next call has R3,R4,I3,B4,A2,B2,sine1,sine2 in y0-7 and next R1,R3,I2,I4,I1,I3 in y8-13.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y3
y3	TEXTEQU	y7
y7	TEXTEQU	y5
y5	TEXTEQU	y11
y11	TEXTEQU	y14
y14	TEXTEQU	y8
y8	TEXTEQU	y15
y15	TEXTEQU	y12
y12	TEXTEQU	y13
y13	TEXTEQU y9
y9	TEXTEQU	y1
y1	TEXTEQU	y4
y4	TEXTEQU	y10
y10	TEXTEQU	y6
y6	TEXTEQU	ytmp
	ENDM

old_yr4_b4cl_wpn_four_complex_djbunfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	yr4_4c_wpn_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 3,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 4,exec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 5,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, 5*srcinc
	bump	screg, 5*scinc
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	yr4_4c_wpn_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 3,exec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 4,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	screg, 4*scinc
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	yr4_4c_wpn_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 2,exec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 3,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	screg, 3*scinc
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	yr4_4c_wpn_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 1,exec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 2,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	screg, 2*scinc
	ELSE
	yr4_4c_wpn_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 0,noexec,exec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 1,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDIF
	ENDM

old_yr4_4c_wpn_djbunfft_unroll MACRO iter,prev,this,next,srcreg,srcinc,d1,d2,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<ymm0>
y1	TEXTEQU	<ymm1>
y2	TEXTEQU	<ymm2>
y3	TEXTEQU	<ymm3>
y4	TEXTEQU	<ymm4>
y5	TEXTEQU	<ymm5>
y6	TEXTEQU	<ymm6>
y7	TEXTEQU	<ymm7>
y8	TEXTEQU	<ymm8>
y9	TEXTEQU	<ymm9>
y10	TEXTEQU	<ymm10>
y11	TEXTEQU	<ymm11>
y12	TEXTEQU	<ymm12>
y13	TEXTEQU	<ymm13>
y14	TEXTEQU	<ymm14>
y15	TEXTEQU	<ymm15>
	ENDIF

;; On later calls, previous R2,I3,I1 will be in y0-2.  This R4,R3,R2,I4,I3,I2,A4,A3,A2,B4,B3,c/s2 will be in y3-14.
;; The remaining register is free.

this	vsubpd	y9, y9, y6			;; A4 = A4 - I4 (new R4/sine)		; 1-3
this	vmulpd	y14, y8, y14			;; B2 = I2 * cosine/sine		; 1-5
this	vbroadcastsd y15, Q [screg+iter*scinc+8] ;; normalization_inverse for R1 & I1

this	vaddpd	y10, y10, y7			;; A3 = A3 + I3 (new R3/sine)		; 2-4
this	vmovapd	y6, [srcreg+iter*srcinc]	;; R1

this	vaddpd	y11, y11, y8			;; A2 = A2 + I2				; 3-5
this	vmulpd	y6, y6, y15			;; R1 * normalization_inverse		; 3-7
this	vmovapd	y7, [srcreg+iter*srcinc+32]	;; I1

this	vaddpd	y12, y12, y3			;; B4 = B4 + R4 (new I4/sine)		; 4-6
this	vmulpd	y7, y7, y15			;; I1 * normalization_inverse		; 4-8
this	vmovapd y8, [screg+iter*scinc+128+64]	;; Sine

this	vsubpd	y13, y13, y4			;; B3 = B3 - R3 (new I3/sine)		; 5-7
this	vmovapd y3, [screg+iter*scinc+32+64]	;; Sine
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1], y0	;; Save R2				; 1

this	vsubpd	y14, y14, y5			;; B2 = B2 - R2				; 6-8
this	vmulpd	y11, y11, y8			;; A2 = A2 * sine (new R2)		; 6-10
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+32], y1 ;; Save I3				; 2
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

this	vaddpd	y1, y9, y10			;; C3 = R4/sine + R3/sine (newer R3/sine) ; 7-9
prev	vmovapd	[srcreg+(iter-1)*srcinc+32], y2	;; Save I1				; 3

this	vsubpd	y9, y9, y10			;; D4 = R4/sine - R3/sine (newer I4/sine) ; 8-10
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	vsubpd	y10, y13, y12			;; C4 = I3/sine - I4/sine (newer R4/sine) ; 9-11
this	vmulpd	y14, y14, y8			;; B2 = B2 * sine (new I2)		; 9-13
next	vmovapd y15, [screg+(iter+1)*scinc+32+32] ;; cosine/sine

this	vaddpd	y13, y13, y12			;; D3 = I3/sine + I4/sine (newer I3/sine) ; 10-12
this	vmulpd	y1, y1, y3			;; C3 * sine (newer R3)			; 10-14
next	vmovapd	y4, [srcreg+(iter+1)*srcinc+d2+d1] ;; R4

this	vaddpd	y12, y6, y11			;; R1 + R2 (newer R1)			; 11-13
this	vmulpd	y9, y9, y3			;; D4 * sine (newer I4)			; 11-15
next	vmovapd	y0, [srcreg+(iter+1)*srcinc+d2]	;; R3

this	vsubpd	y6, y6, y11			;; R1 - R2 (newer R2)			; 12-14
this	vmulpd	y10, y10, y3			;; C4 * sine (newer R4)			; 12-16
next	vmovapd y5, [screg+(iter+1)*scinc+128+32] ;; cosine/sine

this	vsubpd	y11, y7, y14			;; I1 - I2 (newer I2)			; 13-15
this	vmulpd	y13, y13, y3			;; D3 * sine (newer I3)			; 13-17
next	vmovapd	y2, [srcreg+(iter+1)*srcinc+d1]	;; R2

this	vaddpd	y7, y7, y14			;; I1 + I2 (newer I1)			; 14-16
this	L1prefetchw srcreg+iter*srcinc+d2+L1pd, L1pt

this	vsubpd	y14, y12, y1			;; R1 - R3 (final R3)			; 15-17
next	vmovapd	y8, [srcreg+(iter+1)*srcinc+d2+d1+32] ;; I4

this	vaddpd	y12, y12, y1			;; R1 + R3 (final R1)			; 16-18
next	vmovapd	y3, [srcreg+(iter+1)*srcinc+d2+32] ;; I3

this	vmovapd	[srcreg+iter*srcinc+d2], y14	;; Save R3				; 18
this	vsubpd	y14, y11, y9			;; I2 - I4 (final I4)			; 17-19
next	vmovapd	y1, [srcreg+(iter+1)*srcinc+d1+32] ;; I2

this	vaddpd	y11, y11, y9			;; I2 + I4 (final I2)			; 18-20
next	vmulpd	y9, y4, y15			;; A4 = R4 * cosine/sine		; 18-22
this	L1prefetchw srcreg+iter*srcinc+d2+d1+L1pd, L1pt

this	vmovapd	[srcreg+iter*srcinc], y12	;; Save R1				; 19
this	vsubpd	y12, y6, y10			;; R2 - R4 (final R4)			; 19-21
this	vmovapd	[srcreg+iter*srcinc+d2+d1+32], y14 ;; Save I4				; 20
next	vmulpd	y14, y0, y15			;; A3 = R3 * cosine/sine		; 19-23

this	vaddpd	y6, y6, y10			;; R2 + R4 (final R2)			; 20-22
next	vmulpd	y10, y2, y5			;; A2 = R2 * cosine/sine		; 20-24

this	vmovapd	[srcreg+iter*srcinc+d1+32], y11	;; Save I2				; 21
this	vsubpd	y11, y7, y13			;; I1 - I3 (final I3)			; 21-23
this	vmovapd	[srcreg+iter*srcinc+d2+d1], y12	;; Save R4				; 22
next	vmulpd	y12, y8, y15			;; B4 = I4 * cosine/sine		; 21-25

this	vaddpd	y7, y7, y13			;; I1 + I3 (final I1)			; 22-24
next	vmulpd	y13, y3, y15			;; B3 = I3 * cosine/sine		; 22-26
this next yloop_unrolled_one

;; Shuffle register assignments so that next call has R2,I3,I1 in y0-2 and next R4,R3,R2,I4,I3,I2,A4,A3,A2,B4,B3,c/s2 in y3-14.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y6
y6	TEXTEQU y8
y8	TEXTEQU	y1
y1	TEXTEQU	y11
y11	TEXTEQU	y10
y10	TEXTEQU	y14
y14	TEXTEQU	y5
y5	TEXTEQU	y2
y2	TEXTEQU	y7
y7	TEXTEQU	y3
y3	TEXTEQU	y4
y4	TEXTEQU	ytmp
	ENDM

ENDIF

;; In this version of the _wpn macros, we use 2 s/c ptrs to save 16 bytes of memory.
;; We also group the data used in the forward and inverse FFT together on their own cache lines.

;; screg1 is normalization weights for R1/I1
;; screg2+0, screg2+64+0, screg2+128+0 is weighted sin/cos values for R2/I2 (w^n)
;; screg2+0+32, screg2+64+32, screg2+128+32 is weighted sin/cos values for R3/I3 (w^2n)

yr4_b4cl_wpn_four_complex_djbfft MACRO srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	vmovapd	ymm0, [srcreg]		;; R1
	vmovapd	ymm6, [srcreg+d2]	;; R3
	vaddpd	ymm2, ymm0, ymm6	;; R1 + R3 (new R1)		; 1-3

	vmovapd	ymm1, [srcreg+d1]	;; R2
	vmovapd	ymm7, [srcreg+d2+d1]	;; R4
	vaddpd	ymm3, ymm1, ymm7	;; R2 + R4 (new R2)		; 2-4

	vsubpd	ymm0, ymm0, ymm6	;; R1 - R3 (new R3)		; 3-5
	vsubpd	ymm1, ymm1, ymm7	;; R2 - R4 (new R4)		; 4-6

	vmovapd	ymm4, [srcreg+32]	;; I1
	vmovapd	ymm7, [srcreg+d2+32]	;; I3
	vaddpd	ymm6, ymm4, ymm7	;; I1 + I3 (new I1)		; 5-7

	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	ymm5, ymm2, ymm3	;; R1 - R2 (final R2)		; 6-8
	vaddpd	ymm2, ymm2, ymm3	;; R1 + R2 (final R1)		; 7-9

	vbroadcastsd ymm3, Q [screg1]	;; Load normalization multiplier for R1
	vmulpd	ymm2, ymm2, ymm3	;; Apply normalization multiplier to R1
	vmovapd	[srcreg], ymm2		;; Save R1

	vmovapd	ymm2, [srcreg+d1+32]	;; I2
	vaddpd	ymm3, ymm2, [srcreg+d2+d1+32] ;; I2 + I4 (new I2)	; 8-10
	vsubpd	ymm2, ymm2, [srcreg+d2+d1+32] ;; I2 - I4 (new I4)	; 9-11

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm4, ymm4, ymm7	;; I1 - I3 (new I3)		; 10-12

	vsubpd	ymm7, ymm6, ymm3	;; I1 - I2 (final I2)		; 11-13
	vaddpd	ymm6, ymm6, ymm3	;; I1 + I2 (final I1)		; 12-14

	vbroadcastsd ymm3, Q [screg1]	;; Load normalization multiplier for I1
	vmulpd	ymm6, ymm6, ymm3	;; Apply normalization multiplier to I1
	vmovapd	[srcreg+32], ymm6	;; Save I1

	vsubpd	ymm3, ymm0, ymm2	;; R3 - I4 (final R3)		; 13-15
	vaddpd	ymm6, ymm4, ymm1	;; I3 + R4 (final I3)		; 14-16

	L1prefetchw srcreg+d2+L1pd, L1pt

	vaddpd	ymm0, ymm0, ymm2	;; R3 + I4 (final R4)		; 15-17
	vsubpd	ymm4, ymm4, ymm1	;; I3 - R4 (final I4)		; 16-18

	vmovapd ymm1, [screg2+32]	;; cosine/sine
	vmulpd	ymm2, ymm5, ymm1	;; A2 = R2 * cosine/sine	;  9-13
	vsubpd	ymm2, ymm2, ymm7	;; A2 = A2 - I2			; 17-19

	vmulpd	ymm7, ymm7, ymm1	;; B2 = I2 * cosine/sine	; 14-18
	vaddpd	ymm7, ymm7, ymm5	;; B2 = B2 + R2			; 19-21

	vmovapd ymm1, [screg2+0]	;; cosine/sine
	vmulpd	ymm5, ymm3, ymm1	;; A3 = R3 * cosine/sine	; 16-20
	vsubpd	ymm5, ymm5, ymm6	;; A3 = A3 - I3			; 21-23

	vmulpd	ymm6, ymm6, ymm1	;; B3 = I3 * cosine/sine	; 17-21
	vaddpd	ymm6, ymm6, ymm3	;; B3 = B3 + R3			; 22-24

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vmulpd	ymm3, ymm0, ymm1	;; A4 = R4 * cosine/sine	; 18-22
	vaddpd	ymm3, ymm3, ymm4	;; A4 = A4 + I4			; 23-25

	vmulpd	ymm4, ymm4, ymm1	;; B4 = I4 * cosine/sine	; 19-23
	vsubpd	ymm4, ymm4, ymm0	;; B4 = B4 - R4			; 24-26

	vmovapd ymm1, [screg2+64+32]	;; Sine
	vmulpd	ymm2, ymm2, ymm1	;; A2 = A2 * sine (final R2)	; 20-24
	vmulpd	ymm7, ymm7, ymm1	;; B2 = B2 * sine (final I2)	; 22-26

	vmovapd ymm1, [screg2+64+0]	;; Sine
	vmulpd	ymm5, ymm5, ymm1	;; A3 = A3 * sine (final R3)	; 24-28
	vmulpd	ymm6, ymm6, ymm1	;; B3 = B3 * sine (final I3)	; 25-29

	vmulpd	ymm3, ymm3, ymm1	;; A4 = A4 * sine (final R4)	; 26-30
	vmulpd	ymm4, ymm4, ymm1	;; B4 = B4 * sine (final I4)	; 27-31

;;	vmovapd	[srcreg], ymm0		;; Save R1
;;	vmovapd	[srcreg+32], ymm0	;; Save I1
	vmovapd	[srcreg+d1], ymm2	;; Save R2
	vmovapd	[srcreg+d1+32], ymm7	;; Save I2
	vmovapd	[srcreg+d2], ymm5	;; Save R3
	vmovapd	[srcreg+d2+32], ymm6	;; Save I3
	vmovapd	[srcreg+d2+d1], ymm3	;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm4	;; Save I4
	bump	srcreg, srcinc
	bump	screg1, scinc1
	bump	screg2, scinc2
	ENDM


yr4_b4cl_wpn_four_complex_djbunfft MACRO srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	vmovapd ymm3, [screg2+32]	;; cosine/sine
	vmovapd	ymm2, [srcreg+d1]	;; R2
	vmulpd	ymm6, ymm2, ymm3	;; A2 = R2 * cosine/sine		; 1-5
	vmovapd	ymm0, [srcreg+d1+32]	;; I2
	vmulpd	ymm3, ymm3, ymm0	;; B2 = I2 * cosine/sine		; 2-6

	vmovapd ymm5, [screg2+0]	;; cosine/sine
	vmovapd	ymm4, [srcreg+d2]	;; R3
	vmulpd	ymm7, ymm4, ymm5	;; A3 = R3 * cosine/sine		; 3-7
	vmovapd	ymm1, [srcreg+d2+32]	;; I3

	vaddpd	ymm6, ymm6, ymm0	;; A2 = A2 + I2				; 6-8

	vmulpd	ymm0, ymm1, ymm5	;; B3 = I3 * cosine/sine		; 4-8

	vsubpd	ymm3, ymm3, ymm2	;; B2 = B2 - R2				; 7-9
	vaddpd	ymm7, ymm7, ymm1	;; A3 = A3 + I3				; 8-10

	vmovapd	ymm2, [srcreg+d2+d1]	;; R4
	vmulpd	ymm1, ymm2, ymm5	;; A4 = R4 * cosine/sine		; 5-9

	vsubpd	ymm0, ymm0, ymm4	;; B3 = B3 - R3				; 9-11

	vmovapd	ymm4, [srcreg+d2+d1+32]	;; I4
	vmulpd	ymm5, ymm4, ymm5	;; B4 = I4 * cosine/sine		; 6-10

	vsubpd	ymm1, ymm1, ymm4	;; A4 = A4 - I4				; 10-12
	vaddpd	ymm5, ymm5, ymm2	;; B4 = B4 + R4				; 11-13

	vbroadcastsd ymm2, Q [screg1]	;; normalization_inverse
	vmulpd	ymm2, ymm2, [srcreg]	;; R1 * normalization_inverse		; 8-12

	vmovapd ymm4, [screg2+128+32]	;; Sine * normalization_inverse
	vmulpd	ymm6, ymm6, ymm4	;; A2 = A2 * sine (new R2)		; 9-13
	vmulpd	ymm3, ymm3, ymm4	;; B2 = B2 * sine (new I2)		; 10-14

	L1prefetchw srcreg+L1pd, L1pt

	vmovapd ymm4, [screg2+128+0]	;; Sine * normalization_inverse
	vmulpd	ymm7, ymm7, ymm4	;; A3 = A3 * sine (new R3)		; 11-15
	vmulpd	ymm0, ymm0, ymm4	;; B3 = B3 * sine (new I3)		; 12-16
	vmulpd	ymm1, ymm1, ymm4	;; A4 = A4 * sine (new R4)		; 13-17
	vmulpd	ymm5, ymm5, ymm4	;; B4 = B4 * sine (new I4)		; 14-18

	vaddpd	ymm4, ymm2, ymm6	;; R1 + R2 (new R1)			; 14-16
	vsubpd	ymm2, ymm2, ymm6	;; R1 - R2 (new R2)			; 15-17

	L1prefetchw srcreg+d1+L1pd, L1pt

	vaddpd	ymm6, ymm1, ymm7	;; R4 + R3 (new R3)			; 16-18
	vsubpd	ymm1, ymm1, ymm7	;; R4 - R3 (new I4)			; 17-19

	vsubpd	ymm7, ymm0, ymm5	;; I3 - I4 (new R4)			; 18-20
	vaddpd	ymm0, ymm0, ymm5	;; I3 + I4 (new I3)			; 19-21

	L1prefetchw srcreg+d2+L1pd, L1pt

	vsubpd	ymm5, ymm4, ymm6	;; R1 - R3 (final R3)			; 20-22
	vaddpd	ymm4, ymm4, ymm6	;; R1 + R3 (final R1)			; 21-23

	vbroadcastsd ymm6, Q [screg1]	;; normalization_inverse
	vmulpd	ymm6, ymm6, [srcreg+32]	;; I1 * normalization_inverse		; 15-19
	vmovapd	[srcreg], ymm4		;; Store R1

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm4, ymm6, ymm3	;; I1 - I2 (new I2)			; 22-24
	vaddpd	ymm6, ymm6, ymm3	;; I1 + I2 (new I1)			; 23-25

	vsubpd	ymm3, ymm2, ymm7	;; R2 - R4 (final R4)			; 24-26
	vaddpd	ymm2, ymm2, ymm7	;; R2 + R4 (final R2)			; 25-27

	vsubpd	ymm7, ymm4, ymm1	;; I2 - I4 (final I4)			; 26-28
	vaddpd	ymm4, ymm4, ymm1	;; I2 + I4 (final I2)			; 27-29

	vsubpd	ymm1, ymm6, ymm0	;; I1 - I3 (final I3)			; 28-30
	vaddpd	ymm6, ymm6, ymm0	;; I1 + I3 (final I1)			; 29-31

;;	vmovapd	[srcreg], ymm0		;; Save R1
	vmovapd	[srcreg+32], ymm6	;; Save I1
	vmovapd	[srcreg+d1], ymm2	;; Save R2
	vmovapd	[srcreg+d1+32], ymm4	;; Save I2
	vmovapd	[srcreg+d2], ymm5	;; Save R3
	vmovapd	[srcreg+d2+32], ymm1	;; Save I3
	vmovapd	[srcreg+d2+d1], ymm3	;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm7	;; Save I4
	bump	srcreg, srcinc
	bump	screg1, scinc1
	bump	screg2, scinc2
	ENDM


;; 64-bit version

IFDEF X86_64

yr4_b4cl_wpn_four_complex_djbfft MACRO srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	yr4_4c_wpn_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 3,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 4,exec,exec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 5,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	bump	srcreg, 5*srcinc
	bump	screg1, 5*scinc1
	bump	screg2, 5*scinc2
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	yr4_4c_wpn_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 3,exec,exec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 4,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	screg1, 4*scinc1
	bump	screg2, 4*scinc2
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	yr4_4c_wpn_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 2,exec,exec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 3,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	screg1, 3*scinc1
	bump	screg2, 3*scinc2
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	yr4_4c_wpn_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 1,exec,exec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 2,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	screg1, 2*scinc1
	bump	screg2, 2*scinc2
	ELSE
	yr4_4c_wpn_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 0,noexec,exec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbfft_unroll 1,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	bump	srcreg, srcinc
	bump	screg1, scinc1
	bump	screg2, scinc2
	ENDIF
	ENDM

yr4_4c_wpn_djbfft_unroll MACRO iter,prev,this,next,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<ymm0>
y1	TEXTEQU	<ymm1>
y2	TEXTEQU	<ymm2>
y3	TEXTEQU	<ymm3>
y4	TEXTEQU	<ymm4>
y5	TEXTEQU	<ymm5>
y6	TEXTEQU	<ymm6>
y7	TEXTEQU	<ymm7>
y8	TEXTEQU	<ymm8>
y9	TEXTEQU	<ymm9>
y10	TEXTEQU	<ymm10>
y11	TEXTEQU	<ymm11>
y12	TEXTEQU	<ymm12>
y13	TEXTEQU	<ymm13>
y14	TEXTEQU	<ymm14>
y15	TEXTEQU	<ymm15>
	ENDIF

;; On later calls, previous R3,R4,I3,B4,A2,B2,sine1,sine2 will be in y0-7.  This R1,R3,I2,I4,I1,I3 will be in y8-13.
;; The remaining registers are free.

this	vsubpd	y14, y8, y9				;; R1 - R3 (new R3)		; 1-3
prev	vmulpd	y3, y3, y6				;; B4 = B4 * sine (final I4)	; 1-5
this	vmovapd	y6, [srcreg+iter*srcinc+d1]		;; R2

this	vaddpd	y8, y8, y9				;; R1 + R3 (new R1)		; 2-4
prev	vmulpd	y4, y4, y7				;; A2 = A2 * sine (final R2)	; 2-6
this	vmovapd	y9, [srcreg+iter*srcinc+d2+d1]		;; R4

this	vsubpd	y15, y10, y11				;; I2 - I4 (new I4)		; 3-5
prev	vmulpd	y5, y5, y7				;; B2 = B2 * sine (final I2)	; 3-7
this	vmovapd y7, [screg2+iter*scinc2+0]		;; cosine/sine
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2], y0		;; Save R3			; 3

this	vaddpd	y10, y10, y11				;; I2 + I4 (new I2)		; 4-6
this	vmovapd y11, [screg2+iter*scinc2+32]		;; cosine/sine
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+d1], y1	;; Save R4			; 4

this	vsubpd	y1, y12, y13				;; I1 - I3 (new I3)		; 5-7
this	vmovapd y0, [screg2+iter*scinc2+64+0]		;; Sine
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+32], y2	;; Save I3			; 5

this	vaddpd	y12, y12, y13				;; I1 + I3 (new I1)		; 6-8
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+d1+32], y3	;; Save I4			; 6
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

this	vsubpd	y3, y6, y9				;; R2 - R4 (new R4)		; 7-9
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1], y4		;; Save R2			; 7

this	vaddpd	y6, y6, y9				;; R2 + R4 (new R2)		; 8-10
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1+32], y5	;; Save I2			; 8

this	vsubpd	y5, y14, y15				;; R3 - I4 (final R3)		; 9-11
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	vaddpd	y14, y14, y15				;; R3 + I4 (final R4)		; 10-12

this	vaddpd	y15, y1, y3				;; I3 + R4 (final I3)		; 11-13

this	vsubpd	y1, y1, y3				;; I3 - R4 (final I4)		; 12-14
this	vmulpd	y3, y5, y7				;; A3 = R3 * cosine/sine	; 12-16
this	L1prefetchw srcreg+iter*srcinc+d2+L1pd, L1pt

this	vsubpd	y9, y8, y6				;; R1 - R2 (final R2)		; 13-15
this	vmulpd	y4, y14, y7				;; A4 = R4 * cosine/sine	; 13-17

this	vsubpd	y13, y12, y10				;; I1 - I2 (final I2)		; 14-16
this	vmulpd	y2, y15, y7				;; B3 = I3 * cosine/sine	; 14-18
this next yloop_unrolled_one

this	vaddpd	y8, y8, y6				;; R1 + R2 (final R1)		; 15-17
this	vmulpd	y7, y1, y7				;; B4 = I4 * cosine/sine	; 15-19
this	vbroadcastsd y6, Q [screg1+iter*scinc1]		;; Normalization multiplier for R1 & I1

this	vaddpd	y12, y12, y10				;; I1 + I2 (final I1)		; 16-18
this	vmulpd	y10, y9, y11				;; A2 = R2 * cosine/sine	; 16-20
this	L1prefetchw srcreg+iter*srcinc+d2+d1+L1pd, L1pt

this	vsubpd	y3, y3, y15				;; A3 = A3 - I3			; 17-19
this	vmulpd	y11, y13, y11				;; B2 = I2 * cosine/sine	; 17-21
next	vmovapd	y15, [srcreg+(iter+1)*srcinc]		;; R1

this	vaddpd	y4, y4, y1				;; A4 = A4 + I4			; 18-20
this	vmulpd	y8, y8, y6				;; Apply normalization multiplier to R1	; 18-22
next	vmovapd	y1, [srcreg+(iter+1)*srcinc+d2]		;; R3

this	vaddpd	y2, y2, y5				;; B3 = B3 + R3			; 19-21
this	vmulpd	y12, y12, y6				;; Apply normalization multiplier to I1 ; 19-23
this	vmovapd y5, [screg2+iter*scinc2+64+32]		;; Sine
next	vmovapd	y6, [srcreg+(iter+1)*srcinc+d1+32]	;; I2

this	vsubpd	y7, y7, y14				;; B4 = B4 - R4			; 20-22
this	vmulpd	y3, y3, y0				;; A3 = A3 * sine (final R3)	; 20-24
next	vmovapd	y14, [srcreg+(iter+1)*srcinc+d2+d1+32]	;; I4

this	vsubpd	y10, y10, y13				;; A2 = A2 - I2			; 21-23
this	vmulpd	y4, y4, y0				;; A4 = A4 * sine (final R4)	; 21-25
next	vmovapd	y13, [srcreg+(iter+1)*srcinc+32]	;; I1
this	vmovapd	[srcreg+iter*srcinc], y8		;; Save R1			; 23

this	vaddpd	y11, y11, y9				;; B2 = B2 + R2			; 22-24
this	vmulpd	y2, y2, y0				;; B3 = B3 * sine (final I3)	; 22-26
next	vmovapd	y9, [srcreg+(iter+1)*srcinc+d2+32]	;; I3
this	vmovapd	[srcreg+iter*srcinc+32], y12		;; Save I1			; 24

;; Shuffle register assignments so that next call has R3,R4,I3,B4,A2,B2,sine1,sine2 in y0-7 and next R1,R3,I2,I4,I1,I3 in y8-13.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y3
y3	TEXTEQU	y7
y7	TEXTEQU	y5
y5	TEXTEQU	y11
y11	TEXTEQU	y14
y14	TEXTEQU	y8
y8	TEXTEQU	y15
y15	TEXTEQU	y12
y12	TEXTEQU	y13
y13	TEXTEQU y9
y9	TEXTEQU	y1
y1	TEXTEQU	y4
y4	TEXTEQU	y10
y10	TEXTEQU	y6
y6	TEXTEQU	ytmp
	ENDM

yr4_b4cl_wpn_four_complex_djbunfft MACRO srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	yr4_4c_wpn_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 3,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 4,exec,exec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 5,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	bump	srcreg, 5*srcinc
	bump	screg1, 5*scinc1
	bump	screg2, 5*scinc2
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	yr4_4c_wpn_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 3,exec,exec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 4,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	screg1, 4*scinc1
	bump	screg2, 4*scinc2
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	yr4_4c_wpn_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 2,exec,exec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 3,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	screg1, 3*scinc1
	bump	screg2, 3*scinc2
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	yr4_4c_wpn_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 1,exec,exec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 2,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	screg1, 2*scinc1
	bump	screg2, 2*scinc2
	ELSE
	yr4_4c_wpn_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 0,noexec,exec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	yr4_4c_wpn_djbunfft_unroll 1,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd
	bump	srcreg, srcinc
	bump	screg1, scinc1
	bump	screg2, scinc2
	ENDIF
	ENDM

yr4_4c_wpn_djbunfft_unroll MACRO iter,prev,this,next,srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<ymm0>
y1	TEXTEQU	<ymm1>
y2	TEXTEQU	<ymm2>
y3	TEXTEQU	<ymm3>
y4	TEXTEQU	<ymm4>
y5	TEXTEQU	<ymm5>
y6	TEXTEQU	<ymm6>
y7	TEXTEQU	<ymm7>
y8	TEXTEQU	<ymm8>
y9	TEXTEQU	<ymm9>
y10	TEXTEQU	<ymm10>
y11	TEXTEQU	<ymm11>
y12	TEXTEQU	<ymm12>
y13	TEXTEQU	<ymm13>
y14	TEXTEQU	<ymm14>
y15	TEXTEQU	<ymm15>
	ENDIF

;; On later calls, previous R2,I3,I1 will be in y0-2.  This R4,R3,R2,I4,I3,I2,A4,A3,A2,B4,B3,c/s2 will be in y3-14.
;; The remaining register is free.

this	vsubpd	y9, y9, y6			;; A4 = A4 - I4 (new R4/sine)		; 1-3
this	vmulpd	y14, y8, y14			;; B2 = I2 * cosine/sine		; 1-5
this	vbroadcastsd y15, Q [screg1+iter*scinc1] ;; normalization_inverse for R1 & I1

this	vaddpd	y10, y10, y7			;; A3 = A3 + I3 (new R3/sine)		; 2-4
this	vmovapd	y6, [srcreg+iter*srcinc]	;; R1

this	vaddpd	y11, y11, y8			;; A2 = A2 + I2				; 3-5
this	vmulpd	y6, y6, y15			;; R1 * normalization_inverse		; 3-7
this	vmovapd	y7, [srcreg+iter*srcinc+32]	;; I1

this	vaddpd	y12, y12, y3			;; B4 = B4 + R4 (new I4/sine)		; 4-6
this	vmulpd	y7, y7, y15			;; I1 * normalization_inverse		; 4-8
this	vmovapd y8, [screg2+iter*scinc2+128+32]	;; Sine

this	vsubpd	y13, y13, y4			;; B3 = B3 - R3 (new I3/sine)		; 5-7
this	vmovapd y3, [screg2+iter*scinc2+128+0]	;; Sine
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1], y0	;; Save R2				; 1

this	vsubpd	y14, y14, y5			;; B2 = B2 - R2				; 6-8
this	vmulpd	y11, y11, y8			;; A2 = A2 * sine (new R2)		; 6-10
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+32], y1 ;; Save I3				; 2
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

this	vaddpd	y1, y9, y10			;; C3 = R4/sine + R3/sine (newer R3/sine) ; 7-9
prev	vmovapd	[srcreg+(iter-1)*srcinc+32], y2	;; Save I1				; 3

this	vsubpd	y9, y9, y10			;; D4 = R4/sine - R3/sine (newer I4/sine) ; 8-10
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	vsubpd	y10, y13, y12			;; C4 = I3/sine - I4/sine (newer R4/sine) ; 9-11
this	vmulpd	y14, y14, y8			;; B2 = B2 * sine (new I2)		; 9-13
next	vmovapd y15, [screg2+(iter+1)*scinc2+0]	;; cosine/sine

this	vaddpd	y13, y13, y12			;; D3 = I3/sine + I4/sine (newer I3/sine) ; 10-12
this	vmulpd	y1, y1, y3			;; C3 * sine (newer R3)			; 10-14
next	vmovapd	y4, [srcreg+(iter+1)*srcinc+d2+d1] ;; R4

this	vaddpd	y12, y6, y11			;; R1 + R2 (newer R1)			; 11-13
this	vmulpd	y9, y9, y3			;; D4 * sine (newer I4)			; 11-15
next	vmovapd	y0, [srcreg+(iter+1)*srcinc+d2]	;; R3

this	vsubpd	y6, y6, y11			;; R1 - R2 (newer R2)			; 12-14
this	vmulpd	y10, y10, y3			;; C4 * sine (newer R4)			; 12-16
next	vmovapd y5, [screg2+(iter+1)*scinc2+32]	;; cosine/sine

this	vsubpd	y11, y7, y14			;; I1 - I2 (newer I2)			; 13-15
this	vmulpd	y13, y13, y3			;; D3 * sine (newer I3)			; 13-17
next	vmovapd	y2, [srcreg+(iter+1)*srcinc+d1]	;; R2

this	vaddpd	y7, y7, y14			;; I1 + I2 (newer I1)			; 14-16
this	L1prefetchw srcreg+iter*srcinc+d2+L1pd, L1pt

this	vsubpd	y14, y12, y1			;; R1 - R3 (final R3)			; 15-17
next	vmovapd	y8, [srcreg+(iter+1)*srcinc+d2+d1+32] ;; I4

this	vaddpd	y12, y12, y1			;; R1 + R3 (final R1)			; 16-18
next	vmovapd	y3, [srcreg+(iter+1)*srcinc+d2+32] ;; I3

this	vmovapd	[srcreg+iter*srcinc+d2], y14	;; Save R3				; 18
this	vsubpd	y14, y11, y9			;; I2 - I4 (final I4)			; 17-19
next	vmovapd	y1, [srcreg+(iter+1)*srcinc+d1+32] ;; I2

this	vaddpd	y11, y11, y9			;; I2 + I4 (final I2)			; 18-20
next	vmulpd	y9, y4, y15			;; A4 = R4 * cosine/sine		; 18-22
this	L1prefetchw srcreg+iter*srcinc+d2+d1+L1pd, L1pt

this	vmovapd	[srcreg+iter*srcinc], y12	;; Save R1				; 19
this	vsubpd	y12, y6, y10			;; R2 - R4 (final R4)			; 19-21
this	vmovapd	[srcreg+iter*srcinc+d2+d1+32], y14 ;; Save I4				; 20
next	vmulpd	y14, y0, y15			;; A3 = R3 * cosine/sine		; 19-23

this	vaddpd	y6, y6, y10			;; R2 + R4 (final R2)			; 20-22
next	vmulpd	y10, y2, y5			;; A2 = R2 * cosine/sine		; 20-24

this	vmovapd	[srcreg+iter*srcinc+d1+32], y11	;; Save I2				; 21
this	vsubpd	y11, y7, y13			;; I1 - I3 (final I3)			; 21-23
this	vmovapd	[srcreg+iter*srcinc+d2+d1], y12	;; Save R4				; 22
next	vmulpd	y12, y8, y15			;; B4 = I4 * cosine/sine		; 21-25

this	vaddpd	y7, y7, y13			;; I1 + I3 (final I1)			; 22-24
next	vmulpd	y13, y3, y15			;; B3 = I3 * cosine/sine		; 22-26
this next yloop_unrolled_one

;; Shuffle register assignments so that next call has R2,I3,I1 in y0-2 and next R4,R3,R2,I4,I3,I2,A4,A3,A2,B4,B3,c/s2 in y3-14.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y6
y6	TEXTEQU y8
y8	TEXTEQU	y1
y1	TEXTEQU	y11
y11	TEXTEQU	y10
y10	TEXTEQU	y14
y14	TEXTEQU	y5
y5	TEXTEQU	y2
y2	TEXTEQU	y7
y7	TEXTEQU	y3
y3	TEXTEQU	y4
y4	TEXTEQU	ytmp
	ENDM

ENDIF


;;
;; ************************************* four-complex-with_square and variants ******************************************
;;
;; These macros are used in the last levels of pass 2 in two pass FFTs.
;;

;; Macros to do four four_complex_fft in the final levels of an FFT.

yr4_4cl_four_complex_fft_final_preload MACRO
	ENDM
yr4_4cl_four_complex_fft_final MACRO srcreg,srcinc,d1,d2
	yr4_4c_simple_fft_mem [srcreg],[srcreg+d1],[srcreg+d2],[srcreg+d2+d1],[srcreg+32],[srcreg+d1+32],[srcreg+d2+32],[srcreg+d2+d1+32],[srcreg]
;;	vmovapd	[srcreg], ymm0			;; Save R1
	vmovapd	[srcreg+32], ymm4		;; Save I1
	vmovapd	[srcreg+d1], ymm2		;; Save R2
	vmovapd	[srcreg+d1+32], ymm1		;; Save I2
	vmovapd	[srcreg+d2], ymm7		;; Save R3
	vmovapd	[srcreg+d2+32], ymm3		;; Save I3
	vmovapd	[srcreg+d2+d1], ymm0		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm6		;; Save I4
	bump	srcreg, srcinc
	ENDM

yr4_4cl_four_complex_with_square_preload MACRO
	ENDM
yr4_4cl_four_complex_with_square MACRO srcreg,srcinc,d1,d2,maxrpt,L1pt,L1pd

	vmovapd	ymm0, [srcreg]			;; R1
	vmovapd	ymm7, [srcreg+d2]		;; R3
	vaddpd	ymm2, ymm0, ymm7		;; R1 + R3 (new R1)
	vsubpd	ymm0, ymm0, ymm7		;; R1 - R3 (new R3)

	vmovapd	ymm1, [srcreg+d1]		;; R2
	vmovapd	ymm7, [srcreg+d2+d1]		;; R4
	vaddpd	ymm3, ymm1, ymm7		;; R2 + R4 (new R2)
	vsubpd	ymm1, ymm1, ymm7		;; R2 - R4 (new R4)

	vaddpd	ymm6, ymm2, ymm3		;; R1 + R2 (final R1)
	vsubpd	ymm2, ymm2, ymm3		;; R1 - R2 (final R2)

	vmovapd	ymm5, [srcreg+d1+32]		;; I2
	vmovapd	ymm7, [srcreg+d2+d1+32]		;; I4
	vsubpd	ymm3, ymm5, ymm7		;; I2 - I4 (new I4)
	vaddpd	ymm5, ymm5, ymm7		;; I2 + I4 (new I2)

	vmovapd	ymm4, [srcreg+32]		;; I1
	vmovapd	ymm7, [srcreg+d2+32]		;; I3

	vmovapd	[srcreg], ymm6			;; Save R1

	vsubpd	ymm6, ymm4, ymm7		;; I1 - I3 (new I3)
	vaddpd	ymm4, ymm4, ymm7		;; I1 + I3 (new I1)

	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	ymm7, ymm0, ymm3		;; R3 - I4 (final R3)
	vaddpd	ymm0, ymm0, ymm3		;; R3 + I4 (final R4)

	vaddpd	ymm3, ymm6, ymm1		;; I3 + R4 (final I3)
	vsubpd	ymm6, ymm6, ymm1		;; I3 - R4 (final I4)

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm1, ymm4, ymm5		;; I1 - I2 (final I2)
	vaddpd	ymm4, ymm4, ymm5		;; I1 + I2 (final I1)

	yp_complex_square ymm2, ymm1, ymm5	;; Square R2, I2
	yp_complex_square ymm7, ymm3, ymm5	;; Square R3, I3
	yp_complex_square ymm0, ymm6, ymm5	;; Square R4, I4
	vmovapd	[srcreg+d1+32], ymm1		;; Save I2
	vmovapd	ymm1, [srcreg]			;; Reload R1
	yp_complex_square ymm1, ymm4, ymm5	;; Square R1, I1

	vsubpd	ymm5, ymm1, ymm2		;; R1 - R2 (new R2)
	vaddpd	ymm1, ymm1, ymm2		;; R1 + R2 (new R1)

	L1prefetchw srcreg+d2+L1pd, L1pt

	vsubpd	ymm2, ymm0, ymm7		;; R4 - R3 (new I4)
	vaddpd	ymm0, ymm0, ymm7		;; R4 + R3 (new R3)

	vsubpd	ymm7, ymm3, ymm6		;; I3 - I4 (new R4)
	vaddpd	ymm3, ymm3, ymm6		;; I3 + I4 (new I3)

	vsubpd	ymm6, ymm1, ymm0		;; R1 - R3 (final R3)
	vaddpd	ymm1, ymm1, ymm0		;; R1 + R3 (final R1)

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm0, ymm5, ymm7		;; R2 - R4 (final R4)
	vaddpd	ymm5, ymm5, ymm7		;; R2 + R4 (final R2)

	vmovapd	ymm7, [srcreg+d1+32]		;; Reload I2

	vmovapd	[srcreg], ymm1			;; Save R1

	vsubpd	ymm1, ymm4, ymm7		;; I1 - I2 (new I2)
	vaddpd	ymm4, ymm4, ymm7		;; I1 + I2 (new I1)

	vsubpd	ymm7, ymm1, ymm2		;; I2 - I4 (final I4)
	vaddpd	ymm1, ymm1, ymm2		;; I2 + I4 (final I2)

	vsubpd	ymm2, ymm4, ymm3		;; I1 - I3 (final I3)
	vaddpd	ymm4, ymm4, ymm3		;; I1 + I3 (final I1)

	vmovapd	[srcreg+32], ymm4		;; Save I1
	vmovapd	[srcreg+d1], ymm5		;; Save R2
	vmovapd	[srcreg+d1+32], ymm1		;; Save I2
	vmovapd	[srcreg+d2], ymm6		;; Save R3
	vmovapd	[srcreg+d2+32], ymm2		;; Save I3
	vmovapd	[srcreg+d2+d1], ymm0		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm7		;; Save I4
	bump	srcreg, srcinc
	ENDM

yr4_4cl_four_complex_with_mult_preload MACRO
	ENDM
yr4_4cl_four_complex_with_mult MACRO srcreg,srcinc,d1,d2
	yr4_4c_simple_fft_mem [srcreg],[srcreg+d1],[srcreg+d2],[srcreg+d2+d1],[srcreg+32],[srcreg+d1+32],[srcreg+d2+32],[srcreg+d2+d1+32],[srcreg]
	vmovapd	[srcreg+32], ymm4		;; Save I1
	yp_complex_mult ymm2, ymm1, [srcreg+d1][rbp], [srcreg+d1+32][rbp], ymm4, ymm5 ;; Mult R2, I2
	yp_complex_mult ymm7, ymm3, [srcreg+d2][rbp], [srcreg+d2+32][rbp], ymm4, ymm5 ;; Mult R3, I3
	yp_complex_mult ymm0, ymm6, [srcreg+d2+d1][rbp], [srcreg+d2+d1+32][rbp], ymm4, ymm5 ;; Mult R4, I4
	vmovapd	[srcreg+d1], ymm2		;; Save R2
	vmovapd	[srcreg+d1+32], ymm1		;; Save I2
	vmovapd	[srcreg+d2], ymm7		;; Save R3
	vmovapd	[srcreg+d2+32], ymm3		;; Save I3
	vmovapd	[srcreg+d2+d1], ymm0		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm6		;; Save I4
	vmovapd	ymm2, [srcreg]			;; Reload R1
	vmovapd	ymm4, [srcreg+32]		;; Reload I1
	yp_complex_mult ymm2, ymm4, [srcreg][rbp], [srcreg+32][rbp], ymm3, ymm5 ;; Mult R1, I1
	vmovapd	[srcreg], ymm2			;; Save R1
	vmovapd	[srcreg+32], ymm4		;; Save I1
	yr4_4c_simple_unfft_mem [srcreg],[srcreg+32],[srcreg+d1],[srcreg+d1+32],[srcreg+d2],[srcreg+d2+32],[srcreg+d2+d1],[srcreg+d2+d1+32],[srcreg]
;;	vmovapd	[srcreg], ymm0			;; Save R1
	vmovapd	[srcreg+32], ymm4		;; Save I1
	vmovapd	[srcreg+d1], ymm5		;; Save R2
	vmovapd	[srcreg+d1+32], ymm7		;; Save I2
	vmovapd	[srcreg+d2], ymm2		;; Save R3
	vmovapd	[srcreg+d2+32], ymm3		;; Save I3
	vmovapd	[srcreg+d2+d1], ymm1		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm6		;; Save I4
	bump	srcreg, srcinc
	ENDM

yr4_4cl_four_complex_with_mulf_preload MACRO
	ENDM
yr4_4cl_four_complex_with_mulf MACRO srcreg,srcinc,d1,d2
	vmovapd	ymm2, [srcreg+d1][rbx]		;; R2
	vmovapd	ymm1, [srcreg+d1+32][rbx]	;; I2
	yp_complex_mult ymm2, ymm1, [srcreg+d1][rbp], [srcreg+d1+32][rbp], ymm4, ymm5 ;; Mult R2, I2
	vmovapd	ymm7, [srcreg+d2][rbx]		;; R3
	vmovapd	ymm3, [srcreg+d2+32][rbx]	;; I3
	yp_complex_mult ymm7, ymm3, [srcreg+d2][rbp], [srcreg+d2+32][rbp], ymm4, ymm5 ;; Mult R3, I3
	vmovapd	ymm0, [srcreg+d2+d1][rbx]	;; R4
	vmovapd	ymm6, [srcreg+d2+d1+32][rbx]	;; I4
	yp_complex_mult ymm0, ymm6, [srcreg+d2+d1][rbp], [srcreg+d2+d1+32][rbp], ymm4, ymm5 ;; Mult R4, I4
	vmovapd	[srcreg+d1], ymm2		;; Save R2
	vmovapd	[srcreg+d1+32], ymm1		;; Save I2
	vmovapd	[srcreg+d2], ymm7		;; Save R3
	vmovapd	[srcreg+d2+32], ymm3		;; Save I3
	vmovapd	[srcreg+d2+d1], ymm0		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm6		;; Save I4
	vmovapd	ymm2, [srcreg][rbx]		;; R1
	vmovapd	ymm4, [srcreg+32][rbx]		;; I1
	yp_complex_mult ymm2, ymm4, [srcreg][rbp], [srcreg+32][rbp], ymm3, ymm5 ;; Mult R1, I1
	vmovapd	[srcreg], ymm2			;; Save R1
	vmovapd	[srcreg+32], ymm4		;; Save I1
	yr4_4c_simple_unfft_mem [srcreg],[srcreg+32],[srcreg+d1],[srcreg+d1+32],[srcreg+d2],[srcreg+d2+32],[srcreg+d2+d1],[srcreg+d2+d1+32],[srcreg]
;;	vmovapd	[srcreg], ymm0			;; Save R1
	vmovapd	[srcreg+32], ymm4		;; Save I1
	vmovapd	[srcreg+d1], ymm5		;; Save R2
	vmovapd	[srcreg+d1+32], ymm7		;; Save I2
	vmovapd	[srcreg+d2], ymm2		;; Save R3
	vmovapd	[srcreg+d2+32], ymm3		;; Save I3
	vmovapd	[srcreg+d2+d1], ymm1		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm6		;; Save I4
	bump	srcreg, srcinc
	ENDM

;; Does a four-complex FFT with no sin/cos data -- appropriate only in the last FFT levels.
yr4_4c_simple_fft_mem MACRO mem1,mem2,mem3,mem4,mem5,mem6,mem7,mem8,dst1
	vmovapd	ymm0, mem1		;; R1
	vmovapd	ymm7, mem3		;; R3
	vaddpd	ymm2, ymm0, ymm7	;; R1 + R3 (new R1)
	vsubpd	ymm0, ymm0, ymm7	;; R1 - R3 (new R3)

	vmovapd	ymm1, mem2		;; R2
	vmovapd	ymm7, mem4		;; R4
	vaddpd	ymm3, ymm1, ymm7	;; R2 + R4 (new R2)
	vsubpd	ymm1, ymm1, ymm7	;; R2 - R4 (new R4)

	vaddpd	ymm6, ymm2, ymm3	;; R1 + R2 (final R1)
	vsubpd	ymm2, ymm2, ymm3	;; R1 - R2 (final R2)

	vmovapd	ymm5, mem6		;; I2
	vmovapd	ymm7, mem8		;; I4
	vsubpd	ymm3, ymm5, ymm7	;; I2 - I4 (new I4)
	vaddpd	ymm5, ymm5, ymm7	;; I2 + I4 (new I2)

	vmovapd	ymm4, mem5		;; I1
	vmovapd	ymm7, mem7		;; I3

	vmovapd	dst1, ymm6		;; Save R1

	vsubpd	ymm6, ymm4, ymm7	;; I1 - I3 (new I3)
	vaddpd	ymm4, ymm4, ymm7	;; I1 + I3 (new I1)

	vsubpd	ymm7, ymm0, ymm3	;; R3 - I4 (final R3)
	vaddpd	ymm0, ymm0, ymm3	;; R3 + I4 (final R4)

	vaddpd	ymm3, ymm6, ymm1	;; I3 + R4 (final I3)
	vsubpd	ymm6, ymm6, ymm1	;; I3 - R4 (final I4)

	vsubpd	ymm1, ymm4, ymm5	;; I1 - I2 (final I2)
	vaddpd	ymm4, ymm4, ymm5	;; I1 + I2 (final I1)
	ENDM

;; Does a four-complex inverse FFT with no sin/cos data -- appropriate only in the last FFT levels.
yr4_4c_simple_unfft_mem MACRO mem1,mem2,mem3,mem4,mem5,mem6,mem7,mem8,dst1
	vmovapd	ymm7, mem1		;; R1
	vmovapd	ymm0, mem3		;; R2
	vsubpd	ymm5, ymm7, ymm0	;; R1 - R2 (new R2)
	vaddpd	ymm7, ymm7, ymm0	;; R1 + R2 (new R1)

	vmovapd	ymm1, mem7		;; R4
	vmovapd	ymm0, mem5		;; R3
	vsubpd	ymm3, ymm1, ymm0	;; R4 - R3 (new I4)
	vaddpd	ymm1, ymm1, ymm0	;; R4 + R3 (new R3)

	vmovapd	ymm0, mem6		;; I3
	vmovapd	ymm2, mem8		;; I4
	vsubpd	ymm4, ymm0, ymm2	;; I3 - I4 (new R4)
	vaddpd	ymm0, ymm0, ymm2	;; I3 + I4 (new I3)

	vsubpd	ymm2, ymm7, ymm1	;; R1 - R3 (final R3)
	vaddpd	ymm7, ymm7, ymm1	;; R1 + R3 (final R1)

	vsubpd	ymm1, ymm5, ymm4	;; R2 - R4 (final R4)
	vaddpd	ymm5, ymm5, ymm4	;; R2 + R4 (final R2)

	vmovapd	dst1, ymm7

	vmovapd	ymm4, mem2		;; I1
	vmovapd	ymm6, mem4		;; I2
	vsubpd	ymm7, ymm4, ymm6	;; I1 - I2 (new I2)
	vaddpd	ymm4, ymm4, ymm6	;; I1 + I2 (new I1)

	vsubpd	ymm6, ymm7, ymm3	;; I2 - I4 (final I4)
	vaddpd	ymm7, ymm7, ymm3	;; I2 + I4 (final I2)

	vsubpd	ymm3, ymm4, ymm0	;; I1 - I3 (final I3)
	vaddpd	ymm4, ymm4, ymm0	;; I1 + I3 (final I1)
	ENDM

;; 64-bit version

IFDEF X86_64

yr4_4cl_four_complex_with_square_preload MACRO
	vmovapd	ymm15, YMM_TWO
	ENDM

yr4_4cl_four_complex_with_square MACRO srcreg,srcinc,d1,d2,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	yr4_4c_with_square_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 3,exec,exec,exec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 4,exec,exec,noexec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 5,exec,noexec,noexec,srcreg,srcinc,d1,d2,L1pt,L1pd
	bump	srcreg, 5*srcinc
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	yr4_4c_with_square_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 3,exec,exec,noexec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 4,exec,noexec,noexec,srcreg,srcinc,d1,d2,L1pt,L1pd
	bump	srcreg, 4*srcinc
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	yr4_4c_with_square_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 2,exec,exec,noexec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 3,exec,noexec,noexec,srcreg,srcinc,d1,d2,L1pt,L1pd
	bump	srcreg, 3*srcinc
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	yr4_4c_with_square_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 1,exec,exec,noexec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 2,exec,noexec,noexec,srcreg,srcinc,d1,d2,L1pt,L1pd
	bump	srcreg, 2*srcinc
	ELSE
	yr4_4c_with_square_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 0,noexec,exec,noexec,srcreg,srcinc,d1,d2,L1pt,L1pd
	yr4_4c_with_square_unroll 1,exec,noexec,noexec,srcreg,srcinc,d1,d2,L1pt,L1pd
	bump	srcreg, srcinc
	ENDIF
	ENDM

yr4_4c_with_square_unroll MACRO iter,prev,this,next,srcreg,srcinc,d1,d2,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<ymm0>
y1	TEXTEQU	<ymm1>
y2	TEXTEQU	<ymm2>
y3	TEXTEQU	<ymm3>
y4	TEXTEQU	<ymm4>
y5	TEXTEQU	<ymm5>
y6	TEXTEQU	<ymm6>
y7	TEXTEQU	<ymm7>
y8	TEXTEQU	<ymm8>
y9	TEXTEQU	<ymm9>
y10	TEXTEQU	<ymm10>
y11	TEXTEQU	<ymm11>
y12	TEXTEQU	<ymm12>
y13	TEXTEQU	<ymm13>
y14	TEXTEQU	<ymm14>
	ENDIF

;; On later calls, previous R2,I3,I1 will be in y0-2.  This R1,R2,R3,R4,new R1,I1,I3 will be in y3-9.
;; The remaining registers are free.  ymm15 is preloaded with the constant 2.

this	vaddpd	y14, y4, y6		;; R2 + R4 (new R2)			; 1-3
this	vmovapd	y10, [srcreg+iter*srcinc+d1+32]	;; I2
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1], y0 ;; Save R2			; 1

this	vaddpd	y13, y8, y9		;; I1 + I3 (new I1)			; 2-4
this	vmovapd	y11, [srcreg+iter*srcinc+d2+d1+32] ;; I4
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+32], y1 ;; Save I3			; 2

this	vaddpd	y1, y10, y11		;; I2 + I4 (new I2)			; 3-5
prev	vmovapd	[srcreg+(iter-1)*srcinc+32], y2 ;; Save I1			; 3

this	vaddpd	y2, y7, y14		;; R1 + R2 (final R1)			; 4-6
next	vmovapd	y12, [srcreg+(iter+1)*srcinc] ;; R1

this	vsubpd	y7, y7, y14		;; R1 - R2 (final R2)			; 5-7
next	vmovapd	y0, [srcreg+(iter+1)*srcinc+d2] ;; R3

this	vaddpd	y14, y13, y1		;; I1 + I2 (final I1)			; 6-8

this	vsubpd	y13, y13, y1		;; I1 - I2 (final I2)			; 7-9
this	vmulpd	y1, y2, y2		;; R1 * R1				; 7-11

this	vsubpd	y3, y3, y5		;; R1 - R3 (new R3)			; 8-10
this	vmulpd	y5, y7, y7		;; R2 * R2				; 8-12
this next yloop_unrolled_one

this	vsubpd	y10, y10, y11		;; I2 - I4 (new I4)			; 9-11
this	vmulpd	y11, y14, y14		;; I1 * I1				; 9-13

this	vsubpd	y4, y4, y6		;; R2 - R4 (new R4)			; 10-12
this	vmulpd	y6, y13, y13		;; I2 * I2				; 10-14

this	vsubpd	y8, y8, y9		;; I1 - I3 (new I3)			; 11-13
this	vmulpd	y14, y14, y2		;; I1 * R1 (I1/2)			; 11-15

this	vsubpd	y2, y3, y10		;; R3 - I4 (final R3)			; 12-14
this	vmulpd	y13, y13, y7		;; I2 * R2 (I2/2)			; 12-16
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

this	vaddpd	y3, y3, y10		;; R3 + I4 (final R4)			; 13-15

this	vaddpd	y10, y8, y4		;; I3 + R4 (final I3)			; 14-16

this	vsubpd	y8, y8, y4		;; I3 - R4 (final I4)			; 15-17
this	vmulpd	y4, y2, y2		;; R3 * R3				; 15-19

this	vsubpd	y1, y1, y11		;; R1^2 - I1^2 (R1)			; 16-18
this	vmulpd	y11, y3, y3		;; R4 * R4				; 16-20
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	vsubpd	y5, y5, y6		;; R2^2 - I2^2 (R2)			; 17-19
this	vmulpd	y6, y10, y10		;; I3 * I3				; 17-21

this	vsubpd	y7, y14, y13		;; I1/2 - I2/2 (new I2/2)		; 18-20
this	vmulpd	y9, y8, y8		;; I4 * I4				; 18-22

this	vaddpd	y14, y14, y13		;; I1/2 + I2/2 (new I1/2)		; 19-21
this	vmulpd	y10, y10, y2		;; I3 * R3 (I3/2)			; 19-23
next	vmovapd	y13, [srcreg+(iter+1)*srcinc+d1] ;; R2

this	vsubpd	y2, y1, y5		;; R1 - R2 (new R2)			; 20-22
this	vmulpd	y8, y8, y3		;; I4 * R4 (I4/2)			; 20-24
next	vmovapd	y3, [srcreg+(iter+1)*srcinc+d2+d1] ;; R4

this	vaddpd	y1, y1, y5		;; R1 + R2 (new R1)			; 21-23
this	vmulpd	y14, y14, ymm15		;; I1/2 * 2				; 21-25

this	vsubpd	y4, y4, y6		;; R3^2 - I3^2 (R3)			; 22-24
this	vmulpd	y7, y7, ymm15		;; I2/2 * 2				; 22-26

this	vsubpd	y11, y11, y9		;; R4^2 - I4^2 (R4)			; 23-25
this	L1prefetchw srcreg+iter*srcinc+d2+L1pd, L1pt

next	vaddpd	y9, y12, y0		;; R1 + R3 (new R1)			; 24-26

this	vsubpd	y6, y10, y8		;; I3/2 - I4/2 (new R4/2)		; 25-27

this	vaddpd	y5, y11, y4		;; R4 + R3 (new R3)			; 26-28

this	vaddpd	y10, y10, y8		;; I3/2 + I4/2 (new I3/2)		; 27-29
next	vmovapd	y8, [srcreg+(iter+1)*srcinc+32] ;; I1

this	vsubpd	y11, y11, y4		;; R4 - R3 (new I4)			; 28-30
this	vmulpd	y6, y6, ymm15		;; R4/2 * 2				; 28-32

this	vsubpd	y4, y1, y5		;; R1 - R3 (final R3)			; 29-31
this	L1prefetchw srcreg+iter*srcinc+d2+d1+L1pd, L1pt

this	vaddpd	y1, y1, y5		;; R1 + R3 (final R1)			; 30-32
this	vmulpd	y10, y10, ymm15		;; I3/2 * 2				; 30-34

this	vsubpd	y5, y7, y11		;; I2 - I4 (final I4)			; 31-33

this	vaddpd	y7, y7, y11		;; I2 + I4 (final I2)			; 32-34
next	vmovapd	y11, [srcreg+(iter+1)*srcinc+d2+32] ;; I3
this	vmovapd	[srcreg+iter*srcinc+d2], y4 ;; Save R3				; 32

this	vsubpd	y4, y2, y6		;; R2 - R4 (final R4)			; 33-35
this	vmovapd	[srcreg+iter*srcinc], y1 ;; Save R1				; 33

this	vaddpd	y2, y2, y6		;; R2 + R4 (final R2)			; 34-36
this	vmovapd	[srcreg+iter*srcinc+d2+d1+32], y5 ;; Save I4			; 34

this	vsubpd	y5, y14, y10		;; I1 - I3 (final I3)			; 35-37
this	vmovapd	[srcreg+iter*srcinc+d1+32], y7 ;; Save I2			; 35

this	vaddpd	y14, y14, y10		;; I1 + I3 (final I1)			; 36-38
this	vmovapd	[srcreg+iter*srcinc+d2+d1], y4 ;; Save R4			; 36

;; Shuffle register assignments so that next call has R2,I3,I1 in y0-2 and next R1,R2,R3,R4,new R1,I1,I3 in y3-9.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y2
y2	TEXTEQU	y14
y14	TEXTEQU	y4
y4	TEXTEQU	y13
y13	TEXTEQU	y6
y6	TEXTEQU	y3
y3	TEXTEQU	y12
y12	TEXTEQU	y7
y7	TEXTEQU	y9
y9	TEXTEQU	y11
y11	TEXTEQU	y1
y1	TEXTEQU y5
y5	TEXTEQU	ytmp
	ENDM

ENDIF


;;
;; ************************************* four-complex-fft4 variants ******************************************
;;
;; These macros are used in the last levels of pass 1.  Four sin/cos multipliers are needed to
;; finish off the partial sin/cos multiplies that were done in the first levels of pass 1.
;; FFTs of type r4delay and r4dwpn do this to reduce memory usage at the cost of some extra
;; complex multiplies.


;;
;; In the split premultiplier case, we apply part of the roots of -1 at the
;; end of the first pass.  Thus we have 4 sin/cos/premultipliers instead
;; of the usual 3.
;;

;; Used in last levels of pass 1 (split premultiplier, delay, and dwpn cases).  Swizzling.
yr4_sg4cl_four_complex_fft4 MACRO srcreg,srcinc,d1,d2,dstreg,dstinc,e1,e2,screg,scinc,maxrpt,L1pt,L1pd

	;; On input the 64-byte cache lines hold these data values:
	;;	0	+1K	+1K	+1K	4K	+1K	+1K	+1K
	;;	1	...
	;;	2	...
	;;	3	...

	;; These are swizzled to:
	;;	0	+1	+1	+1	4K	+1	+1	+1
	;;	1K	...
	;;	2K	...
	;;	3K	...

	;; Swizzle using 8 shufpds and 8 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm1, [srcreg]			;; R1
	vmovapd	ymm7, [srcreg+d1]		;; R2
	vshufpd	ymm0, ymm1, ymm7, 15		;; Shuffle R1 and R2 to create R1/R2 hi			; 1
	vshufpd	ymm1, ymm1, ymm7, 0		;; Shuffle R1 and R2 to create R1/R2 low		; 2

	vmovapd	ymm2, [srcreg+d2]		;; R3
	vmovapd	ymm7, [srcreg+d2+d1]		;; R4
	vshufpd	ymm3, ymm2, ymm7, 15		;; Shuffle R3 and R4 to create R3/R4 hi			; 3
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle R3 and R4 to create R3/R4 low		; 4

	vperm2f128 ymm4, ymm0, ymm3, 32		;; Shuffle R1/R2 hi and R3/R4 hi (new R2)		; 5-6
	vperm2f128 ymm0, ymm0, ymm3, 49		;; Shuffle R1/R2 hi and R3/R4 hi (new R4)		; 6-7

	vperm2f128 ymm3, ymm1, ymm2, 32		;; Shuffle R1/R2 low and R3/R4 low (new R1)		; 7-8
	vperm2f128 ymm1, ymm1, ymm2, 49		;; Shuffle R1/R2 low and R3/R4 low (new R3)		; 8-9

	vaddpd	ymm5, ymm4, ymm0		;; R2 + R4 (newer R2)
	vsubpd	ymm4, ymm4, ymm0		;; R2 - R4 (newer R4)

	L1prefetch srcreg+L1pd, L1pt

	vaddpd	ymm0, ymm3, ymm1		;; R1 + R3 (newer R1)
	vsubpd	ymm3, ymm3, ymm1		;; R1 - R3 (newer R3)

	vsubpd	ymm1, ymm0, ymm5		;; R1 - R2 (final R2)
	vaddpd	ymm0, ymm0, ymm5		;; R1 + R2 (final R1)

	vmovapd	ymm6, [srcreg+32]		;; I1
	vmovapd	ymm2, [srcreg+d1+32]		;; I2
	vshufpd	ymm5, ymm6, ymm2, 15		;; Shuffle I1 and I2 to create I1/I2 hi
	vshufpd	ymm6, ymm6, ymm2, 0		;; Shuffle I1 and I2 to create I1/I2 low

	vmovapd	[dstreg], ymm0			;; Temporarily save final R1

	vmovapd	ymm2, [srcreg+d2+32]		;; I3
	vmovapd	ymm7, [srcreg+d2+d1+32]		;; I4
	vshufpd	ymm0, ymm2, ymm7, 15		;; Shuffle I3 and I4 to create I3/I4 hi
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle I3 and I4 to create I3/I4 low

	vperm2f128 ymm7, ymm5, ymm0, 32		;; Shuffle I1/I2 hi and I3/I4 hi (new I2)
	vperm2f128 ymm5, ymm5, ymm0, 49		;; Shuffle I1/I2 hi and I3/I4 hi (new I4)

	vperm2f128 ymm0, ymm6, ymm2, 32		;; Shuffle I1/I2 low and I3/I4 low (new I1)
	vperm2f128 ymm6, ymm6, ymm2, 49		;; Shuffle I1/I2 low and I3/I4 low (new I3)

	vaddpd	ymm2, ymm7, ymm5		;; I2 + I4 (newer I2)
	vsubpd	ymm7, ymm7, ymm5		;; I2 - I4 (newer I4)

	L1prefetch srcreg+d1+L1pd, L1pt

	vaddpd	ymm5, ymm0, ymm6		;; I1 + I3 (newer I1)
	vsubpd	ymm0, ymm0, ymm6		;; I1 - I3 (newer I3)

	vsubpd	ymm6, ymm3, ymm7		;; R3 - I4 (final R3)
	vaddpd	ymm3, ymm3, ymm7		;; R3 + I4 (final R4)

	vsubpd	ymm7, ymm0, ymm4		;; I3 - R4 (final I4)
	vaddpd	ymm0, ymm0, ymm4		;; I3 + R4 (final I3)

	L1prefetch srcreg+d2+L1pd, L1pt

	vsubpd	ymm4, ymm5, ymm2		;; I1 - I2 (final I2)
	vaddpd	ymm5, ymm5, ymm2		;; I1 + I2 (final I1)

	vmovapd	[dstreg+32], ymm5		;; Temporarily save final I1

	vmovapd	ymm2, [screg+64+32]		;; cosine/sine
	vmulpd	ymm5, ymm6, ymm2		;; A3 = R3 * cosine/sine
	vsubpd	ymm5, ymm5, ymm0		;; A3 = A3 - I3
	vmulpd	ymm0, ymm0, ymm2		;; B3 = I3 * cosine/sine
	vaddpd	ymm0, ymm0, ymm6		;; B3 = B3 + R3

	L1prefetch srcreg+d2+d1+L1pd, L1pt

	vmovapd	ymm2, [screg+128+32]		;; cosine/sine
	vmulpd	ymm6, ymm1, ymm2		;; A2 = R2 * cosine/sine
	vsubpd	ymm6, ymm6, ymm4		;; A2 = A2 - I2
	vmulpd	ymm4, ymm4, ymm2		;; B2 = I2 * cosine/sine
	vaddpd	ymm4, ymm4, ymm1		;; B2 = B2 + R2

	vmovapd	ymm2, [screg+192+32]		;; cosine/sine
	vmulpd	ymm1, ymm3, ymm2		;; A4 = R4 * cosine/sine
	vsubpd	ymm1, ymm1, ymm7		;; A4 = A4 - I4
	vmulpd	ymm7, ymm7, ymm2		;; B4 = I4 * cosine/sine
	vaddpd	ymm7, ymm7, ymm3		;; B4 = B4 + R4

	vmovapd	ymm2, [screg+64]
	vmulpd	ymm5, ymm5, ymm2		;; A3 = A3 * sine (final R3)
	vmulpd	ymm0, ymm0, ymm2		;; B3 = B3 * sine (final I3)
	vmovapd	ymm2, [screg+128]
	vmulpd	ymm6, ymm6, ymm2		;; A2 = A2 * sine (final R2)
	vmulpd	ymm4, ymm4, ymm2		;; B2 = B2 * sine (final I2)

	vmovapd	[dstreg+e2], ymm5		;; Save R3
	vmovapd	[dstreg+e2+32], ymm0		;; Save I3

	vmovapd	ymm2, [screg+0+32]		;; cosine/sine
	vmovapd	ymm3, [dstreg]			;; Reload R1
	vmulpd	ymm5, ymm3, ymm2		;; A1 = R1 * cosine/sine
	vmovapd	ymm0, [dstreg+32]		;; Restore I1
	vsubpd	ymm5, ymm5, ymm0		;; A1 = A1 - I1
	vmulpd	ymm0, ymm0, ymm2		;; B1 = I1 * cosine/sine
	vaddpd	ymm0, ymm0, ymm3		;; B1 = B1 + R1

	vmovapd	ymm3, [screg+192]		;; sine
	vmulpd	ymm1, ymm1, ymm3		;; A4 = A4 * sine (final R4)
	vmulpd	ymm7, ymm7, ymm3		;; B4 = B4 * sine (final I4)
	vmovapd	ymm3, [screg+0]			;; sine
	vmulpd	ymm5, ymm5, ymm3		;; A1 = A1 * sine (final R1)
	vmulpd	ymm0, ymm0, ymm3		;; B1 = B1 * sine (final I1)

	vmovapd	[dstreg], ymm5			;; Save R1
	vmovapd	[dstreg+32], ymm0		;; Save I1
	vmovapd	[dstreg+e1], ymm6		;; Save R2
	vmovapd	[dstreg+e1+32], ymm4		;; Save I2
	vmovapd	[dstreg+e2+e1], ymm1		;; Save R4
	vmovapd	[dstreg+e2+e1+32], ymm7		;; Save I4

	bump	srcreg, srcinc
	bump	dstreg, dstinc
	bump	screg, scinc
	ENDM


;; 64-bit version

IFDEF X86_64

yr4_sg4cl_four_complex_fft4 MACRO srcreg,srcinc,d1,d2,dstreg,dstinc,e1,e2,screg,scinc,maxrpt,L1pt,L1pd

	;; On input the 64-byte cache lines hold these data values:
	;;	0	+1K	+1K	+1K	4K	+1K	+1K	+1K
	;;	1	...
	;;	2	...
	;;	3	...

	;; These are swizzled to:
	;;	0	+1	+1	+1	4K	+1	+1	+1
	;;	1K	...
	;;	2K	...
	;;	3K	...

	;; Swizzle using 8 shufpds and 8 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm1, [srcreg]			;; R1
	vmovapd	ymm7, [srcreg+d1]		;; R2
	vshufpd	ymm0, ymm1, ymm7, 15		;; Shuffle R1 and R2 to create R1/R2 hi			; 1
	vshufpd	ymm1, ymm1, ymm7, 0		;; Shuffle R1 and R2 to create R1/R2 low		; 2

	vmovapd	ymm2, [srcreg+d2]		;; R3
	vmovapd	ymm7, [srcreg+d2+d1]		;; R4
	vshufpd	ymm3, ymm2, ymm7, 15		;; Shuffle R3 and R4 to create R3/R4 hi			; 3
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle R3 and R4 to create R3/R4 low		; 4

	vperm2f128 ymm4, ymm0, ymm3, 32		;; Shuffle R1/R2 hi and R3/R4 hi (new R2)		; 5-6
	vperm2f128 ymm0, ymm0, ymm3, 49		;; Shuffle R1/R2 hi and R3/R4 hi (new R4)		; 6-7

	vperm2f128 ymm3, ymm1, ymm2, 32		;; Shuffle R1/R2 low and R3/R4 low (new R1)		; 7-8
	vperm2f128 ymm1, ymm1, ymm2, 49		;; Shuffle R1/R2 low and R3/R4 low (new R3)		; 8-9

	vaddpd	ymm5, ymm4, ymm0		;; R2 + R4 (newer R2)
	vsubpd	ymm4, ymm4, ymm0		;; R2 - R4 (newer R4)

	L1prefetch srcreg+L1pd, L1pt

	vaddpd	ymm0, ymm3, ymm1		;; R1 + R3 (newer R1)
	vsubpd	ymm3, ymm3, ymm1		;; R1 - R3 (newer R3)

	vsubpd	ymm1, ymm0, ymm5		;; R1 - R2 (final R2)
	vaddpd	ymm8, ymm0, ymm5		;; R1 + R2 (final R1)

	vmovapd	ymm6, [srcreg+32]		;; I1
	vmovapd	ymm2, [srcreg+d1+32]		;; I2
	vshufpd	ymm5, ymm6, ymm2, 15		;; Shuffle I1 and I2 to create I1/I2 hi
	vshufpd	ymm6, ymm6, ymm2, 0		;; Shuffle I1 and I2 to create I1/I2 low

	vmovapd	ymm2, [srcreg+d2+32]		;; I3
	vmovapd	ymm7, [srcreg+d2+d1+32]		;; I4
	vshufpd	ymm0, ymm2, ymm7, 15		;; Shuffle I3 and I4 to create I3/I4 hi
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle I3 and I4 to create I3/I4 low

	vperm2f128 ymm7, ymm5, ymm0, 32		;; Shuffle I1/I2 hi and I3/I4 hi (new I2)
	vperm2f128 ymm5, ymm5, ymm0, 49		;; Shuffle I1/I2 hi and I3/I4 hi (new I4)

	vperm2f128 ymm0, ymm6, ymm2, 32		;; Shuffle I1/I2 low and I3/I4 low (new I1)
	vperm2f128 ymm6, ymm6, ymm2, 49		;; Shuffle I1/I2 low and I3/I4 low (new I3)

	vaddpd	ymm2, ymm7, ymm5		;; I2 + I4 (newer I2)
	vsubpd	ymm7, ymm7, ymm5		;; I2 - I4 (newer I4)

	L1prefetch srcreg+d1+L1pd, L1pt

	vaddpd	ymm5, ymm0, ymm6		;; I1 + I3 (newer I1)
	vsubpd	ymm0, ymm0, ymm6		;; I1 - I3 (newer I3)

	vsubpd	ymm6, ymm3, ymm7		;; R3 - I4 (final R3)
	vaddpd	ymm3, ymm3, ymm7		;; R3 + I4 (final R4)

	vsubpd	ymm7, ymm0, ymm4		;; I3 - R4 (final I4)
	vaddpd	ymm0, ymm0, ymm4		;; I3 + R4 (final I3)

	L1prefetch srcreg+d2+L1pd, L1pt

	vsubpd	ymm4, ymm5, ymm2		;; I1 - I2 (final I2)
	vaddpd	ymm9, ymm5, ymm2		;; I1 + I2 (final I1)

	vmovapd	ymm2, [screg+64+32]		;; cosine/sine
	vmulpd	ymm5, ymm6, ymm2		;; A3 = R3 * cosine/sine
	vsubpd	ymm5, ymm5, ymm0		;; A3 = A3 - I3
	vmulpd	ymm0, ymm0, ymm2		;; B3 = I3 * cosine/sine
	vaddpd	ymm0, ymm0, ymm6		;; B3 = B3 + R3

	L1prefetch srcreg+d2+d1+L1pd, L1pt

	vmovapd	ymm2, [screg+128+32]		;; cosine/sine
	vmulpd	ymm6, ymm1, ymm2		;; A2 = R2 * cosine/sine
	vsubpd	ymm6, ymm6, ymm4		;; A2 = A2 - I2
	vmulpd	ymm4, ymm4, ymm2		;; B2 = I2 * cosine/sine
	vaddpd	ymm4, ymm4, ymm1		;; B2 = B2 + R2

	vmovapd	ymm2, [screg+192+32]		;; cosine/sine
	vmulpd	ymm1, ymm3, ymm2		;; A4 = R4 * cosine/sine
	vsubpd	ymm1, ymm1, ymm7		;; A4 = A4 - I4
	vmulpd	ymm7, ymm7, ymm2		;; B4 = I4 * cosine/sine
	vaddpd	ymm7, ymm7, ymm3		;; B4 = B4 + R4

	vmovapd	ymm2, [screg+64]
	vmulpd	ymm5, ymm5, ymm2		;; A3 = A3 * sine (final R3)
	vmulpd	ymm0, ymm0, ymm2		;; B3 = B3 * sine (final I3)
	vmovapd	ymm2, [screg+128]
	vmulpd	ymm6, ymm6, ymm2		;; A2 = A2 * sine (final R2)
	vmulpd	ymm4, ymm4, ymm2		;; B2 = B2 * sine (final I2)

	vmovapd	[dstreg+e2], ymm5		;; Save R3
	vmovapd	[dstreg+e2+32], ymm0		;; Save I3

	vmovapd	ymm2, [screg+0+32]		;; cosine/sine
	vmulpd	ymm5, ymm8, ymm2		;; A1 = R1 * cosine/sine
	vsubpd	ymm5, ymm5, ymm9		;; A1 = A1 - I1
	vmulpd	ymm0, ymm9, ymm2		;; B1 = I1 * cosine/sine
	vaddpd	ymm0, ymm0, ymm8		;; B1 = B1 + R1

	vmovapd	ymm3, [screg+192]		;; sine
	vmulpd	ymm1, ymm1, ymm3		;; A4 = A4 * sine (final R4)
	vmulpd	ymm7, ymm7, ymm3		;; B4 = B4 * sine (final I4)
	vmovapd	ymm3, [screg+0]			;; sine
	vmulpd	ymm5, ymm5, ymm3		;; A1 = A1 * sine (final R1)
	vmulpd	ymm0, ymm0, ymm3		;; B1 = B1 * sine (final I1)

	vmovapd	[dstreg], ymm5			;; Save R1
	vmovapd	[dstreg+32], ymm0		;; Save I1
	vmovapd	[dstreg+e1], ymm6		;; Save R2
	vmovapd	[dstreg+e1+32], ymm4		;; Save I2
	vmovapd	[dstreg+e2+e1], ymm1		;; Save R4
	vmovapd	[dstreg+e2+e1+32], ymm7		;; Save I4

	bump	srcreg, srcinc
	bump	dstreg, dstinc
	bump	screg, scinc
	ENDM


IFDEF THIS_VERSION_IS_WORSE_DONT_KNOW_WHY
yr4_sg4cl_four_complex_fft4 MACRO srcreg,srcinc,d1,d2,dstreg,dstinc,e1,e2,screg,scinc,maxrpt,L1pt,L1pd

	;; On input the 64-byte cache lines hold these data values:
	;;	0	+1K	+1K	+1K	4K	+1K	+1K	+1K
	;;	1	...
	;;	2	...
	;;	3	...

	;; These are swizzled to:
	;;	0	+1	+1	+1	4K	+1	+1	+1
	;;	1K	...
	;;	2K	...
	;;	3K	...

	;; Swizzle using 8 shufpds and 8 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm0, [srcreg]			;; R1
	vmovapd	ymm1, [srcreg+d1]		;; R2
	vshufpd	ymm2, ymm0, ymm1, 15		;; Shuffle R1 and R2 to create R1/R2 hi		; 1

	vshufpd	ymm0, ymm0, ymm1, 0		;; Shuffle R1 and R2 to create R1/R2 low	; 2

	vmovapd	ymm1, [srcreg+d2]		;; R3
	vmovapd	ymm3, [srcreg+d2+d1]		;; R4
	vshufpd	ymm4, ymm1, ymm3, 15		;; Shuffle R3 and R4 to create R3/R4 hi		; 3

	vshufpd	ymm1, ymm1, ymm3, 0		;; Shuffle R3 and R4 to create R3/R4 low	; 4
	vmovapd	ymm15, [srcreg+32]		;; I1

	vperm2f128 ymm3, ymm2, ymm4, 32		;; Shuffle R1/R2 hi and R3/R4 hi (new R2)	; 5-6
	vmovapd	ymm14, [srcreg+d1+32]		;; I2

	vperm2f128 ymm2, ymm2, ymm4, 49		;; Shuffle R1/R2 hi and R3/R4 hi (new R4)	; 6-7
	vmovapd	ymm13, [srcreg+d2+32]		;; I3

	vperm2f128 ymm4, ymm0, ymm1, 32		;; Shuffle R1/R2 low and R3/R4 low (new R1)	; 7-8
	vmovapd	ymm12, [srcreg+d2+d1+32]	;; I4

	vperm2f128 ymm0, ymm0, ymm1, 49		;; Shuffle R1/R2 low and R3/R4 low (new R3)	; 8-9
	vaddpd	ymm1, ymm3, ymm2		;; R2 + R4 (newer R2)				; 8-10
	vmovapd	ymm6, [screg+128+32]		;; cosine/sine

	vsubpd	ymm3, ymm3, ymm2		;; R2 - R4 (newer R4)				; 9-11
	vmovapd	ymm7, [screg+0+32]		;; cosine/sine

	vshufpd	ymm2, ymm15, ymm14, 15		;; Shuffle I1 and I2 to create I1/I2 hi		; 10
	vaddpd	ymm5, ymm4, ymm0		;; R1 + R3 (newer R1)				; 10-12
	vmovapd	ymm8, [screg+64+32]		;; cosine/sine

	vshufpd	ymm15, ymm15, ymm14, 0		;; Shuffle I1 and I2 to create I1/I2 low	; 11
	vsubpd	ymm4, ymm4, ymm0		;; R1 - R3 (newer R3)				; 11-13
	vmovapd	ymm9, [screg+192+32]		;; cosine/sine

	vshufpd	ymm0, ymm13, ymm12, 15		;; Shuffle I3 and I4 to create I3/I4 hi		; 12

	vshufpd	ymm13, ymm13, ymm12, 0		;; Shuffle I3 and I4 to create I3/I4 low	; 13
	vsubpd	ymm12, ymm5, ymm1		;; R1 - R2 (final R2)				; 13-15

	vperm2f128 ymm14, ymm2, ymm0, 32	;; Shuffle I1/I2 hi and I3/I4 hi (new I2)	; 14-15
	vaddpd	ymm5, ymm5, ymm1		;; R1 + R2 (final R1)				; 14-16
	L1prefetch srcreg+L1pd, L1pt

	vperm2f128 ymm2, ymm2, ymm0, 49		;; Shuffle I1/I2 hi and I3/I4 hi (new I4)	; 15-16

	vperm2f128 ymm0, ymm15, ymm13, 32	;; Shuffle I1/I2 low and I3/I4 low (new I1)	; 16-17

	vperm2f128 ymm15, ymm15, ymm13, 49	;; Shuffle I1/I2 low and I3/I4 low (new I3)	; 17-18
	vaddpd	ymm13, ymm14, ymm2		;; I2 + I4 (newer I2)				; 17-19

	vsubpd	ymm14, ymm14, ymm2		;; I2 - I4 (newer I4)				; 18-20
	L1prefetch srcreg+d1+L1pd, L1pt

	vaddpd	ymm2, ymm0, ymm15		;; I1 + I3 (newer I1)				; 19-21

	vsubpd	ymm0, ymm0, ymm15		;; I1 - I3 (newer I3)				; 20-22

	vsubpd	ymm15, ymm4, ymm14		;; R3 - I4 (final R3)				; 21-23

	vaddpd	ymm4, ymm4, ymm14		;; R3 + I4 (final R4)				; 22-24
	vmulpd	ymm14, ymm12, ymm6		;; A2 = R2 * cosine/sine			; 22-26
	L1prefetch srcreg+d2+L1pd, L1pt

	vsubpd	ymm1, ymm0, ymm3		;; I3 - R4 (final I4)				; 23-25
	vmulpd	ymm11, ymm5, ymm7		;; A1 = R1 * cosine/sine			; 23-27

	vaddpd	ymm0, ymm0, ymm3		;; I3 + R4 (final I3)				; 24-26
	vmulpd	ymm3, ymm15, ymm8		;; A3 = R3 * cosine/sine			; 24-28

	vsubpd	ymm10, ymm2, ymm13		;; I1 - I2 (final I2)				; 25-27

	vaddpd	ymm2, ymm2, ymm13		;; I1 + I2 (final I1)				; 26-28
	vmulpd	ymm13, ymm4, ymm9		;; A4 = R4 * cosine/sine			; 25-29
	vmulpd	ymm9, ymm1, ymm9		;; B4 = I4 * cosine/sine			; 26-30
	L1prefetch srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm14, ymm14, ymm10		;; A2 = A2 - I2					; 27-29
	vmulpd	ymm8, ymm0, ymm8		;; B3 = I3 * cosine/sine			; 27-31

	vsubpd	ymm11, ymm11, ymm2		;; A1 = A1 - I1					; 28-30
	vmulpd	ymm6, ymm10, ymm6		;; B2 = I2 * cosine/sine			; 28-32
	vmovapd	ymm10, [screg+128]		;; sine

	vsubpd	ymm3, ymm3, ymm0		;; A3 = A3 - I3					; 29-31
	vmulpd	ymm7, ymm2, ymm7		;; B1 = I1 * cosine/sine			; 29-33
	vmovapd	ymm0, [screg+0]			;; sine

	vsubpd	ymm13, ymm13, ymm1		;; A4 = A4 - I4					; 30-32
	vmulpd	ymm14, ymm14, ymm10		;; A2 = A2 * sine (final R2)			; 30-34
	vmovapd	ymm2, [screg+64]		;; sine

	vaddpd	ymm9, ymm9, ymm4		;; B4 = B4 + R4					; 31-33
	vmulpd	ymm11, ymm11, ymm0		;; A1 = A1 * sine (final R1)			; 31-35
	vmovapd	ymm1, [screg+192]		;; sine

	vaddpd	ymm8, ymm8, ymm15		;; B3 = B3 + R3					; 32-34
	vmulpd	ymm3, ymm3, ymm2		;; A3 = A3 * sine (final R3)			; 32-36

	vaddpd	ymm6, ymm6, ymm12		;; B2 = B2 + R2					; 33-35
	vmulpd	ymm13, ymm13, ymm1		;; A4 = A4 * sine (final R4)			; 33-37

	vaddpd	ymm7, ymm7, ymm5		;; B1 = B1 + R1					; 34-36
	vmulpd	ymm9, ymm9, ymm1		;; B4 = B4 * sine (final I4)			; 34-38

	vmulpd	ymm8, ymm8, ymm2		;; B3 = B3 * sine (final I3)			; 35-39
	vmovapd	[dstreg+e1], ymm14		;; Save R2					; 35

	vmulpd	ymm6, ymm6, ymm10		;; B2 = B2 * sine (final I2)			; 36-40
	vmovapd	[dstreg], ymm11			;; Save R1					; 36

	vmulpd	ymm7, ymm7, ymm0		;; B1 = B1 * sine (final I1)			; 37-41
	vmovapd	[dstreg+e2], ymm3		;; Save R3					; 37

	vmovapd	[dstreg+e2+e1], ymm13		;; Save R4					; 38
	vmovapd	[dstreg+e2+e1+32], ymm9		;; Save I4					; 39
	vmovapd	[dstreg+e2+32], ymm8		;; Save I3					; 40
	vmovapd	[dstreg+e1+32], ymm6		;; Save I2					; 41
	vmovapd	[dstreg+32], ymm7		;; Save I1					; 42

	bump	srcreg, srcinc
	bump	dstreg, dstinc
	bump	screg, scinc
	ENDM
ENDIF

ENDIF


;;
;; ************************************* four-complex-unfft4 variants ******************************************
;;

;; Used in last levels of pass 1 (r4delay and r4dwpn cases).  Swizzling.
yr4_sg4cl_four_complex_unfft4 MACRO srcreg,srcinc,d1,d2,dstreg,dstinc,e1,e2,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd	ymm7, [screg+0+32]		;; cosine/sine
	vmovapd	ymm0, [srcreg]			;; R1
	vmulpd	ymm6, ymm0, ymm7		;; A1 = R1 * cosine/sine

	vmovapd	ymm5, [screg+128+32]		;; cosine/sine
	vmovapd	ymm4, [srcreg+d1]		;; R2
	vmulpd	ymm2, ymm4, ymm5		;; A2 = R2 * cosine/sine

	vmovapd	ymm1, [srcreg+32]		;; I1
	vaddpd	ymm6, ymm6, ymm1		;; A1 = A1 + I1
	vmulpd	ymm1, ymm1, ymm7		;; B1 = I1 * cosine/sine
	vmovapd	ymm3, [srcreg+d1+32]		;; I2
	vaddpd	ymm2, ymm2, ymm3		;; A2 = A2 + I2
	vmulpd	ymm3, ymm3, ymm5		;; B2 = I2 * cosine/sine
	vsubpd	ymm1, ymm1, ymm0		;; B1 = B1 - R1
	vsubpd	ymm3, ymm3, ymm4		;; B2 = B2 - R2

	vmulpd	ymm6, ymm6, [screg+0]		;; A1 = A1 * sine (new R1)
	vmulpd	ymm2, ymm2, [screg+128]		;; A2 = A2 * sine (new R2)

	vmovapd	ymm0, [screg+64+32]		;; cosine/sine
	vmovapd	ymm4, [srcreg+d2]		;; R3
	vmulpd	ymm5, ymm4, ymm0		;; A3 = R3 * cosine/sine

	vsubpd	ymm7, ymm6, ymm2		;; R1 - R2 (newer R2)
	vaddpd	ymm6, ymm6, ymm2		;; R1 + R2 (newer R1)

	vmovapd	[dstreg], ymm6			;; Save newer R1 temporarily
	vmovapd	[dstreg+32], ymm7		;; Save newer R2 temporarily

	vmovapd	ymm2, [srcreg+d2+32]		;; I3
	vaddpd	ymm5, ymm5, ymm2		;; A3 = A3 + I3
	vmulpd	ymm2, ymm2, ymm0		;; B3 = I3 * cosine/sine

	vmovapd	ymm0, [screg+192+32]		;; cosine/sine
	vmovapd	ymm6, [srcreg+d2+d1]		;; R4
	vmulpd	ymm7, ymm6, ymm0		;; A4 = R4 * cosine/sine
	vsubpd	ymm2, ymm2, ymm4		;; B3 = B3 - R3
	vmovapd ymm4, [srcreg+d2+d1+32]		;; I4
	vaddpd	ymm7, ymm7, ymm4		;; A4 = A4 + I4
	vmulpd	ymm4, ymm4, ymm0		;; B4 = I4 * cosine/sine
	vsubpd	ymm4, ymm4, ymm6		;; B4 = B4 - R4

	vmulpd	ymm1, ymm1, [screg+0]		;; B1 = B1 * sine (new I1)
	vmulpd	ymm3, ymm3, [screg+128]		;; B2 = B2 * sine (new I2)
	vmovapd	ymm6, [screg+64]		;; sine
	vmulpd	ymm5, ymm5, ymm6		;; A3 = A3 * sine (new R3)
	vmulpd	ymm2, ymm2, ymm6		;; B3 = B3 * sine (new I3)
	vmovapd	ymm6, [screg+192]		;; sine
	vmulpd	ymm7, ymm7, ymm6		;; A4 = A4 * sine (new R4)
	vmulpd	ymm4, ymm4, ymm6		;; B4 = B4 * sine (new I4)

	vsubpd	ymm0, ymm1, ymm3		;; I1 - I2 (newer I2)
	vaddpd	ymm1, ymm1, ymm3		;; I1 + I2 (newer I1)

	L1prefetch srcreg+L1pd, L1pt

	vsubpd	ymm3, ymm7, ymm5		;; R4 - R3 (newer I4)
	vaddpd	ymm7, ymm7, ymm5		;; R4 + R3 (newer R3)

	vsubpd	ymm5, ymm2, ymm4		;; I3 - I4 (newer R4)
	vaddpd	ymm2, ymm2, ymm4		;; I3 + I4 (newer I3)

	L1prefetch srcreg+d1+L1pd, L1pt

	vsubpd	ymm4, ymm0, ymm3		;; I2 - I4 (final I4)
	vaddpd	ymm0, ymm0, ymm3		;; I2 + I4 (final I2)

	vsubpd	ymm3, ymm1, ymm2		;; I1 - I3 (final I3)
	vaddpd	ymm1, ymm1, ymm2		;; I1 + I3 (final I1)

	L1prefetch srcreg+d2+L1pd, L1pt

	;; Swizzle using 4 shufpds and 4 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vshufpd	ymm2, ymm1, ymm0, 0		;; Shuffle I1 and I2 to create I1/I2 low
	vshufpd	ymm1, ymm1, ymm0, 15		;; Shuffle I1 and I2 to create I1/I2 hi

	vshufpd	ymm0, ymm3, ymm4, 0		;; Shuffle I3 and I4 to create I3/I4 low
	vshufpd	ymm3, ymm3, ymm4, 15		;; Shuffle I3 and I4 to create I3/I4 hi

	vperm2f128 ymm4, ymm2, ymm0, 32		;; Shuffle I1/I2 low and I3/I4 low (final I1)
	vperm2f128 ymm2, ymm2, ymm0, 49		;; Shuffle I1/I2 low and I3/I4 low (final I3)

	vperm2f128 ymm0, ymm1, ymm3, 32		;; Shuffle I1/I2 hi and I3/I4 hi (final I2)
	vperm2f128 ymm1, ymm1, ymm3, 49		;; Shuffle I1/I2 hi and I3/I4 hi (final I4)

	vmovapd	ymm3, [dstreg]			;; Reload newer R1
	vmovapd	ymm6, [dstreg+32]		;; Reload newer R2

	vmovapd	[dstreg+32], ymm4		;; Save I1
	vmovapd	[dstreg+e2+32], ymm2		;; Save I3

	vsubpd	ymm4, ymm6, ymm5		;; R2 - R4 (final R4)
	vaddpd	ymm6, ymm6, ymm5		;; R2 + R4 (final R2)

	L1prefetch srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm5, ymm3, ymm7		;; R1 - R3 (final R3)
	vaddpd	ymm3, ymm3, ymm7		;; R1 + R3 (final R1)

	vmovapd	[dstreg+e1+32], ymm0		;; Save I2
	vmovapd	[dstreg+e2+e1+32], ymm1		;; Save I4

	;; Swizzle using 4 shufpds and 4 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vshufpd	ymm0, ymm3, ymm6, 0		;; Shuffle R1 and R2 to create R1/R2 low
	vshufpd	ymm3, ymm3, ymm6, 15		;; Shuffle R1 and R2 to create R1/R2 hi

	vshufpd	ymm6, ymm5, ymm4, 0		;; Shuffle R3 and R4 to create R3/R4 low
	vshufpd	ymm5, ymm5, ymm4, 15		;; Shuffle R3 and R4 to create R3/R4 hi

	vperm2f128 ymm4, ymm0, ymm6, 32		;; Shuffle R1/R2 low and R3/R4 low (final R1)
	vperm2f128 ymm0, ymm0, ymm6, 49		;; Shuffle R1/R2 low and R3/R4 low (final R3)

	vperm2f128 ymm6, ymm3, ymm5, 32		;; Shuffle R1/R2 hi and R3/R4 hi (final R2)
	vperm2f128 ymm3, ymm3, ymm5, 49		;; Shuffle R1/R2 hi and R3/R4 hi (final R4)

	vmovapd	[dstreg], ymm4			;; Save R1
	vmovapd	[dstreg+e2], ymm0		;; Save R3
	vmovapd	[dstreg+e1], ymm6		;; Save R2
	vmovapd	[dstreg+e2+e1], ymm3		;; Save R4

	bump	srcreg, srcinc
	bump	dstreg, dstinc
	bump	screg, scinc
	ENDM

IFDEF X86_64

yr4_sg4cl_four_complex_unfft4 MACRO srcreg,srcinc,d1,d2,dstreg,dstinc,e1,e2,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd	ymm0, [screg+0+32]		;; cosine/sine
	vmovapd	ymm1, [srcreg+32]		;; I1
	vmulpd	ymm2, ymm1, ymm0		;; B1 = I1 * cosine/sine			; 1-5

	vmovapd	ymm3, [screg+128+32]		;; cosine/sine
	vmovapd	ymm4, [srcreg+d1+32]		;; I2
	vmulpd	ymm5, ymm4, ymm3		;; B2 = I2 * cosine/sine			; 2-6

	vmovapd	ymm6, [screg+64+32]		;; cosine/sine
	vmovapd	ymm7, [srcreg+d2]		;; R3
	vmulpd	ymm8, ymm7, ymm6		;; A3 = R3 * cosine/sine			; 3-7

	vmovapd	ymm9, [screg+192+32]		;; cosine/sine
	vmovapd	ymm10, [srcreg+d2+d1]		;; R4
	vmulpd	ymm11, ymm10, ymm9		;; A4 = R4 * cosine/sine			; 4-8

	vmovapd	ymm12, [srcreg+d2+32]		;; I3
	vmulpd	ymm6, ymm12, ymm6		;; B3 = I3 * cosine/sine			; 5-9

	vmovapd	ymm13, [srcreg]			;; R1
	vsubpd	ymm2, ymm2, ymm13		;; B1 = B1 - R1					; 6-8
	vmovapd ymm14, [srcreg+d2+d1+32]	;; I4
	vmulpd	ymm9, ymm14, ymm9		;; B4 = I4 * cosine/sine			; 6-10

	vmovapd	ymm15, [srcreg+d1]		;; R2
	vsubpd	ymm5, ymm5, ymm15		;; B2 = B2 - R2					; 7-9
	vmulpd	ymm0, ymm13, ymm0		;; A1 = R1 * cosine/sine			; 7-11
	vmovapd	ymm13, [screg+0]		;; sine

	vaddpd	ymm8, ymm8, ymm12		;; A3 = A3 + I3					; 8-10
	vmulpd	ymm3, ymm15, ymm3		;; A2 = R2 * cosine/sine			; 8-12
	vmovapd	ymm12, [screg+128]		;; sine

	vaddpd	ymm11, ymm11, ymm14		;; A4 = A4 + I4					; 9-11
	vmulpd	ymm2, ymm2, ymm13		;; B1 = B1 * sine (new I1)			; 9-13
	vmovapd	ymm14, [screg+64]		;; sine

	vsubpd	ymm6, ymm6, ymm7		;; B3 = B3 - R3					; 10-12
	vmulpd	ymm5, ymm5, ymm12		;; B2 = B2 * sine (new I2)			; 10-14
	vmovapd	ymm7, [screg+192]		;; sine

	vsubpd	ymm9, ymm9, ymm10		;; B4 = B4 - R4					; 11-13
	vmulpd	ymm8, ymm8, ymm14		;; A3 = A3 * sine (new R3)			; 11-15

	vaddpd	ymm0, ymm0, ymm1		;; A1 = A1 + I1					; 12-14
	vmulpd	ymm11, ymm11, ymm7		;; A4 = A4 * sine (new R4)			; 12-16

	vaddpd	ymm3, ymm3, ymm4		;; A2 = A2 + I2					; 13-15
	vmulpd	ymm6, ymm6, ymm14		;; B3 = B3 * sine (new I3)			; 13-17

	vmulpd	ymm9, ymm9, ymm7		;; B4 = B4 * sine (new I4)			; 14-18
	L1prefetch srcreg+L1pd, L1pt

	vsubpd	ymm7, ymm2, ymm5		;; I1 - I2 (newer I2)				; 15-17
	vmulpd	ymm0, ymm0, ymm13		;; A1 = A1 * sine (new R1)			; 15-19

	vaddpd	ymm2, ymm2, ymm5		;; I1 + I2 (newer I1)				; 16-18
	vmulpd	ymm3, ymm3, ymm12		;; A2 = A2 * sine (new R2)			; 16-20

	vsubpd	ymm12, ymm11, ymm8		;; R4 - R3 (newer I4)				; 17-19
	vaddpd	ymm11, ymm11, ymm8		;; R4 + R3 (newer R3)				; 18-20
	vaddpd	ymm8, ymm6, ymm9		;; I3 + I4 (newer I3)				; 19-21
	L1prefetch srcreg+d1+L1pd, L1pt

	vsubpd	ymm5, ymm7, ymm12		;; I2 - I4 (final I4)				; 20-22
	vaddpd	ymm7, ymm7, ymm12		;; I2 + I4 (final I2)				; 21-23
	vsubpd	ymm12, ymm2, ymm8		;; I1 - I3 (final I3)				; 22-24
	vaddpd	ymm2, ymm2, ymm8		;; I1 + I3 (final I1)				; 23-25
	L1prefetch srcreg+d2+L1pd, L1pt

	vaddpd	ymm8, ymm0, ymm3		;; R1 + R2 (newer R1)				; 24-26

	vsubpd	ymm6, ymm6, ymm9		;; I3 - I4 (newer R4)				; 25-27
	vshufpd	ymm9, ymm12, ymm5, 0		;; Shuffle I3 and I4 to create I3/I4 low	; 25

	vsubpd	ymm0, ymm0, ymm3		;; R1 - R2 (newer R2)				; 26-28
	vshufpd	ymm12, ymm12, ymm5, 15		;; Shuffle I3 and I4 to create I3/I4 hi		; 26

	vsubpd	ymm5, ymm8, ymm11		;; R1 - R3 (final R3)				; 27-29
	vshufpd	ymm3, ymm2, ymm7, 0		;; Shuffle I1 and I2 to create I1/I2 low	; 27
	L1prefetch srcreg+d2+d1+L1pd, L1pt

	vaddpd	ymm8, ymm8, ymm11		;; R1 + R3 (final R1)				; 28-30
	vshufpd	ymm2, ymm2, ymm7, 15		;; Shuffle I1 and I2 to create I1/I2 hi		; 28

	vsubpd	ymm7, ymm0, ymm6		;; R2 - R4 (final R4)				; 29-31
	vperm2f128 ymm11, ymm3, ymm9, 32	;; Shuffle I1/I2 low and I3/I4 low (final I1)	; 29-30

	vaddpd	ymm0, ymm0, ymm6		;; R2 + R4 (final R2)				; 30-32
	vperm2f128 ymm3, ymm3, ymm9, 49		;; Shuffle I1/I2 low and I3/I4 low (final I3)	; 30-31

	vperm2f128 ymm9, ymm2, ymm12, 32	;; Shuffle I1/I2 hi and I3/I4 hi (final I2)	; 31-32
	vmovapd	[dstreg+32], ymm11		;; Save I1					; 31

	vperm2f128 ymm2, ymm2, ymm12, 49	;; Shuffle I1/I2 hi and I3/I4 hi (final I4)	; 32-33
	vmovapd	[dstreg+e2+32], ymm3		;; Save I3					; 32

	vmovapd	[dstreg+e1+32], ymm9		;; Save I2					; 33

	vshufpd	ymm9, ymm5, ymm7, 0		;; Shuffle R3 and R4 to create R3/R4 low	; 34
	vmovapd	[dstreg+e2+e1+32], ymm2		;; Save I4					; 34

	vshufpd	ymm5, ymm5, ymm7, 15		;; Shuffle R3 and R4 to create R3/R4 hi		; 35

	vshufpd	ymm7, ymm8, ymm0, 0		;; Shuffle R1 and R2 to create R1/R2 low	; 36

	vshufpd	ymm8, ymm8, ymm0, 15		;; Shuffle R1 and R2 to create R1/R2 hi		; 37

	vperm2f128 ymm0, ymm7, ymm9, 32		;; Shuffle R1/R2 low and R3/R4 low (final R1)	; 38-39

	vperm2f128 ymm7, ymm7, ymm9, 49		;; Shuffle R1/R2 low and R3/R4 low (final R3)	; 39-40

	vperm2f128 ymm9, ymm8, ymm5, 32		;; Shuffle R1/R2 hi and R3/R4 hi (final R2)	; 40-41
	vmovapd	[dstreg], ymm0			;; Save R1					; 40

	vperm2f128 ymm8, ymm8, ymm5, 49		;; Shuffle R1/R2 hi and R3/R4 hi (final R4)	; 41-42
	vmovapd	[dstreg+e2], ymm7		;; Save R3					; 41

	vmovapd	[dstreg+e1], ymm9		;; Save R2					; 42
	vmovapd	[dstreg+e2+e1], ymm8		;; Save R4					; 43

	bump	srcreg, srcinc
	bump	dstreg, dstinc
	bump	screg, scinc
	ENDM

ENDIF


;;
;; ************************************* eight-reals-fft variants ******************************************
;;

;; These macros operate on eight reals doing 2 and 3/4 levels of the FFT and applying
;; the sin/cos multipliers afterwards.  The output is 2 reals (only 2 levels of FFT done)
;; and 3 complex numbers (3 levels of FFT performed).  These macros take a screg
;; that points to twiddles w^n, w^2n, and w^5n.

;; Standard eight-reals FFT macro.
yr4_4cl_eight_reals_fft_preload MACRO
	yr4_8r_fft_cmn_preload
	ENDM
yr4_4cl_eight_reals_fft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_8r_fft_cmn srcreg,srcinc,0,d1,d2,screg,screg+64,screg+128,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

;; Like 4cl but uses one combined sin/cos data ptr.  The first two sin/cos entries can be used by
;; four complex macros at the same FFT level.  The third and fourth sin/cos entries are used by
;; this eight reals macro.
yr4_4cl_csc_eight_reals_fft_preload MACRO
	yr4_8r_fft_cmn_preload
	ENDM
yr4_4cl_csc_eight_reals_fft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_8r_fft_cmn srcreg,srcinc,0,d1,d2,screg+128,screg,screg+192,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

;; Like 4cl but swizzles the outputs before storing the results.
;; This macro is used in the middle levels of the second pass of two pass AVX FFTs.
yr4_s4cl_eight_reals_fft_preload MACRO
	yr4_s8r_fft_cmn_preload
	ENDM
yr4_s4cl_eight_reals_fft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_s8r_fft_cmn srcreg,srcinc,0,d1,d2,screg,screg+64,screg+128,screg,scinc,,,maxrpt,L1pt,L1pd
	ENDM

;; Like s4cl but uses two sin/cos data ptrs.
yr4_s4cl_2sc_eight_reals_fft_preload MACRO
	yr4_s8r_fft_cmn_preload
	ENDM
yr4_s4cl_2sc_eight_reals_fft MACRO srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	yr4_s8r_fft_cmn srcreg,srcinc,0,d1,d2,screg2,screg1,screg2+64,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM

;; Like s4cl but uses one combined sin/cos data ptr.  The first two sin/cos entries can be used by
;; four complex macros at the same FFT level.  The third and fourth sin/cos entries are used by
;; this eight reals macro.
yr4_s4cl_csc_eight_reals_fft_preload MACRO
	yr4_s8r_fft_cmn_preload
	ENDM
yr4_s4cl_csc_eight_reals_fft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_s8r_fft_cmn srcreg,srcinc,0,d1,d2,screg+128,screg,screg+192,screg,scinc,,,maxrpt,L1pt,L1pd
	ENDM

;; Like s4cl but offsets the input by rbx.
;; This macro is used in the first levels of one pass and two pass AVX FFTs.
yr4_fs4cl_eight_reals_fft_preload MACRO
	yr4_s8r_fft_cmn_preload
	ENDM
yr4_fs4cl_eight_reals_fft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_s8r_fft_cmn srcreg,srcinc,rbx,d1,d2,screg,screg+64,screg+128,screg,scinc,,,maxrpt,L1pt,L1pd
	ENDM

;; Like fs4cl but uses 2 sin/cos ptrs.
yr4_fs4cl_2sc_eight_reals_fft_preload MACRO
	yr4_s8r_fft_cmn_preload
	ENDM
yr4_fs4cl_2sc_eight_reals_fft MACRO srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	yr4_s8r_fft_cmn srcreg,srcinc,rbx,d1,d2,screg2,screg1,screg2+64,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM

;; Common macro for eight-reals-FFT doing 2 and 3/4 levels.

yr4_8r_fft_cmn_preload MACRO
	ENDM
yr4_8r_fft_cmn MACRO srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd	ymm0, [srcreg+srcoff+d2+d1]	;; R4
	vmovapd	ymm7, [srcreg+srcoff+d2+d1+32]	;; R8
	vsubpd	ymm3, ymm0, ymm7	;; new R8 = R4 - R8			; 1-3
	vaddpd	ymm7, ymm0, ymm7	;; new R4 = R4 + R8			; 2-4

	vmovapd	ymm4, [srcreg+srcoff+d1]	;; R2
	vmovapd	ymm5, [srcreg+srcoff+d1+32]	;; R6
	vsubpd	ymm1, ymm4, ymm5	;; new R6 = R2 - R6			; 3-5
	vaddpd	ymm5, ymm4, ymm5	;; new R2 = R2 + R6			; 4-6

	vbroadcastsd ymm2, YMM_SQRTHALF ;; sqrt(1/2)
	vmulpd	ymm3, ymm3, ymm2	;; R8 = R8 * square root		;  4-8
	vmulpd	ymm1, ymm1, ymm2	;; R6 = R6 * square root		;  6-10

	vmovapd	ymm2, [srcreg+srcoff]		;; R1
	vmovapd	ymm4, [srcreg+srcoff+32]	;; R5
	vsubpd	ymm0, ymm2, ymm4	;; new R5 = R1 - R5			; 5-7
	vaddpd	ymm4, ymm2, ymm4	;; new R1 = R1 + R5			; 6-8

	vaddpd	ymm2, ymm5, ymm7	;; R2 + R4 (final I1, a.k.a 2nd real result) ; 7-9
	vsubpd	ymm5, ymm5, ymm7	;; R2 - R4 (final I2)			; 8-10

	vmovapd	ymm6, [srcreg+srcoff+d2]	;; R3
	vmovapd	ymm7, [srcreg+srcoff+d2+32]	;; R7

	vmovapd	[srcreg+32], ymm2	;; Save final I1			; 10

	vaddpd	ymm2, ymm6, ymm7	;; new R3 = R3 + R7			; 9-11
	vsubpd	ymm6, ymm6, ymm7	;; new R7 = R3 - R7			; 10-12

	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	ymm7, ymm1, ymm3	;; R6 = R6 - R8 (Real part)		; 11-13
	vaddpd	ymm3, ymm1, ymm3	;; R8 = R6 + R8 (Imaginary part)	; 12-14

	vaddpd	ymm1, ymm4, ymm2	;; R1 + R3 (final R1)			; 13-15
	vsubpd	ymm4, ymm4, ymm2	;; R1 - R3 (final R2)			; 14-16

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm2, ymm0, ymm7	;; R5 - R6 (final R4)			; 15-17
	vaddpd	ymm0, ymm0, ymm7	;; R5 + R6 (final R3)			; 16-18

	vsubpd	ymm7, ymm6, ymm3	;; R7 - R8 (final I4)			; 17-19
	vaddpd	ymm6, ymm6, ymm3	;; R7 + R8 (final I3)			; 18-20

	vmovapd	[srcreg], ymm1		;; Save final R1			; 16
	vmovapd	ymm1, [screg2+32]	;; cosine/sine for w^2n
	vmulpd	ymm3, ymm4, ymm1	;; A2 = R2 * cosine/sine for w^2n	;  17-21
	vsubpd	ymm3, ymm3, ymm5	;; A2 = A2 - I2				; 22-24
	vmulpd	ymm5, ymm5, ymm1	;; B2 = I2 * cosine/sine for w^2n	;  18-22
	vaddpd	ymm5, ymm5, ymm4	;; B2 = B2 + R2				; 23-25

	L1prefetchw srcreg+d2+L1pd, L1pt

	vmovapd	ymm1, [screg5+32]	;; cosine/sine for w^5n
	vmulpd	ymm4, ymm2, ymm1	;; A4 = R4 * cosine/sine for w^5n	;  19-23
	vsubpd	ymm4, ymm4, ymm7	;; A4 = A4 - I4				; 24-26
	vmulpd	ymm7, ymm7, ymm1	;; B4 = I4 * cosine/sine for w^5n	;  20-24
	vaddpd	ymm7, ymm7, ymm2	;; B4 = B4 + R4				; 25-27

	vmovapd	ymm1, [screg1+32]	;; cosine/sine for w^n
	vmulpd	ymm2, ymm0, ymm1	;; A3 = R3 * cosine/sine for w^n	;  21-25
	vsubpd	ymm2, ymm2, ymm6	;; A3 = A3 - I3				; 26-28
	vmulpd	ymm6, ymm6, ymm1	;; B3 = I3 * cosine/sine for w^n	;  22-26
	vaddpd	ymm6, ymm6, ymm0	;; B3 = B3 + R3				; 27-29

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vmovapd	ymm0, [screg2]		;; sine for w^2n
	vmulpd	ymm3, ymm3, ymm0	;; A2 = A2 * sine (final R2)		;  25-29
	vmulpd	ymm5, ymm5, ymm0	;; B2 = B2 * sine (final I2)		;  26-30
	vmovapd	ymm0, [screg5]		;; sine for w^5n
	vmulpd	ymm4, ymm4, ymm0	;; A4 = A4 * sine (final R4)		;  27-31
	vmulpd	ymm7, ymm7, ymm0	;; B4 = B4 * sine (final I4)		;  28-32
	vmovapd	ymm0, [screg1]		;; sine for w^n
	vmulpd	ymm2, ymm2, ymm0	;; A3 = A3 * sine (final R3)		;  29-33
	vmulpd	ymm6, ymm6, ymm0	;; B3 = B3 * sine (final I3)		;  30-34

	vmovapd	[srcreg+d1], ymm3	;; Save R2
	vmovapd	[srcreg+d1+32], ymm5	;; Save I2
	vmovapd	[srcreg+d2], ymm2	;; Save R3
	vmovapd	[srcreg+d2+32], ymm6	;; Save I3
	vmovapd	[srcreg+d2+d1], ymm4	;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm7	;; Save I4

	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

yr4_s8r_fft_cmn_preload MACRO
	ENDM
yr4_s8r_fft_cmn MACRO srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,scregA,scincA,scregB,scincB,maxrpt,L1pt,L1pd
	vmovapd	ymm0, [srcreg+srcoff+d2+d1]	;; R4
	vmovapd	ymm7, [srcreg+srcoff+d2+d1+32]	;; R8
	vsubpd	ymm3, ymm0, ymm7	;; new R8 = R4 - R8			; 1-3
	vaddpd	ymm7, ymm0, ymm7	;; new R4 = R4 + R8			; 2-4

	vmovapd	ymm4, [srcreg+srcoff+d1]	;; R2
	vmovapd	ymm5, [srcreg+srcoff+d1+32]	;; R6
	vsubpd	ymm1, ymm4, ymm5	;; new R6 = R2 - R6			; 3-5
	vaddpd	ymm5, ymm4, ymm5	;; new R2 = R2 + R6			; 4-6

	vbroadcastsd ymm2, YMM_SQRTHALF ;; sqrt(1/2)
	vmulpd	ymm3, ymm3, ymm2	;; R8 = R8 * square root		;  4-8
	vmulpd	ymm1, ymm1, ymm2	;; R6 = R6 * square root		;  6-10

	vmovapd	ymm2, [srcreg+srcoff]		;; R1
	vmovapd	ymm4, [srcreg+srcoff+32]	;; R5
	vsubpd	ymm0, ymm2, ymm4	;; new R5 = R1 - R5			; 5-7
	vaddpd	ymm4, ymm2, ymm4	;; new R1 = R1 + R5			; 6-8

	vaddpd	ymm2, ymm5, ymm7	;; R2 + R4 (final I1, a.k.a 2nd real result) ; 7-9
	vsubpd	ymm5, ymm5, ymm7	;; R2 - R4 (final I2)			; 8-10

	vmovapd	ymm6, [srcreg+srcoff+d2]	;; R3
	vmovapd	ymm7, [srcreg+srcoff+d2+32]	;; R7

	vmovapd	[srcreg+32], ymm2	;; Save final I1			; 10

	vaddpd	ymm2, ymm6, ymm7	;; new R3 = R3 + R7			; 9-11
	vsubpd	ymm6, ymm6, ymm7	;; new R7 = R3 - R7			; 10-12

	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	ymm7, ymm1, ymm3	;; R6 = R6 - R8 (Real part)		; 11-13
	vaddpd	ymm3, ymm1, ymm3	;; R8 = R6 + R8 (Imaginary part)	; 12-14

	vaddpd	ymm1, ymm4, ymm2	;; R1 + R3 (final R1)			; 13-15
	vsubpd	ymm4, ymm4, ymm2	;; R1 - R3 (final R2)			; 14-16

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm2, ymm0, ymm7	;; R5 - R6 (final R4)			; 15-17
	vaddpd	ymm0, ymm0, ymm7	;; R5 + R6 (final R3)			; 16-18

	vsubpd	ymm7, ymm6, ymm3	;; R7 - R8 (final I4)			; 17-19
	vaddpd	ymm6, ymm6, ymm3	;; R7 + R8 (final I3)			; 18-20

	vmovapd	[srcreg], ymm1		;; Save final R1			; 16
	vmovapd	ymm1, [screg2+32]	;; cosine/sine for w^2n
	vmulpd	ymm3, ymm4, ymm1	;; A2 = R2 * cosine/sine for w^2n	;  17-21
	vsubpd	ymm3, ymm3, ymm5	;; A2 = A2 - I2				; 22-24
	vmulpd	ymm5, ymm5, ymm1	;; B2 = I2 * cosine/sine for w^2n	;  18-22
	vaddpd	ymm5, ymm5, ymm4	;; B2 = B2 + R2				; 23-25

	L1prefetchw srcreg+d2+L1pd, L1pt

	vmovapd	ymm1, [screg5+32]	;; cosine/sine for w^5n
	vmulpd	ymm4, ymm2, ymm1	;; A4 = R4 * cosine/sine for w^5n	;  19-23
	vsubpd	ymm4, ymm4, ymm7	;; A4 = A4 - I4				; 24-26
	vmulpd	ymm7, ymm7, ymm1	;; B4 = I4 * cosine/sine for w^5n	;  20-24
	vaddpd	ymm7, ymm7, ymm2	;; B4 = B4 + R4				; 25-27

	vmovapd	ymm1, [screg1+32]	;; cosine/sine for w^n
	vmulpd	ymm2, ymm0, ymm1	;; A3 = R3 * cosine/sine for w^n	;  21-25
	vsubpd	ymm2, ymm2, ymm6	;; A3 = A3 - I3				; 26-28
	vmulpd	ymm6, ymm6, ymm1	;; B3 = I3 * cosine/sine for w^n	;  22-26
	vaddpd	ymm6, ymm6, ymm0	;; B3 = B3 + R3				; 27-29

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vmovapd	ymm0, [screg2]		;; sine for w^2n
	vmulpd	ymm3, ymm3, ymm0	;; A2 = A2 * sine (final R2)		;  25-29
	vmulpd	ymm5, ymm5, ymm0	;; B2 = B2 * sine (final I2)		;  26-30
	vmovapd	ymm0, [screg5]		;; sine for w^5n
	vmulpd	ymm4, ymm4, ymm0	;; A4 = A4 * sine (final R4)		;  27-31
	vmulpd	ymm7, ymm7, ymm0	;; B4 = B4 * sine (final I4)		;  28-32
	vmovapd	ymm0, [screg1]		;; sine for w^n
	vmulpd	ymm2, ymm2, ymm0	;; A3 = A3 * sine (final R3)		;  29-33
	vmulpd	ymm6, ymm6, ymm0	;; B3 = B3 * sine (final I3)		;  30-34

	;; Swizzle using 8 shufpds and 8 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm1, [srcreg]		;; Reload saved R1

	vshufpd	ymm0, ymm1, ymm3, 0	;; Shuffle R1 and R2 to create 0 8 2 10
	vshufpd	ymm1, ymm1, ymm3, 15	;; Shuffle R1 and R2 to create 1 9 3 11

	vshufpd	ymm3, ymm2, ymm4, 0	;; Shuffle R3 and R4 to create 16 24 18 26
	vshufpd	ymm2, ymm2, ymm4, 15	;; Shuffle R3 and R4 to create 17 25 19 27

	vperm2f128 ymm4, ymm0, ymm3, 32	;; Shuffle R1/R2 low and R3/R4 low (0 8 16 24)
	vperm2f128 ymm0, ymm0, ymm3, 49	;; Shuffle R1/R2 low and R3/R4 low (2 10 18 26)

	vperm2f128 ymm3, ymm1, ymm2, 32	;; Shuffle R1/R2 hi and R3/R4 hi (1 9 17 25)
	vperm2f128 ymm1, ymm1, ymm2, 49	;; Shuffle R1/R2 hi and R3/R4 hi (3 11 19 27)

	vmovapd	[srcreg], ymm4		;; Save (0 8 16 24)
	vmovapd	[srcreg+d2], ymm0	;; Save (2 10 18 26)
	vmovapd	[srcreg+d1], ymm3	;; Save (1 9 17 25)
	vmovapd	[srcreg+d2+d1], ymm1	;; Save (3 11 19 27)

	vmovapd	ymm4, [srcreg+32]	;; Reload saved I1

	vshufpd	ymm0, ymm4, ymm5, 0	;; Shuffle I1 and I2 to create 4 12 6 14
	vshufpd	ymm4, ymm4, ymm5, 15	;; Shuffle I1 and I2 to create 5 13 7 15

	vshufpd	ymm5, ymm6, ymm7, 0	;; Shuffle I3 and I4 to create 16 28 22 30
	vshufpd	ymm6, ymm6, ymm7, 15	;; Shuffle I3 and I4 to create 21 29 23 31

	vperm2f128 ymm7, ymm0, ymm5, 32	;; Shuffle I1/I2 low and I3/I4 low (4 12 20 28)
	vperm2f128 ymm0, ymm0, ymm5, 49	;; Shuffle I1/I2 low and I3/I4 low (6 14 22 30)

	vperm2f128 ymm5, ymm4, ymm6, 32	;; Shuffle I1/I2 hi and I3/I4 hi (5 13 21 29)
	vperm2f128 ymm4, ymm4, ymm6, 49	;; Shuffle I1/I2 hi and I3/I4 hi (7 15 23 31)

	vmovapd	[srcreg+32], ymm7	;; Save (4 12 20 28)
	vmovapd	[srcreg+d2+32], ymm0	;; Save (6 14 22 30)
	vmovapd	[srcreg+d1+32], ymm5	;; Save (5 13 21 29)
	vmovapd	[srcreg+d2+d1+32], ymm4	;; Save (7 15 23 31)

	bump	srcreg, srcinc
	bump	scregA, scincA
	bump	scregB, scincB
	ENDM

;; 64-bit version

IFDEF X86_64

yr4_8r_fft_cmn_preload MACRO
	vmovapd	ymm15, YMM_SQRTHALF
	ENDM

yr4_8r_fft_cmn MACRO srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	yr4_8r_fft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 2,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 3,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 4,exec,exec,noexec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 5,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	bump	srcreg, 5*srcinc
	bump	screg, 5*scinc
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	yr4_8r_fft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 2,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 3,exec,exec,noexec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 4,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	screg, 4*scinc
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	yr4_8r_fft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 2,exec,exec,noexec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 3,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	screg, 3*scinc
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	yr4_8r_fft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 1,exec,exec,noexec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 2,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	screg, 2*scinc
	ELSE
	yr4_8r_fft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 0,noexec,exec,noexec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_fft_unroll 1,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDIF
	ENDM

yr4_8r_fft_unroll MACRO iter,prev,this,next,srcreg,srcinc,srcoff,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<ymm0>
y1	TEXTEQU	<ymm1>
y2	TEXTEQU	<ymm2>
y3	TEXTEQU	<ymm3>
y4	TEXTEQU	<ymm4>
y5	TEXTEQU	<ymm5>
y6	TEXTEQU	<ymm6>
y7	TEXTEQU	<ymm7>
y8	TEXTEQU	<ymm8>
y9	TEXTEQU	<ymm9>
y10	TEXTEQU	<ymm10>
y11	TEXTEQU	<ymm11>
y12	TEXTEQU	<ymm12>
y13	TEXTEQU	<ymm13>
y14	TEXTEQU	<ymm14>
	ENDIF

;; On later calls, previous R2,I2,R4,B4,A3,B3,sine5,sine1 will be in y0-7.  This R2,R4,R6,R8,R1 will be in y8-12.
;; The remaining registers are free,  ymm15 preloaded with YMM_SQRTHALF.

this	vsubpd	y14, y9, y11	;; new R8 = R4 - R8			; 1-3
prev	vmulpd	y3, y3, y6	;; B4 = B4 * sine (final I4)		;  1-5
this	vmovapd	y6, [srcreg+iter*srcinc+srcoff+32] ;; R5

this	vsubpd	y13, y8, y10	;; new R6 = R2 - R6			; 2-4
prev	vmulpd	y4, y4, y7	;; A3 = A3 * sine (final R3)		;  2-6
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1], y0	;; Save R2		; 3
this	vmovapd	y0, [srcreg+iter*srcinc+srcoff+d2] ;; R3

this	vaddpd	y9, y9, y11	;; new R4 = R4 + R8			; 3-5
prev	vmulpd	y5, y5, y7	;; B3 = B3 * sine (final I3)		;  3-7
this	vmovapd	y7, [srcreg+iter*srcinc+srcoff+d2+32] ;; R7

this	vaddpd	y8, y8, y10	;; new R2 = R2 + R6			; 4-6
this	vmulpd	y14, y14, ymm15	;; R8 = R8 * square root 1/2		;  4-8
this	vmovapd	y10, [screg2+iter*scinc+32] ;; cosine/sine for w^2n

this	vaddpd	y11, y12, y6	;; new R1 = R1 + R5			; 5-7
this	vmulpd	y13, y13, ymm15	;; R6 = R6 * square root 1/2		;  5-9
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1+32], y1 ;; Save I2		; 4

this	vaddpd	y1, y0, y7	;; new R3 = R3 + R7			; 6-8
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+d1], y2 ;; Save R4		; 5
this	vmovapd	y2, [screg5+iter*scinc+32] ;; cosine/sine for w^5n

this	vsubpd	y12, y12, y6	;; new R5 = R1 - R5			; 7-9
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+d1+32], y3 ;; Save I4	; 6

this	vsubpd	y0, y0, y7	;; new R7 = R3 - R7			; 8-10
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2], y4	;; Save R3		; 7

this	vsubpd	y4, y11, y1	;; R1 - R3 (final R2)			; 9-11
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+32], y5 ;; Save I3		; 8

this	vsubpd	y5, y13, y14	;; R6 = R6 - R8 (Real part)		; 10-12

this	vaddpd	y13, y13, y14	;; R8 = R6 + R8 (Imaginary part)	; 11-13
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	vsubpd	y14, y8, y9	;; R2 - R4 (final I2)			; 12-14

this	vsubpd	y7, y12, y5	;; R5 - R6 (final R4)			; 13-15
this next yloop_unrolled_one

this	vsubpd	y3, y0, y13	;; R7 - R8 (final I4)			; 14-16
this	vmulpd	y6, y4, y10	;; A2 = R2 * cosine/sine for w^2n	;  14-18
this	L1prefetchw srcreg+iter*srcinc+d2+L1pd, L1pt

this	vaddpd	y12, y12, y5	;; R5 + R6 (final R3)			; 15-17
this	vmulpd	y10, y14, y10	;; B2 = I2 * cosine/sine for w^2n	;  15-19
this	vmovapd	y5, [screg1+iter*scinc+32] ;; cosine/sine for w^n

this	vaddpd	y0, y0, y13	;; R7 + R8 (final I3)			; 16-18
this	vmulpd	y13, y7, y2	;; A4 = R4 * cosine/sine for w^5n	;  16-20
this	L1prefetchw srcreg+iter*srcinc+d2+d1+L1pd, L1pt

this	vaddpd	y11, y11, y1	;; R1 + R3 (final R1)			; 17-19
this	vmulpd	y2, y3, y2	;; B4 = I4 * cosine/sine for w^5n	;  17-21
this	vmovapd	y1, [screg2+iter*scinc] ;; sine for w^2n

this	vaddpd	y8, y8, y9	;; R2 + R4 (final I1, a.k.a 2nd real result) ; 18-20
this	vmulpd	y9, y12, y5	;; A3 = R3 * cosine/sine for w^n	;  18-22
this	vmovapd	[srcreg+iter*srcinc], y11 ;; Save R1			; 20
this	vmovapd	y11, [screg5+iter*scinc] ;; sine for w^5n

this	vsubpd	y6, y6, y14	;; A2 = A2 - I2				; 19-21
this	vmulpd	y5, y0, y5	;; B3 = I3 * cosine/sine for w^n	;  19-23
next	vmovapd	y14, [srcreg+(iter+1)*srcinc+srcoff+d2+d1] ;; R4

this	vaddpd	y10, y10, y4	;; B2 = B2 + R2				; 20-22
next	vmovapd	y4, [srcreg+(iter+1)*srcinc+srcoff+d2+d1+32] ;; R8

this	vsubpd	y13, y13, y3	;; A4 = A4 - I4				; 21-23
next	vmovapd	y3, [srcreg+(iter+1)*srcinc+srcoff+d1] ;; R2
this	vmovapd	[srcreg+iter*srcinc+32], y8 ;; Save I1			; 21

this	vaddpd	y2, y2, y7	;; B4 = B4 + R4				; 22-24
this	vmulpd	y6, y6, y1	;; A2 = A2 * sine (final R2)		;  22-26
next	vmovapd	y7, [srcreg+(iter+1)*srcinc+srcoff+d1+32] ;; R6

this	vsubpd	y9, y9, y0	;; A3 = A3 - I3				; 23-25
this	vmulpd	y10, y10, y1	;; B2 = B2 * sine (final I2)		;  23-27
this	vmovapd	y1, [screg1+iter*scinc] ;; sine for w^n

this	vaddpd	y5, y5, y12	;; B3 = B3 + R3				; 24-26
this	vmulpd	y13, y13, y11	;; A4 = A4 * sine (final R4)		;  24-28
next	vmovapd	y12, [srcreg+(iter+1)*srcinc+srcoff] ;; R1

;; Shuffle register assignments so that next call has R2,I2,R4,B4,A3,B3,sine5,sine1 in y0-7 and next R2,R4,R6,R8,R1 in y8-12.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y6
y6	TEXTEQU	y11
y11	TEXTEQU	y4
y4	TEXTEQU	y9
y9	TEXTEQU	y14
y14	TEXTEQU	y8
y8	TEXTEQU	y3
y3	TEXTEQU	y2
y2	TEXTEQU	y13
y13	TEXTEQU	ytmp
ytmp	TEXTEQU y1
y1	TEXTEQU	y10
y10	TEXTEQU	y7
y7	TEXTEQU ytmp
	ENDM

ENDIF


;;
;; ************************************* eight-reals-unfft variants ******************************************
;;

;; These macros produce eight reals after doing 2 and 3/4 levels of the inverse FFT applying
;; the sin/cos multipliers beforehand.  The input is 2 reals (only 2 levels of inverse FFT done)
;; and 3 complex numbers (3 levels of inverse FFT performed).  These macros take a screg
;; that points to twiddles w^n, w^2n, and w^5n.

;; Standard eight-reals unfft.
yr4_4cl_eight_reals_unfft_preload MACRO
	yr4_8r_unfft_cmn_preload
	ENDM
yr4_4cl_eight_reals_unfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_8r_unfft_cmn srcreg,srcinc,d1,d2,screg,screg+64,screg+128,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

;; Like 4cl but uses one combined sin/cos data ptr.  The first two sin/cos entries can be used by
;; four complex macros at the same FFT level.  The third and fourth sin/cos entries are used by
;; this eight reals macro.
yr4_4cl_csc_eight_reals_unfft_preload MACRO
	yr4_8r_unfft_cmn_preload
	ENDM
yr4_4cl_csc_eight_reals_unfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_8r_unfft_cmn srcreg,srcinc,d1,d2,screg+128,screg,screg+192,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

;; Like 4cl, but swizzles the inputs.
;; This version is used in the last levels of a one pass AVX FFT.
yr4_s4cl_eight_reals_unfft_preload MACRO
	yr4_s8r_unfft_cmn_preload
	ENDM
yr4_s4cl_eight_reals_unfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_s8r_unfft_cmn srcreg,srcinc,d1,d2,screg,screg+64,screg+128,screg,scinc,,,maxrpt,L1pt,L1pd
	ENDM

;; Like s4cl but uses two sin/cos data ptrs.
yr4_s4cl_2sc_eight_reals_unfft_preload MACRO
	yr4_s8r_unfft_cmn_preload
	ENDM
yr4_s4cl_2sc_eight_reals_unfft MACRO srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	yr4_s8r_unfft_cmn srcreg,srcinc,d1,d2,screg2,screg1,screg2+64,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM

;; Like s4cl but uses one combined sin/cos data ptr.  The first two sin/cos entries can be used by
;; four complex macros at the same FFT level.  The third and fourth sin/cos entries are used by
;; this eight reals macro.
yr4_s4cl_csc_eight_reals_unfft_preload MACRO
	yr4_s8r_unfft_cmn_preload
	ENDM
yr4_s4cl_csc_eight_reals_unfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	yr4_s8r_unfft_cmn srcreg,srcinc,d1,d2,screg+128,screg,screg+192,screg,scinc,,,maxrpt,L1pt,L1pd
	ENDM

;; Common code for eight reals unfft
yr4_8r_unfft_cmn_preload MACRO
	ENDM
yr4_8r_unfft_cmn MACRO srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd	ymm5, [screg5+32]	;; cosine/sine for w^5n
	vmovapd	ymm2, [srcreg+d2+d1]	;; R4
	vmulpd	ymm4, ymm2, ymm5	;; A4 = R4 * cosine/sine		; 1-5

	vmovapd	ymm6, [screg1+32] 	;; cosine/sine for w^n
	vmovapd	ymm3, [srcreg+d2]	;; R3
	vmulpd	ymm0, ymm3, ymm6	;; A3 = R3 * cosine/sine for w^n	; 2-6

	vmovapd	ymm7, [srcreg+d2+d1+32]	;; I4
	vmulpd	ymm5, ymm7, ymm5	;; B4 = I4 * cosine/sine		; 3-7

	vmovapd	ymm1, [srcreg+d2+32]	;; I3
	vmulpd	ymm6, ymm1, ymm6	;; B3 = I3 * cosine/sine for w^n	; 4-8

	vaddpd	ymm4, ymm4, ymm7	;; A4 = A4 + I4				; 6-8
	vaddpd	ymm0, ymm0, ymm1	;; A3 = A3 + I3				; 7-9
	vsubpd	ymm5, ymm5, ymm2	;; B4 = B4 - R4				; 8-10

	vmovapd	ymm7, [screg2+32]	;; cosine/sine for w^2n
	vmovapd	ymm1, [srcreg+d1]	;; R2
	vmulpd	ymm2, ymm1, ymm7	;; A2 = R2 * cosine/sine		; 5-9

	vsubpd	ymm6, ymm6, ymm3	;; B3 = B3 - R3				; 9-11

	vmovapd	ymm3, [srcreg+d1+32]	;; I2
	vmulpd	ymm7, ymm3, ymm7	;; B2 = I2 * cosine/sine		; 6-10

	vaddpd	ymm2, ymm2, ymm3	;; A2 = A2 + I2				; 10-12
	vsubpd	ymm7, ymm7, ymm1	;; B2 = B2 - R2				; 11-13

	L1prefetchw srcreg+L1pd, L1pt

	vmovapd	ymm1, [screg5]		;; sine for w^5n			
	vmulpd	ymm4, ymm4, ymm1	;; new R7 = A4 * sine			; 9-13
	vmovapd	ymm3, [screg1]		;; sine for w^n
	vmulpd	ymm0, ymm0, ymm3	;; new R5 = A3 * sine			; 10-14
	vmulpd	ymm5, ymm5, ymm1	;; new R8 = B4 * sine			; 11-15
	vmulpd	ymm6, ymm6, ymm3	;; new R6 = B3 * sine			; 12-16
	vmovapd	ymm1, [screg2]		;; sine for w^2n
	vmulpd	ymm2, ymm2, ymm1	;; new R3 = A2 * sine			; 13-17
	vmulpd	ymm7, ymm7, ymm1	;; new R4 = B2 * sine			; 14-18

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm1, ymm0, ymm4	;; new R6 = R5 - R7			; 15-17
	vaddpd	ymm0, ymm0, ymm4	;; new R5 = R5 + R7			; 16-18

	vsubpd	ymm4, ymm6, ymm5	;; new R8 = R6 - R8			; 17-19
	vaddpd	ymm6, ymm6, ymm5	;; new R7 = R6 + R8			; 18-20

	vaddpd	ymm5, ymm1, ymm4	;; R6 = R6 + R8				; 20-22
	vsubpd	ymm4, ymm4, ymm1	;; R8 = R8 - R6				; 21-23

	L1prefetchw srcreg+d2+L1pd, L1pt

	vmovapd	ymm1, [srcreg]		;; R1
	vsubpd	ymm3, ymm1, ymm2	;; R1 - R3 (new R3)			; 19-21
	vaddpd	ymm1, ymm1, ymm2	;; R1 + R3 (new R1)			; 22-24

	vsubpd	ymm2, ymm3, ymm6	;; R3 - R7 (final R7)			; 23-25
	vmulpd	ymm5, ymm5, YMM_SQRTHALF ;; R6 = R6 * square root of 1/2	; 23-27

	vaddpd	ymm3, ymm3, ymm6	;; R3 + R7 (final R3)			; 24-26
	vmulpd	ymm4, ymm4, YMM_SQRTHALF ;; R8 = R8 * square root of 1/2	; 24-28

	vmovapd	ymm6, [srcreg+32]	;; I1 (a.k.a R2)
	vmovapd	[srcreg+d2+32], ymm2	;; Save R7				; 26
	vaddpd	ymm2, ymm6, ymm7	;; R2 + R4 (new R2)			; 25-27

	vsubpd	ymm6, ymm6, ymm7	;; R2 - R4 (new R4)			; 26-28

	vsubpd	ymm7, ymm1, ymm0	;; R1 - R5 (final R5)			; 27-29
	vmovapd	[srcreg+d2], ymm3	;; Save R3				; 27

	vaddpd	ymm1, ymm1, ymm0	;; R1 + R5 (final R1)			; 28-30

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm0, ymm2, ymm5	;; R2 - R6 (final R6)			; 29-31

	vaddpd	ymm2, ymm2, ymm5	;; R2 + R6 (final R2)			; 30-32
	vmovapd	[srcreg+32], ymm7	;; Save R5				; 30

	vsubpd	ymm7, ymm6, ymm4	;; R4 - R8 (final R8)			; 31-33
	vmovapd	[srcreg], ymm1		;; Save R1				; 31

	vaddpd	ymm6, ymm6, ymm4	;; R4 + R8 (final R4)			; 32-34
	vmovapd	[srcreg+d1+32], ymm0	;; Save R6				; 32

	vmovapd	[srcreg+d1], ymm2	;; Save R2				; 33
	vmovapd	[srcreg+d2+d1+32], ymm7	;; Save R8				; 34
	vmovapd	[srcreg+d2+d1], ymm6	;; Save R4				; 35

	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; Common swizzling eight reals unfft
yr4_s8r_unfft_cmn_preload MACRO
	ENDM
yr4_s8r_unfft_cmn MACRO srcreg,srcinc,d1,d2,screg1,screg2,screg5,scregA,scincA,scregB,scincB,maxrpt,L1pt,L1pd

	;; On input the 64-byte cache lines hold these data values:
	;;	0	8	16	24	4	12	20	28
	;;	1	...
	;;	2	...
	;;	3	...

	;; These are swizzled to:
	;;	0	1	2	3	4	5	6	7
	;;	8	...
	;;	16	...
	;;	24	...

	;; Swizzle using 8 shufpds and 8 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm1, [srcreg]		;; R1
	vmovapd	ymm7, [srcreg+d1]	;; R2
	vshufpd	ymm0, ymm1, ymm7, 15	;; Shuffle R1 and R2 to create 8 9 24 25		; 1
	vshufpd	ymm1, ymm1, ymm7, 0	;; Shuffle R1 and R2 to create 0 1 16 17		; 2

	vmovapd	ymm2, [srcreg+d2]	;; R3
	vmovapd	ymm7, [srcreg+d2+d1]	;; R4
	vshufpd	ymm3, ymm2, ymm7, 15	;; Shuffle R3 and R4 to create 10 11 26 27		; 3
	vshufpd	ymm2, ymm2, ymm7, 0	;; Shuffle R3 and R4 to create 2 3 18 19		; 4

	vperm2f128 ymm4, ymm0, ymm3, 32	;; Shuffle R1/R2 hi and R3/R4 hi (8 9 10 11)		; 5-6
	vperm2f128 ymm0, ymm0, ymm3, 49	;; Shuffle R1/R2 hi and R3/R4 hi (24 25 26 27)		; 6-7

	vperm2f128 ymm3, ymm1, ymm2, 32	;; Shuffle R1/R2 low and R3/R4 low (0 1 2 3)		; 7-8
	vperm2f128 ymm1, ymm1, ymm2, 49	;; Shuffle R1/R2 low and R3/R4 low (16 17 18 19)	; 8-9

	vmovapd	ymm6, [srcreg+32]	;; I1
	vmovapd	ymm2, [srcreg+d1+32]	;; I2
	vshufpd	ymm5, ymm6, ymm2, 15	;; Shuffle I1 and I2 to create 12 13 28 29
	vshufpd	ymm6, ymm6, ymm2, 0	;; Shuffle I1 and I2 to create 4 5 20 21

	vmovapd	[srcreg], ymm3		;; Temporarily save R1 (0 1 2 3)

	vmovapd	ymm2, [srcreg+d2+32]	;; I3
	vmovapd	ymm7, [srcreg+d2+d1+32]	;; I4
	vshufpd	ymm3, ymm2, ymm7, 15	;; Shuffle I3 and I4 to create 14 15 30 31
	vshufpd	ymm2, ymm2, ymm7, 0	;; Shuffle I3 and I4 to create 6 7 22 23

	vperm2f128 ymm7, ymm5, ymm3, 32	;; Shuffle I1/I2 hi and I3/I4 hi (12 13 14 15)
	vperm2f128 ymm5, ymm5, ymm3, 49	;; Shuffle I1/I2 hi and I3/I4 hi (28 29 30 31)

	vperm2f128 ymm3, ymm6, ymm2, 32	;; Shuffle I1/I2 low and I3/I4 low (4 5 6 7)
	vperm2f128 ymm6, ymm6, ymm2, 49	;; Shuffle I1/I2 low and I3/I4 low (20 21 22 23)

	vmovapd	[srcreg+32], ymm1	;; Temporarily save R3 (16 17 18 19)

	vmovapd	ymm1, [screg2+32]	;; cosine/sine for w^2n
	vmulpd	ymm2, ymm4, ymm1	;; A2 = R2 * cosine/sine
	vaddpd	ymm2, ymm2, ymm7	;; A2 = A2 + I2
	vmulpd	ymm7, ymm7, ymm1	;; B2 = I2 * cosine/sine
	vsubpd	ymm7, ymm7, ymm4	;; B2 = B2 - R2

	vmovapd	ymm1, [screg5+32]	;; cosine/sine for w^5n
	vmulpd	ymm4, ymm0, ymm1	;; A4 = R4 * cosine/sine
	vaddpd	ymm4, ymm4, ymm5	;; A4 = A4 + I4
	vmulpd	ymm5, ymm5, ymm1	;; B4 = I4 * cosine/sine
	vsubpd	ymm5, ymm5, ymm0	;; B4 = B4 - R4

	vmovapd	ymm1, [screg1+32] 	;; cosine/sine for w^n
	vmulpd	ymm0, ymm1, [srcreg+32]	;; A3 = R3 * cosine/sine for w^n
	vaddpd	ymm0, ymm0, ymm6	;; A3 = A3 + I3
	vmulpd	ymm6, ymm6, ymm1	;; B3 = I3 * cosine/sine for w^n
	vsubpd	ymm6, ymm6, [srcreg+32]	;; B3 = B3 - R3

	L1prefetchw srcreg+L1pd, L1pt

	vmovapd	ymm1, [screg2]		;; sine for w^2n
	vmulpd	ymm2, ymm2, ymm1	;; new R3 = A2 * sine
	vmulpd	ymm7, ymm7, ymm1	;; new R4 = B2 * sine
	vmovapd	ymm1, [screg5]		;; sine for w^5n
	vmulpd	ymm4, ymm4, ymm1	;; new R7 = A4 * sine
	vmulpd	ymm5, ymm5, ymm1	;; new R8 = B4 * sine
	vmovapd	ymm1, [screg1]		;; sine for w^n
	vmulpd	ymm0, ymm0, ymm1	;; new R5 = A3 * sine
	vmulpd	ymm6, ymm6, ymm1	;; new R6 = B3 * sine

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm1, ymm0, ymm4	;; new R6 = R5 - R7
	vaddpd	ymm0, ymm0, ymm4	;; new R5 = R5 + R7

	vsubpd	ymm4, ymm6, ymm5	;; new R8 = R6 - R8
	vaddpd	ymm6, ymm6, ymm5	;; new R7 = R6 + R8

	L1prefetchw srcreg+d2+L1pd, L1pt

	vaddpd	ymm5, ymm1, ymm4	;; R6 = R6 + R8
	vsubpd	ymm4, ymm4, ymm1	;; R8 = R8 - R6

	vmulpd	ymm5, ymm5, YMM_SQRTHALF ;; R6 = R6 * square root of 1/2
	vmulpd	ymm4, ymm4, YMM_SQRTHALF ;; R8 = R8 * square root of 1/2

	vsubpd	ymm1, ymm3, ymm7	;; R2 - R4 (new R4)
	vaddpd	ymm3, ymm3, ymm7	;; R2 + R4 (new R2)

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm7, ymm1, ymm4	;; R4 - R8 (final R8)
	vaddpd	ymm1, ymm1, ymm4	;; R4 + R8 (final R4)

	vsubpd	ymm4, ymm3, ymm5	;; R2 - R6 (final R6)
	vaddpd	ymm3, ymm3, ymm5	;; R2 + R6 (final R2)

	vmovapd ymm5, [srcreg]		;; Reload R1

	vmovapd	[srcreg+d2+d1+32], ymm7	;; Save R8
	vmovapd	[srcreg+d2+d1], ymm1	;; Save R4

	vsubpd	ymm7, ymm5, ymm2	;; R1 - R3 (new R3)
	vaddpd	ymm5, ymm5, ymm2	;; R1 + R3 (new R1)

	vsubpd	ymm2, ymm7, ymm6	;; R3 - R7 (final R7)
	vaddpd	ymm7, ymm7, ymm6	;; R3 + R7 (final R3)

	vsubpd	ymm6, ymm5, ymm0	;; R1 - R5 (final R5)
	vaddpd	ymm5, ymm5, ymm0	;; R1 + R5 (final R1)

	vmovapd	[srcreg+d1+32], ymm4	;; Save R6
	vmovapd	[srcreg+d1], ymm3	;; Save R2

	vmovapd	[srcreg+d2+32], ymm2	;; Save R7
	vmovapd	[srcreg+d2], ymm7	;; Save R3

	vmovapd	[srcreg+32], ymm6	;; Save R5
	vmovapd	[srcreg], ymm5		;; Save R1

	bump	srcreg, srcinc
	bump	scregA, scincA
	bump	scregB, scincB
	ENDM

;; 64-bit version

IFDEF X86_64

yr4_8r_unfft_cmn_preload MACRO
	vmovapd	ymm15, YMM_SQRTHALF
	ENDM
yr4_8r_unfft_cmn MACRO srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	yr4_8r_unfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 3,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 4,exec,exec,noexec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 5,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	bump	srcreg, 5*srcinc
	bump	screg, 5*scinc
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	yr4_8r_unfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 3,exec,exec,noexec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 4,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	screg, 4*scinc
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	yr4_8r_unfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 2,exec,exec,noexec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 3,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	screg, 3*scinc
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	yr4_8r_unfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 1,exec,exec,noexec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 2,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	screg, 2*scinc
	ELSE
	yr4_8r_unfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 0,noexec,exec,noexec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	yr4_8r_unfft_unroll 1,exec,noexec,noexec,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDIF
	ENDM

yr4_8r_unfft_unroll MACRO iter,prev,this,next,srcreg,srcinc,d1,d2,screg1,screg2,screg5,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<ymm0>
y1	TEXTEQU	<ymm1>
y2	TEXTEQU	<ymm2>
y3	TEXTEQU	<ymm3>
y4	TEXTEQU	<ymm4>
y5	TEXTEQU	<ymm5>
y6	TEXTEQU	<ymm6>
y7	TEXTEQU	<ymm7>
y8	TEXTEQU	<ymm8>
y9	TEXTEQU	<ymm9>
y10	TEXTEQU	<ymm10>
y11	TEXTEQU	<ymm11>
y12	TEXTEQU	<ymm12>
y13	TEXTEQU	<ymm13>
y14	TEXTEQU	<ymm14>
	ENDIF

;; On later calls, previous R2,R4,R6,R8 will be in y0-3.  This A4,A3,A2,B4,B3,I4,I3,R4,R3,R2,c/s will be in y4-14.
;; ymm15 has YMM_SQRTHALF.

this	vaddpd	y4, y4, y9			;; A4 = A4 + I4				; 1-3
this	vmovapd	y9, [srcreg+iter*srcinc+d1+32]	;; I2

this	vaddpd	y5, y5, y10			;; A3 = A3 + I3				; 2-4
this	vmulpd	y14, y9, y14			;; B2 = I2 * cosine/sine w^2		; 1-5
this	vmovapd	y10, [screg5+iter*scinc]	;; sine for w^5			

this	vsubpd	y7, y7, y11			;; B4 = B4 - R4				; 3-5
this	vmovapd	y11, [screg1+iter*scinc]	;; sine for w^1

this	vsubpd	y8, y8, y12			;; B3 = B3 - R3				; 4-6
this	vmulpd	y4, y4, y10			;; new R7 = A4 * sine w^5		; 4-8
this	vmovapd	y12, [screg2+iter*scinc]	;; sine for w^2

this	vaddpd	y6, y6, y9			;; A2 = A2 + I2				; 5-7
this	vmulpd	y5, y5, y11			;; new R5 = A3 * sine w^1		; 5-9
this	vmovapd	y9, [srcreg+iter*srcinc]	;; R1

this	vsubpd	y14, y14, y13			;; B2 = B2 - R2				; 6-8
this	vmulpd	y7, y7, y10			;; new R8 = B4 * sine w^5		; 6-10
this	vmovapd	y10, [srcreg+iter*srcinc+32]	;; I1 (a.k.a R2)

prev	vaddpd	y0, y0, y2			;; R2 + R6 (final R2)			; 7-9
this	vmulpd	y8, y8, y11			;; new R6 = B3 * sine w^1		; 7-11
next	vmovapd	y11, [screg5+(iter+1)*scinc+32]	;; cosine/sine for w^5

prev	vsubpd	y2, y1, y3			;; R4 - R8 (final R8)			; 8-10
this	vmulpd	y6, y6, y12			;; new R3 = A2 * sine w^2		; 8-12
next	vmovapd	y13, [srcreg+(iter+1)*srcinc+d2+d1] ;; R4

prev	vaddpd	y1, y1, y3			;; R4 + R8 (final R4)			; 9-11
this	vmulpd	y14, y14, y12			;; new R4 = B2 * sine w^2		; 9-13
next	vmovapd	y12, [screg1+(iter+1)*scinc+32]	;; cosine/sine for w^1

this	vsubpd	y3, y5, y4			;; new R6 = R5 - R7			; 10-12
prev	vmovapd	[srcreg+(iter-1)*srcinc+d1], y0 ;; Save R2				; 10
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

this	vaddpd	y5, y5, y4			;; new R5 = R5 + R7			; 11-13
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+d1+32], y2 ;; Save R8			; 11

this	vsubpd	y2, y8, y7			;; new R8 = R6 - R8			; 12-14
prev	vmovapd	[srcreg+(iter-1)*srcinc+d2+d1], y1 ;; Save R4				; 12

this	vaddpd	y8, y8, y7			;; new R7 = R6 + R8			; 13-15
next	vmovapd	y4, [srcreg+(iter+1)*srcinc+d2+d1+32] ;; I4
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	vsubpd	y1, y9, y6			;; R1 - R3 (new R3)			; 14-16
next	vmovapd	y7, [srcreg+(iter+1)*srcinc+d2]	;; R3

this	vaddpd	y0, y3, y2			;; R6 = R6 + R8				; 15-17
this next yloop_unrolled_one

this	vsubpd	y2, y2, y3			;; R8 = R8 - R6				; 16-18
next	vmovapd	y3, [screg2+(iter+1)*scinc+32]	;; cosine/sine for w^2

this	vaddpd	y9, y9, y6			;; R1 + R3 (new R1)			; 17-19
this	L1prefetchw srcreg+iter*srcinc+d2+L1pd, L1pt

this	vsubpd	y6, y1, y8			;; R3 - R7 (final R7)			; 18-20
this	vmulpd	y0, y0, ymm15			;; R6 = R6 * square root of 1/2		; 18-22

this	vaddpd	y1, y1, y8			;; R3 + R7 (final R3)			; 19-21
this	vmulpd	y2, y2, ymm15			;; R8 = R8 * square root of 1/2		; 19-23
this	L1prefetchw srcreg+iter*srcinc+d2+d1+L1pd, L1pt

this	vaddpd	y8, y10, y14			;; R2 + R4 (new R2)			; 20-22
this	vmovapd	[srcreg+iter*srcinc+d2+32], y6	;; Save R7				; 21
next	vmulpd	y6, y13, y11			;; A4 = R4 * cosine/sine w^5		; 20-24

this	vsubpd	y10, y10, y14			;; R2 - R4 (new R4)			; 21-23
next	vmulpd	y14, y7, y12			;; A3 = R3 * cosine/sine w^1		; 21-25
this	vmovapd	[srcreg+iter*srcinc+d2], y1	;; Save R3				; 22

this	vsubpd	y1, y9, y5			;; R1 - R5 (final R5)			; 22-24
next	vmulpd	y11, y4, y11			;; B4 = I4 * cosine/sine w^5		; 22-26
this	vmovapd	[srcreg+iter*srcinc+32], y1	;; Save R5				; 25
next	vmovapd	y1, [srcreg+(iter+1)*srcinc+d2+32] ;; I3

this	vaddpd	y9, y9, y5			;; R1 + R5 (final R1)			; 23-25
next	vmulpd	y12, y1, y12			;; B3 = I3 * cosine/sine w^1		; 23-27
next	vmovapd	y5, [srcreg+(iter+1)*srcinc+d1] ;; R2
this	vmovapd	[srcreg+iter*srcinc], y9	;; Save R1				; 26

this	vsubpd	y9, y8, y0			;; R2 - R6 (final R6)			; 24-26
this	vmovapd	[srcreg+iter*srcinc+d1+32], y9	;; Save R6				; 27
next	vmulpd	y9, y5, y3			;; A2 = R2 * cosine/sine w^2		; 24-28

;; Shuffle register assignments so that next call has R2,R4,R6,R8 in y0-3 and next A4,A3,A2,B4,B3,I4,I3,R4,R3,R2,c/s in y4-14.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y8
y8	TEXTEQU	y12
y12	TEXTEQU	y7
y7	TEXTEQU	y11
y11	TEXTEQU	y13
y13	TEXTEQU	y5
y5	TEXTEQU	y14
y14	TEXTEQU	y3
y3	TEXTEQU	y2
y2	TEXTEQU	ytmp
ytmp	TEXTEQU y1
y1	TEXTEQU	y10
y10	TEXTEQU	ytmp
ytmp	TEXTEQU y4
y4	TEXTEQU	y6
y6	TEXTEQU	y9
y9	TEXTEQU ytmp
	ENDM

ENDIF


;;
;; ******************************* eight-real-with-partial-normalization variants *************************************
;;
;; These macros are used in pass 1 of yr4dwpn two pass FFTs.  They are like the standard eight-real
;; FFT macros except that a normalization multiplier has been pre-applied to the sine multiplier.
;; Consequently, the forward FFT and inverse FFT use different sine multipliers.
;; Also, a normalization multiplier must be applied to the final R1/I1 value.
;;

;; Uses one combined sin/cos data ptr.  The first two sin/cos entries can be used by four complex
;; macros at the same FFT level.  The third and fourth sin/cos entries are used by this eight reals macro.
old_yr4_b4cl_csc_wpn_eight_reals_fft_preload MACRO
	ENDM
old_yr4_b4cl_csc_wpn_eight_reals_fft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	;; screg is normalization weights for R1/I1
	;; screg+32 is weighted sin/cos values for R2/I2 (w^2n)
	;; screg+32+2*96 = screg+224 is weighted sin/cos values for R3/I3 (w^n)
	;; screg+32+3*96 = screg+320 is weighted sin/cos values for R4/I4 (w^5n)

	vmovapd	ymm0, [srcreg+d2+d1]	;; R4
	vmovapd	ymm7, [srcreg+d2+d1+32]	;; R8
	vsubpd	ymm3, ymm0, ymm7	;; new R8 = R4 - R8			; 1-3
	vaddpd	ymm7, ymm0, ymm7	;; new R4 = R4 + R8			; 2-4

	vmovapd	ymm4, [srcreg+d1]	;; R2
	vmovapd	ymm5, [srcreg+d1+32]	;; R6
	vsubpd	ymm1, ymm4, ymm5	;; new R6 = R2 - R6			; 3-5
	vaddpd	ymm5, ymm4, ymm5	;; new R2 = R2 + R6			; 4-6

	vmulpd	ymm3, ymm3, YMM_SQRTHALF ;; R8 = R8 * square root		;  4-8
	vmulpd	ymm1, ymm1, YMM_SQRTHALF ;; R6 = R6 * square root		;  6-10

	vmovapd	ymm2, [srcreg]		;; R1
	vmovapd	ymm4, [srcreg+32]	;; R5
	vsubpd	ymm0, ymm2, ymm4	;; new R5 = R1 - R5			; 5-7
	vaddpd	ymm4, ymm2, ymm4	;; new R1 = R1 + R5			; 6-8

	vaddpd	ymm2, ymm5, ymm7	;; R2 + R4 (final I1, a.k.a 2nd real result) ; 7-9
	vsubpd	ymm5, ymm5, ymm7	;; R2 - R4 (final I2)			; 8-10

	vmovapd	ymm6, [srcreg+d2]	;; R3
	vbroadcastsd ymm7, Q [screg]	;; normalization
	vmulpd	ymm2, ymm2, ymm7	;; I1 * normalization			;  10-14
	vmovapd	ymm7, [srcreg+d2+32]	;; R7
	vmovapd	[srcreg+32], ymm2	;; Save final I1			;  15

	vaddpd	ymm2, ymm6, ymm7	;; new R3 = R3 + R7			; 9-11
	vsubpd	ymm6, ymm6, ymm7	;; new R7 = R3 - R7			; 10-12

	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	ymm7, ymm1, ymm3	;; R6 = R6 - R8 (Real part)		; 11-13
	vaddpd	ymm3, ymm1, ymm3	;; R8 = R6 + R8 (Imaginary part)	; 12-14

	vaddpd	ymm1, ymm4, ymm2	;; R1 + R3 (final R1)			; 13-15
	vsubpd	ymm4, ymm4, ymm2	;; R1 - R3 (final R2)			; 14-16

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm2, ymm0, ymm7	;; R5 - R6 (final R4)			; 15-17
	vaddpd	ymm0, ymm0, ymm7	;; R5 + R6 (final R3)			; 16-18

	vsubpd	ymm7, ymm6, ymm3	;; R7 - R8 (final I4)			; 17-19
	vaddpd	ymm6, ymm6, ymm3	;; R7 + R8 (final I3)			; 18-20

	vmulpd	ymm3, ymm4, [(screg+32)+32] ;; A2 = R2 * cosine/sine for w^2n	;  17-21
	vsubpd	ymm3, ymm3, ymm5	;; A2 = A2 - I2				; 22-24
	vmulpd	ymm5, ymm5, [(screg+32)+32] ;; B2 = I2 * cosine/sine for w^2n	;  18-22
	vaddpd	ymm5, ymm5, ymm4	;; B2 = B2 + R2				; 23-25

	L1prefetchw srcreg+d2+L1pd, L1pt

	vmulpd	ymm4, ymm2, [(screg+320)+32] ;; A4 = R4 * cosine/sine for w^5n	;  19-23
	vsubpd	ymm4, ymm4, ymm7	;; A4 = A4 - I4				; 24-26
	vmulpd	ymm7, ymm7, [(screg+320)+32] ;; B4 = I4 * cosine/sine for w^5n	;  20-24
	vaddpd	ymm7, ymm7, ymm2	;; B4 = B4 + R4				; 25-27

	vmulpd	ymm2, ymm0, [(screg+224)+32] ;; A3 = R3 * cosine/sine for w^n	;  21-25
	vsubpd	ymm2, ymm2, ymm6	;; A3 = A3 - I3				; 26-28
	vmulpd	ymm6, ymm6, [(screg+224)+32] ;; B3 = I3 * cosine/sine for w^n	;  22-26
	vaddpd	ymm6, ymm6, ymm0	;; B3 = B3 + R3				; 27-29

	vbroadcastsd ymm0, Q [screg]	;; normalization
	vmulpd	ymm1, ymm1, ymm0	;; R1 * normalization			;  23-27

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vmovapd	ymm0, [(screg+32)]	;; sine * normalization for w^2n
	vmulpd	ymm3, ymm3, ymm0	;; A2 = A2 * sine (final R2)		;  25-29
	vmulpd	ymm5, ymm5, ymm0	;; B2 = B2 * sine (final I2)		;  26-30
	vmovapd	ymm0, [(screg+320)]	;; sine * normalization for w^5n
	vmulpd	ymm4, ymm4, ymm0	;; A4 = A4 * sine (final R4)		;  27-31
	vmulpd	ymm7, ymm7, ymm0	;; B4 = B4 * sine (final I4)		;  28-32
	vmovapd	ymm0, [(screg+224)]	;; sine * normalization for w^n
	vmulpd	ymm2, ymm2, ymm0	;; A3 = A3 * sine (final R3)		;  29-33
	vmulpd	ymm6, ymm6, ymm0	;; B3 = B3 * sine (final I3)		;  30-34

	vmovapd	[srcreg], ymm1		;; Save R1
	;;vmovapd [srcreg+32], ymm0	;; Save I1
	vmovapd	[srcreg+d1], ymm3	;; Save R2
	vmovapd	[srcreg+d1+32], ymm5	;; Save I2
	vmovapd	[srcreg+d2], ymm2	;; Save R3
	vmovapd	[srcreg+d2+32], ymm6	;; Save I3
	vmovapd	[srcreg+d2+d1], ymm4	;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm7	;; Save I4
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM


;; Uses one combined sin/cos data ptr.  The first two sin/cos entries can be used by four complex
;; macros at the same FFT level.  The third and fourth sin/cos entries are used by this eight reals macro.
old_yr4_b4cl_csc_wpn_eight_reals_unfft_preload MACRO
	ENDM
old_yr4_b4cl_csc_wpn_eight_reals_unfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd
	;; screg is normalization weights for R1/I1
	;; screg+32 is weighted sin/cos values for R2/I2 (w^2n)
	;; screg+32+2*96 = screg+224 is weighted sin/cos values for R3/I3 (w^n)
	;; screg+32+3*96 = screg+320 is weighted sin/cos values for R4/I4 (w^5n)

	vmovapd	ymm1, [(screg+32)+32]	;; cosine/sine for w^2n
	vmovapd	ymm4, [srcreg+d1]	;; R2
	vmulpd	ymm2, ymm4, ymm1	;; A2 = R2 * cosine/sine
	vmovapd	ymm7, [srcreg+d1+32]	;; I2
	vaddpd	ymm2, ymm2, ymm7	;; A2 = A2 + I2
	vmulpd	ymm7, ymm7, ymm1	;; B2 = I2 * cosine/sine
	vsubpd	ymm7, ymm7, ymm4	;; B2 = B2 - R2

	vmovapd	ymm1, [(screg+320)+32]	;; cosine/sine for w^5n
	vmovapd	ymm0, [srcreg+d2+d1]	;; R4
	vmulpd	ymm4, ymm0, ymm1	;; A4 = R4 * cosine/sine
	vmovapd	ymm5, [srcreg+d2+d1+32]	;; I4
	vaddpd	ymm4, ymm4, ymm5	;; A4 = A4 + I4
	vmulpd	ymm5, ymm5, ymm1	;; B4 = I4 * cosine/sine
	vsubpd	ymm5, ymm5, ymm0	;; B4 = B4 - R4

	vmovapd	ymm1, [(screg+224)+32] 	;; cosine/sine for w^n
	vmovapd	ymm3, [srcreg+d2]	;; R3
	vmulpd	ymm0, ymm1, ymm3	;; A3 = R3 * cosine/sine for w^n
	vmovapd	ymm6, [srcreg+d2+32]	;; I3
	vaddpd	ymm0, ymm0, ymm6	;; A3 = A3 + I3
	vmulpd	ymm6, ymm6, ymm1	;; B3 = I3 * cosine/sine for w^n
	vsubpd	ymm6, ymm6, ymm3	;; B3 = B3 - R3

	L1prefetchw srcreg+L1pd, L1pt

	vmovapd	ymm1, [(screg+32)+64]	;; sine * normalization_inverse for w^2n
	vmulpd	ymm2, ymm2, ymm1	;; new R3 = A2 * sine
	vmulpd	ymm7, ymm7, ymm1	;; new R4 = B2 * sine
	vmovapd	ymm1, [(screg+320)+64]	;; sine * normalization_inverse for w^5n
	vmulpd	ymm4, ymm4, ymm1	;; new R7 = A4 * sine
	vmulpd	ymm5, ymm5, ymm1	;; new R8 = B4 * sine
	vmovapd	ymm1, [(screg+224)+64]	;; sine * normalization_inverse for w^n
	vmulpd	ymm0, ymm0, ymm1	;; new R5 = A3 * sine
	vmulpd	ymm6, ymm6, ymm1	;; new R6 = B3 * sine

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm1, ymm0, ymm4	;; new R6 = R5 - R7
	vaddpd	ymm0, ymm0, ymm4	;; new R5 = R5 + R7

	vsubpd	ymm4, ymm6, ymm5	;; new R8 = R6 - R8
	vaddpd	ymm6, ymm6, ymm5	;; new R7 = R6 + R8

	vbroadcastsd ymm3, Q [screg+8]	;; normalization_inverse
	vmulpd	ymm3, ymm3, [srcreg+32]	;; R2 * normalization_inverse

	vaddpd	ymm5, ymm1, ymm4	;; R6 = R6 + R8
	vsubpd	ymm4, ymm4, ymm1	;; R8 = R8 - R6

	L1prefetchw srcreg+d2+L1pd, L1pt

	vmulpd	ymm5, ymm5, YMM_SQRTHALF ;; R6 = R6 * square root of 1/2
	vmulpd	ymm4, ymm4, YMM_SQRTHALF ;; R8 = R8 * square root of 1/2

	vsubpd	ymm1, ymm3, ymm7	;; R2 - R4 (new R4)
	vaddpd	ymm3, ymm3, ymm7	;; R2 + R4 (new R2)

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm7, ymm1, ymm4	;; R4 - R8 (final R8)
	vaddpd	ymm1, ymm1, ymm4	;; R4 + R8 (final R4)

	vsubpd	ymm4, ymm3, ymm5	;; R2 - R6 (final R6)
	vaddpd	ymm3, ymm3, ymm5	;; R2 + R6 (final R2)

	vbroadcastsd ymm5, Q [screg+8]	;; normalization_inverse
	vmulpd	ymm5, ymm5, [srcreg]	;; R1 * normalization_inverse

	vmovapd	[srcreg+d2+d1+32], ymm7	;; Save R8
	vmovapd	[srcreg+d2+d1], ymm1	;; Save R4

	vsubpd	ymm7, ymm5, ymm2	;; R1 - R3 (new R3)
	vaddpd	ymm5, ymm5, ymm2	;; R1 + R3 (new R1)

	vsubpd	ymm2, ymm7, ymm6	;; R3 - R7 (final R7)
	vaddpd	ymm7, ymm7, ymm6	;; R3 + R7 (final R3)

	vsubpd	ymm6, ymm5, ymm0	;; R1 - R5 (final R5)
	vaddpd	ymm5, ymm5, ymm0	;; R1 + R5 (final R1)

	vmovapd	[srcreg+d1+32], ymm4	;; Save R6
	vmovapd	[srcreg+d1], ymm3	;; Save R2

	vmovapd	[srcreg+d2+32], ymm2	;; Save R7
	vmovapd	[srcreg+d2], ymm7	;; Save R3

	vmovapd	[srcreg+32], ymm6	;; Save R5
	vmovapd	[srcreg], ymm5		;; Save R1

	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM


;; Uses one combined sin/cos data ptr.  The first two sin/cos entries can be used by four complex
;; macros at the same FFT level.  The third and fourth sin/cos entries are used by this eight reals macro.
yr4_b4cl_csc_wpn_eight_reals_fft_preload MACRO
	ENDM
yr4_b4cl_csc_wpn_eight_reals_fft MACRO srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	;; screg1 is normalization weights for R1/I1
	;; screg2+0, screg2+64+0, screg2+128+0 is weighted sin/cos values for R2/I2 (w^2n)
	;; screg2+192+0, screg2+256+0, screg2+320+0 is weighted sin/cos values for R3/I3 (w^n)
	;; screg2+192+32, screg2+256+32, screg2+320+32 is weighted sin/cos values for R4/I4 (w^5n)

	vmovapd	ymm0, [srcreg+d2+d1]	;; R4
	vmovapd	ymm7, [srcreg+d2+d1+32]	;; R8
	vsubpd	ymm3, ymm0, ymm7	;; new R8 = R4 - R8			; 1-3
	vaddpd	ymm7, ymm0, ymm7	;; new R4 = R4 + R8			; 2-4

	vmovapd	ymm4, [srcreg+d1]	;; R2
	vmovapd	ymm5, [srcreg+d1+32]	;; R6
	vsubpd	ymm1, ymm4, ymm5	;; new R6 = R2 - R6			; 3-5
	vaddpd	ymm5, ymm4, ymm5	;; new R2 = R2 + R6			; 4-6

	vmulpd	ymm3, ymm3, YMM_SQRTHALF ;; R8 = R8 * square root		;  4-8
	vmulpd	ymm1, ymm1, YMM_SQRTHALF ;; R6 = R6 * square root		;  6-10

	vmovapd	ymm2, [srcreg]		;; R1
	vmovapd	ymm4, [srcreg+32]	;; R5
	vsubpd	ymm0, ymm2, ymm4	;; new R5 = R1 - R5			; 5-7
	vaddpd	ymm4, ymm2, ymm4	;; new R1 = R1 + R5			; 6-8

	vaddpd	ymm2, ymm5, ymm7	;; R2 + R4 (final I1, a.k.a 2nd real result) ; 7-9
	vsubpd	ymm5, ymm5, ymm7	;; R2 - R4 (final I2)			; 8-10

	vmovapd	ymm6, [srcreg+d2]	;; R3
	vbroadcastsd ymm7, Q [screg1]	;; normalization
	vmulpd	ymm2, ymm2, ymm7	;; I1 * normalization			;  10-14
	vmovapd	ymm7, [srcreg+d2+32]	;; R7
	vmovapd	[srcreg+32], ymm2	;; Save final I1			;  15

	vaddpd	ymm2, ymm6, ymm7	;; new R3 = R3 + R7			; 9-11
	vsubpd	ymm6, ymm6, ymm7	;; new R7 = R3 - R7			; 10-12

	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	ymm7, ymm1, ymm3	;; R6 = R6 - R8 (Real part)		; 11-13
	vaddpd	ymm3, ymm1, ymm3	;; R8 = R6 + R8 (Imaginary part)	; 12-14

	vaddpd	ymm1, ymm4, ymm2	;; R1 + R3 (final R1)			; 13-15
	vsubpd	ymm4, ymm4, ymm2	;; R1 - R3 (final R2)			; 14-16

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm2, ymm0, ymm7	;; R5 - R6 (final R4)			; 15-17
	vaddpd	ymm0, ymm0, ymm7	;; R5 + R6 (final R3)			; 16-18

	vsubpd	ymm7, ymm6, ymm3	;; R7 - R8 (final I4)			; 17-19
	vaddpd	ymm6, ymm6, ymm3	;; R7 + R8 (final I3)			; 18-20

	vmulpd	ymm3, ymm4, [screg2+0]	;; A2 = R2 * cosine/sine for w^2n	;  17-21
	vsubpd	ymm3, ymm3, ymm5	;; A2 = A2 - I2				; 22-24
	vmulpd	ymm5, ymm5, [screg2+0]	;; B2 = I2 * cosine/sine for w^2n	;  18-22
	vaddpd	ymm5, ymm5, ymm4	;; B2 = B2 + R2				; 23-25

	L1prefetchw srcreg+d2+L1pd, L1pt

	vmulpd	ymm4, ymm2, [screg2+192+32] ;; A4 = R4 * cosine/sine for w^5n	;  19-23
	vsubpd	ymm4, ymm4, ymm7	;; A4 = A4 - I4				; 24-26
	vmulpd	ymm7, ymm7, [screg2+192+32] ;; B4 = I4 * cosine/sine for w^5n	;  20-24
	vaddpd	ymm7, ymm7, ymm2	;; B4 = B4 + R4				; 25-27

	vmulpd	ymm2, ymm0, [screg2+192+0] ;; A3 = R3 * cosine/sine for w^n	;  21-25
	vsubpd	ymm2, ymm2, ymm6	;; A3 = A3 - I3				; 26-28
	vmulpd	ymm6, ymm6, [screg2+192+0] ;; B3 = I3 * cosine/sine for w^n	;  22-26
	vaddpd	ymm6, ymm6, ymm0	;; B3 = B3 + R3				; 27-29

	vbroadcastsd ymm0, Q [screg1]	;; normalization
	vmulpd	ymm1, ymm1, ymm0	;; R1 * normalization			;  23-27

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vmovapd	ymm0, [screg2+64+0]	;; sine * normalization for w^2n
	vmulpd	ymm3, ymm3, ymm0	;; A2 = A2 * sine (final R2)		;  25-29
	vmulpd	ymm5, ymm5, ymm0	;; B2 = B2 * sine (final I2)		;  26-30
	vmovapd	ymm0, [screg2+256+32]	;; sine * normalization for w^5n
	vmulpd	ymm4, ymm4, ymm0	;; A4 = A4 * sine (final R4)		;  27-31
	vmulpd	ymm7, ymm7, ymm0	;; B4 = B4 * sine (final I4)		;  28-32
	vmovapd	ymm0, [screg2+256+0]	;; sine * normalization for w^n
	vmulpd	ymm2, ymm2, ymm0	;; A3 = A3 * sine (final R3)		;  29-33
	vmulpd	ymm6, ymm6, ymm0	;; B3 = B3 * sine (final I3)		;  30-34

	vmovapd	[srcreg], ymm1		;; Save R1
	;;vmovapd [srcreg+32], ymm0	;; Save I1
	vmovapd	[srcreg+d1], ymm3	;; Save R2
	vmovapd	[srcreg+d1+32], ymm5	;; Save I2
	vmovapd	[srcreg+d2], ymm2	;; Save R3
	vmovapd	[srcreg+d2+32], ymm6	;; Save I3
	vmovapd	[srcreg+d2+d1], ymm4	;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm7	;; Save I4
	bump	srcreg, srcinc
	bump	screg1, scinc1
	bump	screg2, scinc2
	ENDM


;; Uses one combined sin/cos data ptr.  The first two sin/cos entries can be used by four complex
;; macros at the same FFT level.  The third and fourth sin/cos entries are used by this eight reals macro.
yr4_b4cl_csc_wpn_eight_reals_unfft_preload MACRO
	ENDM
yr4_b4cl_csc_wpn_eight_reals_unfft MACRO srcreg,srcinc,d1,d2,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	;; screg1 is normalization weights for R1/I1
	;; screg2+0, screg2+64+0, screg2+128+0 is weighted sin/cos values for R2/I2 (w^2n)
	;; screg2+192+0, screg2+256+0, screg2+320+0 is weighted sin/cos values for R3/I3 (w^n)
	;; screg2+192+32, screg2+256+32, screg2+320+32 is weighted sin/cos values for R4/I4 (w^5n)

	vmovapd	ymm1, [screg2+0]	;; cosine/sine for w^2n
	vmovapd	ymm4, [srcreg+d1]	;; R2
	vmulpd	ymm2, ymm4, ymm1	;; A2 = R2 * cosine/sine
	vmovapd	ymm7, [srcreg+d1+32]	;; I2
	vaddpd	ymm2, ymm2, ymm7	;; A2 = A2 + I2
	vmulpd	ymm7, ymm7, ymm1	;; B2 = I2 * cosine/sine
	vsubpd	ymm7, ymm7, ymm4	;; B2 = B2 - R2

	vmovapd	ymm1, [screg2+192+32]	;; cosine/sine for w^5n
	vmovapd	ymm0, [srcreg+d2+d1]	;; R4
	vmulpd	ymm4, ymm0, ymm1	;; A4 = R4 * cosine/sine
	vmovapd	ymm5, [srcreg+d2+d1+32]	;; I4
	vaddpd	ymm4, ymm4, ymm5	;; A4 = A4 + I4
	vmulpd	ymm5, ymm5, ymm1	;; B4 = I4 * cosine/sine
	vsubpd	ymm5, ymm5, ymm0	;; B4 = B4 - R4

	vmovapd	ymm1, [screg2+192+0] 	;; cosine/sine for w^n
	vmovapd	ymm3, [srcreg+d2]	;; R3
	vmulpd	ymm0, ymm1, ymm3	;; A3 = R3 * cosine/sine for w^n
	vmovapd	ymm6, [srcreg+d2+32]	;; I3
	vaddpd	ymm0, ymm0, ymm6	;; A3 = A3 + I3
	vmulpd	ymm6, ymm6, ymm1	;; B3 = I3 * cosine/sine for w^n
	vsubpd	ymm6, ymm6, ymm3	;; B3 = B3 - R3

	L1prefetchw srcreg+L1pd, L1pt

	vmovapd	ymm1, [screg2+128+0]	;; sine * normalization_inverse for w^2n
	vmulpd	ymm2, ymm2, ymm1	;; new R3 = A2 * sine
	vmulpd	ymm7, ymm7, ymm1	;; new R4 = B2 * sine
	vmovapd	ymm1, [screg2+320+32]	;; sine * normalization_inverse for w^5n
	vmulpd	ymm4, ymm4, ymm1	;; new R7 = A4 * sine
	vmulpd	ymm5, ymm5, ymm1	;; new R8 = B4 * sine
	vmovapd	ymm1, [screg2+320+0]	;; sine * normalization_inverse for w^n
	vmulpd	ymm0, ymm0, ymm1	;; new R5 = A3 * sine
	vmulpd	ymm6, ymm6, ymm1	;; new R6 = B3 * sine

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	ymm1, ymm0, ymm4	;; new R6 = R5 - R7
	vaddpd	ymm0, ymm0, ymm4	;; new R5 = R5 + R7

	vsubpd	ymm4, ymm6, ymm5	;; new R8 = R6 - R8
	vaddpd	ymm6, ymm6, ymm5	;; new R7 = R6 + R8

	vbroadcastsd ymm3, Q [screg1]	;; normalization_inverse
	vmulpd	ymm3, ymm3, [srcreg+32]	;; R2 * normalization_inverse

	vaddpd	ymm5, ymm1, ymm4	;; R6 = R6 + R8
	vsubpd	ymm4, ymm4, ymm1	;; R8 = R8 - R6

	L1prefetchw srcreg+d2+L1pd, L1pt

	vmulpd	ymm5, ymm5, YMM_SQRTHALF ;; R6 = R6 * square root of 1/2
	vmulpd	ymm4, ymm4, YMM_SQRTHALF ;; R8 = R8 * square root of 1/2

	vsubpd	ymm1, ymm3, ymm7	;; R2 - R4 (new R4)
	vaddpd	ymm3, ymm3, ymm7	;; R2 + R4 (new R2)

	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm7, ymm1, ymm4	;; R4 - R8 (final R8)
	vaddpd	ymm1, ymm1, ymm4	;; R4 + R8 (final R4)

	vsubpd	ymm4, ymm3, ymm5	;; R2 - R6 (final R6)
	vaddpd	ymm3, ymm3, ymm5	;; R2 + R6 (final R2)

	vbroadcastsd ymm5, Q [screg1]	;; normalization_inverse
	vmulpd	ymm5, ymm5, [srcreg]	;; R1 * normalization_inverse

	vmovapd	[srcreg+d2+d1+32], ymm7	;; Save R8
	vmovapd	[srcreg+d2+d1], ymm1	;; Save R4

	vsubpd	ymm7, ymm5, ymm2	;; R1 - R3 (new R3)
	vaddpd	ymm5, ymm5, ymm2	;; R1 + R3 (new R1)

	vsubpd	ymm2, ymm7, ymm6	;; R3 - R7 (final R7)
	vaddpd	ymm7, ymm7, ymm6	;; R3 + R7 (final R3)

	vsubpd	ymm6, ymm5, ymm0	;; R1 - R5 (final R5)
	vaddpd	ymm5, ymm5, ymm0	;; R1 + R5 (final R1)

	vmovapd	[srcreg+d1+32], ymm4	;; Save R6
	vmovapd	[srcreg+d1], ymm3	;; Save R2

	vmovapd	[srcreg+d2+32], ymm2	;; Save R7
	vmovapd	[srcreg+d2], ymm7	;; Save R3

	vmovapd	[srcreg+32], ymm6	;; Save R5
	vmovapd	[srcreg], ymm5		;; Save R1

	bump	srcreg, srcinc
	bump	screg1, scinc1
	bump	screg2, scinc2
	ENDM

;;
;; ************************************* eight-reals-fft4 variants ******************************************
;;
;; These macros are used in the last levels of pass 1.  Four sin/cos multipliers are needed to
;; finish off the partial sin/cos multiplies that were done in the first levels of pass 1.
;; FFTs of type r4delay and r4dwpn do this to reduce memory usage at the cost of some extra
;; complex multiplies.

;; Used in last levels of pass 1 (split premultiplier, delay, and dwpn cases).  Swizzling.
;; In this particular case, we know that the sin/cos multipliers for R1/I1 are one.  Thus
;; we can get by with only 3 sin/cos multiplies.
yr4_sg4cl_2sc_eight_reals_fft4_preload MACRO
	ENDM
yr4_sg4cl_2sc_eight_reals_fft4 MACRO srcreg,srcinc,d1,d2,dstreg,dstinc,e1,e2,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd

	;; On input the 64-byte cache lines hold these data values:
	;;	0	+1K	+1K	+1K	4	+1K	+1K	+1K
	;;	1	...
	;;	2	...
	;;	3	...

	;; These are swizzled to:
	;;	0	+1	+1	+1	4K	+1	+1	+1
	;;	1K	...
	;;	2K	...
	;;	3K	...

	;; Swizzle using 8 shufpds and 8 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vmovapd	ymm1, [srcreg]			;; R1
	vmovapd	ymm7, [srcreg+d1]		;; R2
	vshufpd	ymm0, ymm1, ymm7, 15		;; Shuffle R1 and R2 to create R1/R2 hi			; 1
	vshufpd	ymm1, ymm1, ymm7, 0		;; Shuffle R1 and R2 to create R1/R2 low		; 2

	vmovapd	ymm2, [srcreg+d2]		;; R3
	vmovapd	ymm7, [srcreg+d2+d1]		;; R4
	vshufpd	ymm3, ymm2, ymm7, 15		;; Shuffle R3 and R4 to create R3/R4 hi			; 3
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle R3 and R4 to create R3/R4 low		; 4

	vperm2f128 ymm4, ymm0, ymm3, 32		;; Shuffle R1/R2 hi and R3/R4 hi (new R2)		; 5-6
	vperm2f128 ymm0, ymm0, ymm3, 49		;; Shuffle R1/R2 hi and R3/R4 hi (new R4)		; 6-7

	vperm2f128 ymm3, ymm1, ymm2, 32		;; Shuffle R1/R2 low and R3/R4 low (new R1)		; 7-8
	vperm2f128 ymm1, ymm1, ymm2, 49		;; Shuffle R1/R2 low and R3/R4 low (new R3)		; 8-9

	vmovapd	[dstreg], ymm1			;; Temporarily save new R3

	vmovapd	ymm6, [srcreg+32]		;; R5
	vmovapd	ymm2, [srcreg+d1+32]		;; R6
	vshufpd	ymm5, ymm6, ymm2, 15		;; Shuffle R5 and R6 to create R5/R6 hi
	vshufpd	ymm6, ymm6, ymm2, 0		;; Shuffle R5 and R6 to create R5/R6 low

	vmovapd	ymm2, [srcreg+d2+32]		;; R7
	vmovapd	ymm7, [srcreg+d2+d1+32]		;; R8
	vshufpd	ymm1, ymm2, ymm7, 15		;; Shuffle R7 and R8 to create R7/R8 hi
	vshufpd	ymm2, ymm2, ymm7, 0		;; Shuffle R7 and R8 to create R7/R8 low

	vperm2f128 ymm7, ymm5, ymm1, 32		;; Shuffle R5/R6 hi and R7/R8 hi (new R6)
	vperm2f128 ymm5, ymm5, ymm1, 49		;; Shuffle R5/R6 hi and R7/R8 hi (new R8)

	L1prefetch srcreg+L1pd, L1pt

	vperm2f128 ymm1, ymm6, ymm2, 32		;; Shuffle R5/R6 low and R7/R8 low (new R5)
	vperm2f128 ymm6, ymm6, ymm2, 49		;; Shuffle R5/R6 low and R7/R8 low (new R7)

	vsubpd	ymm2, ymm0, ymm5	;; new R8 = R4 - R8			; 1-3
	vaddpd	ymm0, ymm0, ymm5	;; new R4 = R4 + R8			; 2-4

	vsubpd	ymm5, ymm4, ymm7	;; new R6 = R2 - R6			; 3-5
	vaddpd	ymm4, ymm4, ymm7	;; new R2 = R2 + R6			; 4-6

	vmulpd	ymm2, ymm2, YMM_SQRTHALF ;; R8 = R8 * square root		;  4-8
	vmulpd	ymm5, ymm5, YMM_SQRTHALF ;; R6 = R6 * square root		;  6-10

	vsubpd	ymm7, ymm3, ymm1	;; new R5 = R1 - R5			; 5-7
	vaddpd	ymm3, ymm3, ymm1	;; new R1 = R1 + R5			; 6-8

	L1prefetch srcreg+d1+L1pd, L1pt

	vaddpd	ymm1, ymm4, ymm0	;; R2 + R4 (final I1, a.k.a 2nd real result) ; 7-9
	vsubpd	ymm4, ymm4, ymm0	;; R2 - R4 (final I2)			; 8-10

	vmovapd	[dstreg+32], ymm1	;; Save final I1			; 10

	vmovapd	ymm0, [dstreg]		;; Reload saved new R3
	vaddpd	ymm1, ymm0, ymm6	;; new R3 = R3 + R7			; 9-11
	vsubpd	ymm0, ymm0, ymm6	;; new R7 = R3 - R7			; 10-12

	vsubpd	ymm6, ymm5, ymm2	;; R6 = R6 - R8 (Real part)		; 11-13
	vaddpd	ymm5, ymm5, ymm2	;; R8 = R6 + R8 (Imaginary part)	; 12-14

	L1prefetch srcreg+d2+L1pd, L1pt

	vaddpd	ymm2, ymm3, ymm1	;; R1 + R3 (final R1)			; 13-15
	vsubpd	ymm3, ymm3, ymm1	;; R1 - R3 (final R2)			; 14-16

	vsubpd	ymm1, ymm7, ymm6	;; R5 - R6 (final R4)			; 15-17
	vaddpd	ymm7, ymm7, ymm6	;; R5 + R6 (final R3)			; 16-18

	L1prefetch srcreg+d2+d1+L1pd, L1pt

	vsubpd	ymm6, ymm0, ymm5	;; R7 - R8 (final I4)			; 17-19
	vaddpd	ymm0, ymm0, ymm5	;; R7 + R8 (final I3)			; 18-20

	vmovapd	[dstreg], ymm2		;; Save final R1			; 16

	;; screg1 is sin/cos values for R1/I1
	;; screg1+64 is sin/cos values for R2/I2 (w^2n)
	;; screg2 is sin/cos values for R3/I3 (w^n)
	;; screg2+64 is sin/cos values for R4/I4 (w^5n)

	vmovapd	ymm2, [screg2+32]		;; cosine/sine
	vmulpd	ymm5, ymm7, ymm2		;; A3 = R3 * cosine/sine
	vsubpd	ymm5, ymm5, ymm0		;; A3 = A3 - I3
	vmulpd	ymm0, ymm0, ymm2		;; B3 = I3 * cosine/sine
	vaddpd	ymm0, ymm0, ymm7		;; B3 = B3 + R3

	vmovapd	ymm2, [(screg1+64)+32]		;; cosine/sine
	vmulpd	ymm7, ymm3, ymm2		;; A2 = R2 * cosine/sine
	vsubpd	ymm7, ymm7, ymm4		;; A2 = A2 - I2
	vmulpd	ymm4, ymm4, ymm2		;; B2 = I2 * cosine/sine
	vaddpd	ymm4, ymm4, ymm3		;; B2 = B2 + R2

	vmovapd	ymm2, [(screg2+64)+32]		;; cosine/sine
	vmulpd	ymm3, ymm1, ymm2		;; A4 = R4 * cosine/sine
	vsubpd	ymm3, ymm3, ymm6		;; A4 = A4 - I4
	vmulpd	ymm6, ymm6, ymm2		;; B4 = I4 * cosine/sine
	vaddpd	ymm6, ymm6, ymm1		;; B4 = B4 + R4

	vmovapd	ymm2, [screg2]
	vmulpd	ymm5, ymm5, ymm2		;; A3 = A3 * sine (final R3)
	vmulpd	ymm0, ymm0, ymm2		;; B3 = B3 * sine (final I3)
	vmovapd	ymm2, [(screg1+64)]
	vmulpd	ymm7, ymm7, ymm2		;; A2 = A2 * sine (final R2)
	vmulpd	ymm4, ymm4, ymm2		;; B2 = B2 * sine (final I2)

	vmovapd	[dstreg+e2], ymm5		;; Save R3
	vmovapd	[dstreg+e2+32], ymm0		;; Save I3

	vmulpd	ymm3, ymm3, [(screg2+64)]	;; A4 = A4 * sine (final R4)
	vmulpd	ymm6, ymm6, [(screg2+64)]	;; B4 = B4 * sine (final I4)

	vmovapd	[dstreg+e1], ymm7		;; Save R2
	vmovapd	[dstreg+e1+32], ymm4		;; Save I2
	vmovapd	[dstreg+e2+e1], ymm3		;; Save R4
	vmovapd	[dstreg+e2+e1+32], ymm6		;; Save I4

	bump	srcreg, srcinc
	bump	dstreg, dstinc
	bump	screg1, scinc1
	bump	screg2, scinc2
	ENDM

;;
;; ************************************* eight-real-unfft4 variants ******************************************
;;

;; Used in last levels of pass 1 (r4delay and r4dwpn cases).  Swizzling.
;; In this particular case, we know that the sin/cos multipliers for R1/I1 are one.  Thus
;; we can get by with only 3 sin/cos multiplies.
yr4_sg4cl_2sc_eight_reals_unfft4 MACRO srcreg,srcinc,d1,d2,dstreg,dstinc,e1,e2,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd

	;; screg1 is sin/cos values for R1/I1
	;; screg1+64 is sin/cos values for R2/I2 (w^2n)
	;; screg2 is sin/cos values for R3/I3 (w^n)
	;; screg2+64 is sin/cos values for R4/I4 (w^5n)

	vmovapd	ymm5, [(screg1+64)+32]		;; cosine/sine
	vmovapd	ymm4, [srcreg+d1]		;; R2
	vmulpd	ymm2, ymm4, ymm5		;; A2 = R2 * cosine/sine
	vmovapd	ymm3, [srcreg+d1+32]		;; I2
	vaddpd	ymm2, ymm2, ymm3		;; A2 = A2 + I2
	vmulpd	ymm3, ymm3, ymm5		;; B2 = I2 * cosine/sine
	vsubpd	ymm3, ymm3, ymm4		;; B2 = B2 - R2

	vmovapd	ymm0, [screg2+32]		;; cosine/sine
	vmovapd	ymm4, [srcreg+d2]		;; R3
	vmulpd	ymm5, ymm4, ymm0		;; A3 = R3 * cosine/sine
	vmovapd	ymm1, [srcreg+d2+32]		;; I3
	vaddpd	ymm5, ymm5, ymm1		;; A3 = A3 + I3
	vmulpd	ymm1, ymm1, ymm0		;; B3 = I3 * cosine/sine
	vsubpd	ymm1, ymm1, ymm4		;; B3 = B3 - R3

	vmovapd	ymm0, [(screg2+64)+32]		;; cosine/sine
	vmovapd	ymm6, [srcreg+d2+d1]		;; R4
	vmulpd	ymm7, ymm6, ymm0		;; A4 = R4 * cosine/sine
	vmovapd ymm4, [srcreg+d2+d1+32]		;; I4
	vaddpd	ymm7, ymm7, ymm4		;; A4 = A4 + I4
	vmulpd	ymm4, ymm4, ymm0		;; B4 = I4 * cosine/sine
	vsubpd	ymm4, ymm4, ymm6		;; B4 = B4 - R4

	vmulpd	ymm2, ymm2, [screg1+64]		;; A2 = A2 * sine (new R3)
	vmulpd	ymm3, ymm3, [screg1+64]		;; B2 = B2 * sine (new R4)
	vmulpd	ymm5, ymm5, [screg2]		;; A3 = A3 * sine (new R5)
	vmulpd	ymm1, ymm1, [screg2]		;; B3 = B3 * sine (new R6)
	vmulpd	ymm7, ymm7, [screg2+64]		;; A4 = A4 * sine (new R7)
	vmulpd	ymm4, ymm4, [screg2+64]		;; B4 = B4 * sine (new R8)

	vsubpd	ymm0, ymm5, ymm7	;; R5 - R7 (newer R6)
	vaddpd	ymm5, ymm5, ymm7	;; R5 + R7 (newer R5)

	L1prefetch srcreg+L1pd, L1pt

	vsubpd	ymm7, ymm1, ymm4	;; R6 - R8 (newer R8)
	vaddpd	ymm1, ymm1, ymm4	;; R6 + R8 (newer R7)

	vaddpd	ymm4, ymm0, ymm7	;; R6 = R6 + R8
	vsubpd	ymm7, ymm7, ymm0	;; R8 = R8 - R6

	vmulpd	ymm4, ymm4, YMM_SQRTHALF ;; R6 = R6 * square root of 1/2
	vmulpd	ymm7, ymm7, YMM_SQRTHALF ;; R8 = R8 * square root of 1/2

	vmovapd	ymm6, [srcreg+32]	;; I1 (a.k.a new R2)

	vsubpd	ymm0, ymm6, ymm3	;; R2 - R4 (newer R4)
	vaddpd	ymm6, ymm6, ymm3	;; R2 + R4 (newer R2)

	L1prefetch srcreg+d1+L1pd, L1pt

	vsubpd	ymm3, ymm0, ymm7	;; R4 - R8 (final R8)
	vaddpd	ymm0, ymm0, ymm7	;; R4 + R8 (final R4)

	vmovapd	ymm7, [srcreg]		;; R1
	vmovapd	[dstreg], ymm3		;; Save final R8 temporarily
	vaddpd	ymm3, ymm7, ymm2	;; R1 + R3 (newer R1)
	vsubpd	ymm7, ymm7, ymm2	;; R1 - R3 (newer R3)

	vsubpd	ymm2, ymm6, ymm4	;; R2 - R6 (final R6)
	vaddpd	ymm6, ymm6, ymm4	;; R2 + R6 (final R2)

	L1prefetch srcreg+d2+L1pd, L1pt

	vsubpd	ymm4, ymm3, ymm5	;; R1 - R5 (final R5)
	vaddpd	ymm3, ymm3, ymm5	;; R1 + R5 (final R1)

	vsubpd	ymm5, ymm7, ymm1	;; R3 - R7 (final R7)
	vaddpd	ymm7, ymm7, ymm1	;; R3 + R7 (final R3)

	;; Swizzle using 4 shufpds and 4 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vshufpd	ymm1, ymm3, ymm6, 0		;; Shuffle R1 and R2 to create R1/R2 low
	vshufpd	ymm3, ymm3, ymm6, 15		;; Shuffle R1 and R2 to create R1/R2 hi

	vshufpd	ymm6, ymm7, ymm0, 0		;; Shuffle R3 and R4 to create R3/R4 low
	vshufpd	ymm7, ymm7, ymm0, 15		;; Shuffle R3 and R4 to create R3/R4 hi

	L1prefetch srcreg+d2+d1+L1pd, L1pt

	vperm2f128 ymm0, ymm1, ymm6, 32		;; Shuffle R1/R2 low and R3/R4 low (final R1)
	vperm2f128 ymm1, ymm1, ymm6, 49		;; Shuffle R1/R2 low and R3/R4 low (final R3)

	vperm2f128 ymm6, ymm3, ymm7, 32		;; Shuffle R1/R2 hi and R3/R4 hi (final R2)
	vperm2f128 ymm3, ymm3, ymm7, 49		;; Shuffle R1/R2 hi and R3/R4 hi (final R4)

	vmovapd	ymm7, [dstreg]			;; Reload final R8

	vmovapd	[dstreg], ymm0			;; Save R1
	vmovapd	[dstreg+e2], ymm1		;; Save R3
	vmovapd	[dstreg+e1], ymm6		;; Save R2
	vmovapd	[dstreg+e2+e1], ymm3		;; Save R4

	;; Swizzle using 4 shufpds and 4 perm2f128  (1 or 2 clocks each on port 5, throughput 1)

	vshufpd	ymm0, ymm4, ymm2, 0		;; Shuffle R5 and R6 to create R5/R6 low
	vshufpd	ymm4, ymm4, ymm2, 15		;; Shuffle R5 and R6 to create R5/R6 hi

	vshufpd	ymm2, ymm5, ymm7, 0		;; Shuffle R7 and R8 to create R7/R8 low
	vshufpd	ymm5, ymm5, ymm7, 15		;; Shuffle R7 and R8 to create R7/R8 hi

	vperm2f128 ymm7, ymm0, ymm2, 32		;; Shuffle R5/R6 low and R7/R8 low (final R5)
	vperm2f128 ymm0, ymm0, ymm2, 49		;; Shuffle R5/R6 low and R7/R8 low (final R7)

	vperm2f128 ymm2, ymm4, ymm5, 32		;; Shuffle R5/R6 hi and R7/R8 hi (final R6)
	vperm2f128 ymm4, ymm4, ymm5, 49		;; Shuffle R5/R6 hi and R7/R8 hi (final R8)

	vmovapd	[dstreg+32], ymm7		;; Save R5
	vmovapd	[dstreg+e2+32], ymm0		;; Save R7
	vmovapd	[dstreg+e1+32], ymm2		;; Save R6
	vmovapd	[dstreg+e2+e1+32], ymm4		;; Save R8

	bump	srcreg, srcinc
	bump	dstreg, dstinc
	bump	screg1, scinc1
	bump	screg2, scinc2
	ENDM

;;
;; ********************************* one-eight-reals-three-four-complex-fft variants ***************************************
;;

;; Macro to do one eight_reals_fft and three four_complex_djbfft.
;; Having eight-real data and four-complex data in the same YMM register
;; is not an ideal situation.  However, sometimes one cannot get the
;; perfect memory layout.

yr4_4cl_eight_reals_four_complex_djbfft_preload MACRO
	yr4_o8r_t4c_djbfft_mem_preload
	ENDM
yr4_4cl_eight_reals_four_complex_djbfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt
	yr4_o8r_t4c_djbfft_mem [srcreg],[srcreg+d1],[srcreg+d2],[srcreg+d2+d1],[srcreg+32],[srcreg+d1+32],[srcreg+d2+32],[srcreg+d2+d1+32],screg,[srcreg]
;;	vmovapd	[srcreg], ymm7		;; Save R1
	vmovapd	[srcreg+32], ymm0	;; Save I1
	vmovapd	[srcreg+d1], ymm3	;; Save R2
	vmovapd	[srcreg+d1+32], ymm6	;; Save I2
	vmovapd	[srcreg+d2], ymm2	;; Save R3
	vmovapd	[srcreg+d2+32], ymm1	;; Save I3
	vmovapd	[srcreg+d2+d1], ymm5	;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm4	;; Save I4
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; Does a yr4_x8r_fft_mem on the low word of the ymm register
;; Does a yr4_x4c_djbfft_mem on the high words of the ymm register
;; This is REALLY funky, as we do both at the same time within
;; the full ymm register whenever possible.
yr4_o8r_t4c_djbfft_mem_preload MACRO
	ENDM
yr4_o8r_t4c_djbfft_mem MACRO mem1,mem2,mem3,mem4,mem5,mem6,mem7,mem8,screg,dst
					;; Eight-reals comments		; Three four-complex comments
	vmovapd	ymm2, mem6		;; R6				; I2
	vmovapd	ymm6, mem4		;; R4				; R4
	vblendpd ymm3, ymm2, ymm6, 1	;; R4				; I2
	vmovapd	ymm5, mem8		;; R8				; I4
	vsubpd	ymm4, ymm3, ymm5	;; interim R8 = R4 - R8		; I2 - I4 (new I4)

	vblendpd ymm6, ymm6, ymm2, 1	;; R6				; R4
	vmovapd	ymm1, mem2		;; R2				; R2
	vsubpd	ymm7, ymm1, ymm6	;; interim R6 = R2 - R6		; R2 - R4 (new R4)

	vaddpd	ymm3, ymm3, ymm5	;; new R4 = R4 + R8		; I2 + I4 (new I2)
	vaddpd	ymm1, ymm1, ymm6	;; new R2 = R2 + R6		; R2 + R4 (new R2)

	vmulsd	xmm0, xmm4, YMM_SQRTHALF;; interim R8 * square root	; meaningless
	vmulsd	xmm2, xmm7, YMM_SQRTHALF;; interim R6 * square root	; meaningless

	vaddsd	xmm5, xmm2, xmm0	;; new R8 = interim R6 + R8	; meaningless
	vsubsd	xmm2, xmm0, xmm2	;; new negR6 = interim R8 - R6	; meaningless

	vblendpd ymm7, ymm7, ymm5, 1	;; new R8			; new R4
	vblendpd ymm4, ymm4, ymm2, 1	;; new negR6			; new I4

	vmovapd	ymm0, mem3		;; R3				; R3
	vmovapd	ymm2, mem5		;; R5				; I1
	vblendpd ymm6, ymm0, ymm2, 1	;; R5				; R3
	vblendpd ymm2, ymm2, ymm0, 1	;; R3				; I1

	vaddpd	ymm0, ymm2, mem7	;; new R3 = R3 + R7		; I1 + I3 (new I1)
	vblendpd ymm5, ymm1, ymm0, 1	;; new R3			; new R2
	vblendpd ymm0, ymm0, ymm1, 1	;; new R2			; new I1
	vsubpd	ymm2, ymm2, mem7	;; new R7 = R3 - R7		; I1 - I3 (new I3)

	vaddpd	ymm1, ymm2, ymm7	;; R7 + R8 (final I3)		; I3 + R4 (final I3)
	vsubpd	ymm2, ymm2, ymm7	;; R7 - R8 (final I4)		; I3 - R4 (final I4)

	vmovapd	ymm7, mem1		;; R1				; R1
	vmovapd	YMM_TMP1, ymm2		;; Temporarily save final I4
	vaddpd	ymm2, ymm7, ymm6	;; new R1 = R1 + R5		; R1 + R3 (new R1)
	vsubpd	ymm7, ymm7, ymm6	;; new R5 = R1 - R5		; R1 - R3 (new R3)

	vsubpd	ymm6, ymm0, ymm3	;; R2 - R4 (final I2)		; I1 - I2 (final I2)
	vaddpd	ymm0, ymm0, ymm3	;; R2 + R4 (final I1, a.k.a 2nd real result) ; I1 + I2 (final I1)

	vaddpd	ymm3, ymm2, ymm5	;; R1 + R3 (final R1)		; R1 + R2 (final R1)
	vsubpd	ymm2, ymm2, ymm5	;; R1 - R3 (final R2)		; R1 - R2 (final R2)

	vsubpd	ymm5, ymm7, ymm4	;; R5 - negR6 (final R3)	; R3 - I4 (final R3)
	vaddpd	ymm7, ymm7, ymm4	;; R5 + negR6 (final R4)	; R3 + I4 (final R4)

	vmovapd	dst, ymm3		;; Save final R1

	vmovapd	ymm4, [screg+64+32]	;; cosine/sine
	vmulpd	ymm3, ymm2, ymm4	;; A2 = R2 * cosine/sine
	vsubpd	ymm3, ymm3, ymm6	;; A2 = A2 - I2

	vmulpd	ymm6, ymm6, ymm4	;; B2 = I2 * cosine/sine
	vaddpd	ymm6, ymm6, ymm2	;; B2 = B2 + R2

	vmovapd	ymm4, [screg+32]	;; cosine/sine
	vmulpd	ymm2, ymm5, ymm4	;; A3 = R3 * cosine/sine
	vsubpd	ymm2, ymm2, ymm1	;; A3 = A3 - I3

	vmulpd	ymm1, ymm1, ymm4	;; B3 = I3 * cosine/sine
	vaddpd	ymm1, ymm1, ymm5	;; B3 = B3 + R3

	vmovapd	ymm4, [screg+128+32]	;; cosine/sine
	vmulpd	ymm5, ymm7, ymm4	;; A4 = R4 * cosine/sine
	vsubpd	ymm5, ymm5, YMM_TMP1	;; A4 = A4 - I4

	vmulpd	ymm4, ymm4, YMM_TMP1	;; B4 = I4 * cosine/sine
	vaddpd	ymm4, ymm4, ymm7	;; B4 = B4 + R4

	vmovapd	ymm7, [screg+64]	;; Sine
	vmulpd	ymm3, ymm3, ymm7	;; A2 = A2 * sine (final R2)
	vmulpd	ymm6, ymm6, ymm7	;; B2 = B2 * sine (final I2)

	vmovapd	ymm7, [screg]		;; Sine
	vmulpd	ymm2, ymm2, ymm7	;; A3 = A3 * sine (final R3)
	vmulpd	ymm1, ymm1, ymm7	;; B3 = B3 * sine (final I3)

	vmovapd	ymm7, [screg+128]	;; Sine
	vmulpd	ymm5, ymm5, ymm7	;; A4 = A4 * sine (final R4)
	vmulpd	ymm4, ymm4, ymm7	;; B4 = B4 * sine (final I4)
	ENDM

;;
;; ********************************* one-eight-reals-three-four-complex-unfft variants ***************************************
;;

;; Macro to do one eight_reals_unfft and three four_complex_djbunfft.
;; The eight-reals operation is done in the lower word of the YMM
;; register.  This isn't very efficient, but this macro isn't called a whole lot.

yr4_4cl_eight_reals_four_complex_djbunfft_preload MACRO
	yr4_o8r_t4c_djbunfft_mem_preload
	ENDM
yr4_4cl_eight_reals_four_complex_djbunfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt
	yr4_o8r_t4c_djbunfft_mem [srcreg],[srcreg+32],[srcreg+d1],[srcreg+d1+32],[srcreg+d2],[srcreg+d2+32],[srcreg+d2+d1],[srcreg+d2+d1+32],screg,[srcreg]
;;	vmovapd	[srcreg], ymm1		;; Save R1
	vmovapd	[srcreg+32], ymm6	;; Save R5/I1
	vmovapd	[srcreg+d1], ymm0	;; Save R2
	vmovapd	[srcreg+d1+32], ymm2	;; Save R6/I2
	vmovapd	[srcreg+d2], ymm5	;; Save R3
	vmovapd	[srcreg+d2+32], ymm4	;; Save R7/I3
	vmovapd	[srcreg+d2+d1], ymm7	;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm3	;; Save R8/I4
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

yr4_o8r_t4c_djbunfft_mem_preload MACRO
	ENDM
yr4_o8r_t4c_djbunfft_mem MACRO mem1,mem2,mem3,mem4,mem5,mem6,mem7,mem8,screg,dst
	vmovapd	ymm3, [screg+64+32]	;; cosine/sine
	vmovapd	ymm2, mem3		;; R2
	vmulpd	ymm6, ymm2, ymm3	;; A2 = R2 * cosine/sine		; 1-5
	vmovapd	ymm0, mem4		;; I2
	vmulpd	ymm3, ymm3, ymm0	;; B2 = I2 * cosine/sine		; 2-6

	vmovapd	ymm5, [screg+32]	;; cosine/sine
	vmovapd	ymm4, mem5		;; R3
	vmulpd	ymm7, ymm4, ymm5	;; A3 = R3 * cosine/sine		; 3-7
	vmovapd	ymm1, mem6		;; I3

	vaddpd	ymm6, ymm6, ymm0	;; A2 = A2 + I2				; 6-8

	vmulpd	ymm0, ymm1, ymm5	;; B3 = I3 * cosine/sine		; 4-8

	vsubpd	ymm3, ymm3, ymm2	;; B2 = B2 - R2				; 7-9
	vaddpd	ymm7, ymm7, ymm1	;; A3 = A3 + I3				; 8-10

	vmovapd	ymm5, [screg+128+32]	;; cosine/sine
	vmovapd	ymm2, mem7		;; R4
	vmulpd	ymm1, ymm2, ymm5	;; A4 = R4 * cosine/sine		; 5-9

	vsubpd	ymm0, ymm0, ymm4	;; B3 = B3 - R3				; 9-11

	vmovapd	ymm4, mem8		;; I4
	vmulpd	ymm5, ymm4, ymm5	;; B4 = I4 * cosine/sine		; 6-10

	vmulpd	ymm6, ymm6, [screg+64]	;; A2 = A2 * sine (new R2)		; 9-13
	vmulpd	ymm3, ymm3, [screg+64]	;; B2 = B2 * sine (new I2)		; 10-14

	vaddpd	ymm1, ymm1, ymm4	;; A4 = A4 + I4				; 10-12
	vsubpd	ymm5, ymm5, ymm2	;; B4 = B4 - R4				; 11-13

	vmovapd	ymm4, [screg]		;; Sine
	vmulpd	ymm7, ymm7, ymm4	;; A3 = A3 * sine (new R3)		; 11-15
	vmulpd	ymm0, ymm0, ymm4	;; B3 = B3 * sine (new I3)		; 12-16
	vmovapd	ymm4, [screg+128]	;; Sine
	vmulpd	ymm1, ymm1, ymm4	;; A4 = A4 * sine (new R4)		; 13-17
	vmulpd	ymm5, ymm5, ymm4	;; B4 = B4 * sine (new I4)		; 14-18

					;; Eight reals comments		; Four complex comments
					;; new R3 = A2
					;; new R4 = B2
					;; new R5 = A3
					;; new R6 = B3
					;; new R7 = A4
					;; new R8 = B4

	vsubpd	ymm2, ymm0, ymm5	;; R6 - R8 (new R8)		; I3 - I4 (new R4)
	vaddpd	ymm0, ymm0, ymm5	;; R6 + R8 (new R7)		; I3 + I4 (new I3)

	vsubpd	ymm4, ymm7, ymm1	;; R5 - R7 (new R6)		; R3 - R4 (new negI4)
	vaddpd	ymm7, ymm7, ymm1	;; R5 + R7 (new R5)		; R3 + R4 (new R3)

	vmovapd	ymm1, mem2		;; R2				; I1
	vsubpd	ymm5, ymm1, ymm3	;; R2 - R4 (new R4)		; I1 - I2 (new I2)
	vaddpd	ymm1, ymm1, ymm3	;; R2 + R4 (new R2)		; I1 + I2 (new I1)

	vmovapd	YMM_TMP1, ymm0		;; Temporarily save new R7/new I3

	vaddsd	xmm3, xmm4, xmm2	;; R6 = R6 + R8			; meaningless
	vsubsd	xmm0, xmm4, xmm2	;; negR8 = R6 - R8		; meaningless

	vmulsd	xmm3, xmm3, YMM_SQRTHALF ;; R6 *= square root of 1/2	; meaningless
	vmulsd	xmm0, xmm0, YMM_SQRTHALF ;; negR8 *= square root of 1/2	; meaningless

	vblendpd ymm2, ymm2, ymm3, 1	;; new R6			; new R4
	vblendpd ymm4, ymm4, ymm0, 1	;; new negR8			; new negI4

	vaddpd	ymm3, ymm5, ymm4	;; R4 + negR8 (final R8)	; I2 + negI4 (final I4)
	vsubpd	ymm5, ymm5, ymm4	;; R4 - negR8 (final R4)	; I2 - negI4 (final I2)

	vmovapd	ymm0, mem1		;; R1				; R1
	vaddpd	ymm4, ymm0, ymm6	;; R1 + R3 (new R1)		; R1 + R2 (new R1)
	vsubpd	ymm0, ymm0, ymm6	;; R1 - R3 (new R3)		; R1 - R2 (new R2)

	vblendpd ymm6, ymm1, ymm0, 1	;; new R3			; new I1
	vblendpd ymm0, ymm0, ymm1, 1	;; new R2			; new R2

	vsubpd	ymm1, ymm4, ymm7	;; R1 - R5 (final R5)		; R1 - R3 (final R3)
	vaddpd	ymm4, ymm4, ymm7	;; R1 + R5 (final R1)		; R1 + R3 (final R1)

	vsubpd	ymm7, ymm0, ymm2	;; R2 - R6 (final R6)		; R2 - R4 (final R4)
	vaddpd	ymm0, ymm0, ymm2	;; R2 + R6 (final R2)		; R2 + R4 (final R2)

	vmovapd	dst, ymm4		;; Store R1

	vmovapd	ymm2, YMM_TMP1		;; Reload saved new R7/new I3
	vsubpd	ymm4, ymm6, ymm2	;; R3 - R7 (final R7)		; I1 - I3 (final I3)
	vaddpd	ymm6, ymm6, ymm2	;; R3 + R7 (final R3)		; I1 + I3 (final I1)

	vblendpd ymm2, ymm5, ymm7, 1	;; final R6			; final I2
	vblendpd ymm7, ymm7, ymm5, 1	;; final R4			; final R4

	vblendpd ymm5, ymm1, ymm6, 1	;; final R3			; final R3
	vblendpd ymm6, ymm6, ymm1, 1	;; final R5			; final I1
	ENDM

;;
;; ********************************* one-eight-reals-three-four-complex-fft-with-square variants ***************************************
;;

;; Macro to do one eight_reals_fft and three four_complex_fft in the final levels of an FFT.
;; The eight-reals data uses only one quarter of a YMM register.  The rest of the YMM register
;; contains four-complex data.  We handle this problem somewhat inefficiently, but this macro
;; is called only once during an FFT.

yr4_4cl_eight_reals_four_complex_fft_final MACRO srcreg,srcinc,d1,d2
	yr4_o8r_t4c_simple_fft_mem [srcreg],[srcreg+d1],[srcreg+d2],[srcreg+d2+d1],[srcreg+32],[srcreg+d1+32],[srcreg+d2+32],[srcreg+d2+d1+32],[srcreg+d2+d1],[srcreg+d2+d1+32]
	vmovapd	[srcreg], ymm3			;; Save R1
	vmovapd	[srcreg+32], ymm0		;; Save I1
	vmovapd	[srcreg+d1], ymm2		;; Save R2
	vmovapd	[srcreg+d1+32], ymm6		;; Save I2
	vmovapd	[srcreg+d2], ymm1		;; Save R3
	vmovapd	[srcreg+d2+32], ymm5		;; Save I3
;;	vmovapd	[srcreg+d2+d1], ymm4		;; Save R4
;;	vmovapd	[srcreg+d2+d1+32], ymm7		;; Save I4
	bump	srcreg, srcinc
	ENDM

yr4_4cl_eight_reals_four_complex_with_square MACRO srcreg,srcinc,d1,d2
	ysquare7 srcreg
	yr4_o8r_t4c_simple_fft_mem [srcreg],[srcreg+d1],[srcreg+d2],[srcreg+d2+d1],[srcreg+32],[srcreg+d1+32],[srcreg+d2+32],[srcreg+d2+d1+32],[srcreg+d2+d1],[srcreg+d2+d1+32]
	yp_complex_square ymm2, ymm6, ymm4	;; Square R2, I2
	yp_complex_square ymm1, ymm5, ymm4	;; Square R3, I3
	vmovapd	[srcreg+d1], ymm2		;; Save R2
	vmovapd	[srcreg+d1+32], ymm6		;; Save I2
	vmovapd	[srcreg+d2], ymm1		;; Save R3
	vmovapd	[srcreg+d2+32], ymm5		;; Save I3
	vmulsd	xmm4, xmm3, xmm3		;; Square R1 low value
	vmovsd	Q [srcreg-16], xmm4		;; Save square of sum of FFT values
	vmulsd	xmm7, xmm0, xmm0		;; Square I1 low value (it is actually a real value)
	yp_complex_square ymm3, ymm0, ymm5	;; Square R1, I1 (3 high values)
	vblendpd ymm3, ymm3, ymm4, 1		;; Blend R1 real and complex results
	vblendpd ymm0, ymm0, ymm7, 1		;; Blend I1 real and complex results
	vmovapd	[srcreg], ymm3			;; Save R1
	vmovapd	[srcreg+32], ymm0		;; Save I1
	vmovapd	ymm0, [srcreg+d2+d1]		;; Reload R4
	vmovapd	ymm6, [srcreg+d2+d1+32]		;; Reload I4
	yp_complex_square ymm0, ymm6, ymm5	;; Square R4, I4
	vmovapd	[srcreg+d2+d1], ymm0		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm6		;; Save I4
	yr4_o8r_t4c_simple_unfft_mem [srcreg],[srcreg+32],[srcreg+d1],[srcreg+d1+32],[srcreg+d2],[srcreg+d2+32],[srcreg+d2+d1],[srcreg+d2+d1+32],[srcreg]
;;	vmovapd	[srcreg], ymm0			;; Save R1
	vmovapd	[srcreg+32], ymm4		;; Save I1
	vmovapd	[srcreg+d1], ymm5		;; Save R2
	vmovapd	[srcreg+d1+32], ymm7		;; Save I2
	vmovapd	[srcreg+d2], ymm2		;; Save R3
	vmovapd	[srcreg+d2+32], ymm3		;; Save I3
	vmovapd	[srcreg+d2+d1], ymm1		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm6		;; Save I4
	bump	srcreg, srcinc
	ENDM

yr4_4cl_eight_reals_four_complex_with_mult MACRO srcreg,srcinc,d1,d2
	ymult7	srcreg, srcreg+rbp
	yr4_o8r_t4c_simple_fft_mem [srcreg],[srcreg+d1],[srcreg+d2],[srcreg+d2+d1],[srcreg+32],[srcreg+d1+32],[srcreg+d2+32],[srcreg+d2+d1+32],[srcreg+d2+d1],[srcreg+d2+d1+32]
	yp_complex_mult ymm2, ymm6, [srcreg+d1][rbp], [srcreg+d1+32][rbp], ymm4, ymm7 ;; Mult R2, I2
	yp_complex_mult ymm1, ymm5, [srcreg+d2][rbp], [srcreg+d2+32][rbp], ymm4, ymm7 ;; Mult R3, I3
	vmovapd	[srcreg+d1], ymm2		;; Save R2
	vmovapd	[srcreg+d1+32], ymm6		;; Save I2
	vmovapd	[srcreg+d2], ymm1		;; Save R3
	vmovapd	[srcreg+d2+32], ymm5		;; Save I3
	vmovapd	ymm6, [srcreg][rbp]		;; Load other R1
	vmulsd	xmm2, xmm3, xmm6		;; Mult R1 low values
	vmovsd	Q [srcreg-16], xmm2		;; Save product of sums of FFT values
	vmovapd	ymm7, [srcreg+32][rbp]		;; Load other I1
	vmulsd	xmm1, xmm0, xmm7		;; Mult I1 low values (they are actually real values)
	yp_complex_mult ymm3, ymm0, ymm6, ymm7, ymm4, ymm5 ;; Mult R1, I1 (3 high values)
	vblendpd ymm3, ymm3, ymm2, 1		;; Blend R1 real and complex results
	vblendpd ymm0, ymm0, ymm1, 1		;; Blend I1 real and complex results
	vmovapd	[srcreg], ymm3			;; Save R1
	vmovapd	[srcreg+32], ymm0		;; Save I1
	vmovapd	ymm0, [srcreg+d2+d1]		;; Reload R4
	vmovapd	ymm6, [srcreg+d2+d1+32]		;; Reload I4
	yp_complex_mult ymm0, ymm6, [srcreg+d2+d1][rbp], [srcreg+d2+d1+32][rbp], ymm4, ymm5 ;; Mult R4, I4
	vmovapd	[srcreg+d2+d1], ymm0		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm6		;; Save I4
	yr4_o8r_t4c_simple_unfft_mem [srcreg],[srcreg+32],[srcreg+d1],[srcreg+d1+32],[srcreg+d2],[srcreg+d2+32],[srcreg+d2+d1],[srcreg+d2+d1+32],[srcreg]
;;	vmovapd	[srcreg], ymm0			;; Save R1
	vmovapd	[srcreg+32], ymm4		;; Save I1
	vmovapd	[srcreg+d1], ymm5		;; Save R2
	vmovapd	[srcreg+d1+32], ymm7		;; Save I2
	vmovapd	[srcreg+d2], ymm2		;; Save R3
	vmovapd	[srcreg+d2+32], ymm3		;; Save I3
	vmovapd	[srcreg+d2+d1], ymm1		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm6		;; Save I4
	bump	srcreg, srcinc
	ENDM

yr4_4cl_eight_reals_four_complex_with_mulf MACRO srcreg,srcinc,d1,d2
	ymult7	srcreg+rbx, srcreg+rbp
	vmovapd	ymm2, [srcreg+d1][rbx]		;; R2
	vmovapd	ymm1, [srcreg+d1+32][rbx]	;; I2
	yp_complex_mult ymm2, ymm1, [srcreg+d1][rbp], [srcreg+d1+32][rbp], ymm4, ymm5 ;; Mult R2, I2
	vmovapd	ymm7, [srcreg+d2][rbx]		;; R3
	vmovapd	ymm3, [srcreg+d2+32][rbx]	;; I3
	yp_complex_mult ymm7, ymm3, [srcreg+d2][rbp], [srcreg+d2+32][rbp], ymm4, ymm5 ;; Mult R3, I3
	vmovapd	ymm0, [srcreg+d2+d1][rbx]	;; R4
	vmovapd	ymm6, [srcreg+d2+d1+32][rbx]	;; I4
	yp_complex_mult ymm0, ymm6, [srcreg+d2+d1][rbp], [srcreg+d2+d1+32][rbp], ymm4, ymm5 ;; Mult R4, I4
	vmovapd	[srcreg+d1], ymm2		;; Save R2
	vmovapd	[srcreg+d1+32], ymm1		;; Save I2
	vmovapd	[srcreg+d2], ymm7		;; Save R3
	vmovapd	[srcreg+d2+32], ymm3		;; Save I3
	vmovapd	[srcreg+d2+d1], ymm0		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm6		;; Save I4
	vmovapd	ymm2, [srcreg][rbx]		;; R1
	vmovapd	ymm6, [srcreg][rbp]		;; Other R1
	vmulsd	xmm0, xmm2, xmm6		;; Mult R1 low values
	vmovsd	Q [srcreg-16], xmm0		;; Save product of sums of FFT values
	vmovapd	ymm4, [srcreg+32][rbx]		;; I1
	vmovapd	ymm7, [srcreg+32][rbp]		;; Other I1
	vmulsd	xmm1, xmm4, xmm7		;; Mult I1 low values (they are actually real values)
	yp_complex_mult ymm2, ymm4, ymm6, ymm7, ymm3, ymm5 ;; Mult R1, I1 (3 high values)
	vblendpd ymm2, ymm2, ymm0, 1		;; Blend R1 real and complex results
	vblendpd ymm4, ymm4, ymm1, 1		;; Blend I1 real and complex results
	vmovapd	[srcreg], ymm2			;; Save R1
	vmovapd	[srcreg+32], ymm4		;; Save I1
	yr4_o8r_t4c_simple_unfft_mem [srcreg],[srcreg+32],[srcreg+d1],[srcreg+d1+32],[srcreg+d2],[srcreg+d2+32],[srcreg+d2+d1],[srcreg+d2+d1+32],[srcreg]
;;	vmovapd	[srcreg], ymm0			;; Save R1
	vmovapd	[srcreg+32], ymm4		;; Save I1
	vmovapd	[srcreg+d1], ymm5		;; Save R2
	vmovapd	[srcreg+d1+32], ymm7		;; Save I2
	vmovapd	[srcreg+d2], ymm2		;; Save R3
	vmovapd	[srcreg+d2+32], ymm3		;; Save I3
	vmovapd	[srcreg+d2+d1], ymm1		;; Save R4
	vmovapd	[srcreg+d2+d1+32], ymm6		;; Save I4
	bump	srcreg, srcinc
	ENDM

;; Does a yr4_8r_fft_mem on the one low value of the ymm register
;; Does a yr4_4c_fft_mem on the three high values of the ymm register
;; This is REALLY funky, as we do both at the same time within
;; the full ymm register whenever possible.  We could improve this code
;; a few clocks by implementing the improvements in yr4_o8r_t4c_djbfft_mem.
yr4_o8r_t4c_simple_fft_mem MACRO mem1,mem2,mem3,mem4,mem5,mem6,mem7,mem8,destR4,destI4
					;; Eight-reals comments		; Three four-complex comments
	vmovapd	ymm6, mem4		;; R4				; R4
	vmovapd	ymm7, mem8		;; R8				; I4
	vsubsd	xmm0, xmm6, xmm7	;; interim R8 = R4 - R8		; meaningless

	vmovapd	ymm1, mem2		;; R2				; R2
	vmovapd	ymm5, mem6		;; R6				; I2
	vsubsd	xmm2, xmm1, xmm5	;; interim R6 = R2 - R6		; meaningless

	vmulsd	xmm0, xmm0, YMM_SQRTHALF;; interim R8 * square root	; meaningless
	vmulsd	xmm2, xmm2, YMM_SQRTHALF;; interim R6 * square root	; meaningless

	vblendpd ymm3, ymm5, ymm6, 1	;; R4				; I2
	vaddpd	ymm3, ymm3, ymm7	;; new R4 = R4 + R8		; I2 + I4 (new I2)

	vblendpd ymm4, ymm5, ymm0, 1	;; interim R8			; I2
	vblendpd ymm7, ymm7, ymm2, 1	;; interim R6			; I4
	vsubpd	ymm4, ymm4, ymm7	;; new negR6 = interim R8 - R6	; I2 - I4 (new I4)

	vaddsd	xmm2, xmm2, xmm0	;; new R8 = interim R6 + R8	; meaningless

	vsubpd	ymm7, ymm1, ymm6	;; meaningless			; R2 - R4 (new R4)
	vblendpd ymm7, ymm7, ymm2, 1	;; new R8			; new R4

	vaddsd	xmm5, xmm1, xmm5	;; new R2 = R2 + R6		; meaningless

	vaddpd	ymm1, ymm1, ymm6	;; meaningless			; R2 + R4 (new R2)

	vmovapd	ymm0, mem3		;; R3				; R3
	vmovapd	ymm2, mem5		;; R5				; I1
	vblendpd ymm6, ymm0, ymm2, 1	;; R5				; R3
	vblendpd ymm2, ymm2, ymm0, 1	;; R3				; I1

	vaddpd	ymm0, ymm2, mem7	;; new R3 = R3 + R7		; I1 + I3 (new I1)
	vblendpd ymm1, ymm1, ymm0, 1	;; new R3			; new R2
	vblendpd ymm0, ymm0, ymm5, 1	;; new R2			; new I1

	vsubpd	ymm2, ymm2, mem7	;; new R7 = R3 - R7		; I1 - I3 (new I3)

	vaddpd	ymm5, ymm2, ymm7	;; R7 + R8 (final I3)		; I3 + R4 (final I3)
	vsubpd	ymm2, ymm2, ymm7	;; R7 - R8 (final I4)		; I3 - R4 (final I4)

	vmovapd	ymm7, mem1		;; R1				; R1
	vmovapd	destI4, ymm2		;; Save final I4
	vaddpd	ymm2, ymm7, ymm6	;; new R1 = R1 + R5		; R1 + R3 (new R1)
	vsubpd	ymm7, ymm7, ymm6	;; new R5 = R1 - R5		; R1 - R3 (new R3)

	vsubpd	ymm6, ymm0, ymm3	;; R2 - R4 (new R4, final I2)	; I1 - I2 (final I2)
	vaddpd	ymm0, ymm0, ymm3	;; R2 + R4 (new R2)		; I1 + I2 (final I1)

	vaddpd	ymm3, ymm2, ymm1	;; R1 + R3 (new R1)		; R1 + R2 (final R1)
	vsubpd	ymm2, ymm2, ymm1	;; R1 - R3 (new R3, final R2)	; R1 - R2 (final R2)

	vsubpd	ymm1, ymm7, ymm4	;; R5 - negR6 (final R3)	; R3 - I4 (final R3)
	vaddpd	ymm7, ymm7, ymm4	;; R5 + negR6 (final R4)	; R3 + I4 (final R4)

	vsubsd	xmm4, xmm3, xmm0	;; R1 - R2 (final I1, a.k.a 2nd real result)
	vmovapd	destR4, ymm7		;; Save final R4
	vaddsd	xmm7, xmm3, xmm0	;; R1 + R2 (final R1)		; meaningless
	vblendpd ymm0, ymm0, ymm4, 1	;; final I1			; final I1
	vblendpd ymm3, ymm3, ymm7, 1	;; final R1			; final R1
	ENDM

yr4_o8r_t4c_simple_unfft_mem MACRO mem1,mem2,mem3,mem4,mem5,mem6,mem7,mem8,dst1

	;; Do the eight-reals part

	vmovsd	xmm1, Q mem5		;; R5
	vmovsd	xmm0, Q mem7		;; R7
	vsubsd	xmm4, xmm1, xmm0	;; new R6 = R5 - R7
	vaddsd	xmm1, xmm1, xmm0	;; new R5 = R5 + R7

	vmovsd	xmm7, Q mem6		;; R6
	vmovsd	xmm0, Q mem8		;; R8
	vsubsd	xmm5, xmm7, xmm0	;; new R8 = R6 - R8
	vaddsd	xmm7, xmm7, xmm0	;; new R7 = R6 + R8

	vaddsd	xmm3, xmm4, xmm5	;; R6 = R6 + R8
	vsubsd	xmm4, xmm5, xmm4	;; R8 = R8 - R6

	vmovsd	xmm2, Q mem1		;; R1
	vmovsd	xmm6, Q mem2		;; R2
	vsubsd	xmm2, xmm2, xmm6	;; new R2 = R1 - R2
	vmulsd	xmm2, xmm2, YMM_HALF	;; Mul new R2 by HALF
	vaddsd	xmm6, xmm2, xmm6	;; new R1 = R1 + R2

	vmulsd	xmm3, xmm3, YMM_SQRTHALF;; R6 = R6 * square root of 1/2
	vmulsd	xmm4, xmm4, YMM_SQRTHALF;; R8 = R8 * square root of 1/2

	vmovsd	xmm0, Q mem4		;; R4
	vaddsd	xmm5, xmm2, xmm0	;; R2 + R4 (new R2)
	vsubsd	xmm2, xmm2, xmm0	;; R2 - R4 (new R4)

	vsubsd	xmm0, xmm5, xmm3	;; R2 - R6 (final R6)
	vaddsd	xmm5, xmm5, xmm3	;; R2 + R6 (final R2)

	vsubsd	xmm3, xmm2, xmm4	;; R4 - R8 (final R8)
	vaddsd	xmm2, xmm2, xmm4	;; R4 + R8 (final R4)

	vmovsd	YMM_TMP6, xmm0		;; Save R6
	vmovsd	YMM_TMP2, xmm5		;; Save R2

	vmovsd	xmm4, Q mem3		;; R3
	vaddsd	xmm0, xmm6, xmm4	;; R1 + R3 (new R1)
	vsubsd	xmm6, xmm6, xmm4	;; R1 - R3 (new R3)

	vmovsd	YMM_TMP8, xmm3		;; Save R8
	vmovsd	YMM_TMP4, xmm2		;; Save R4

	vsubsd	xmm2, xmm0, xmm1	;; R1 - R5 (final R5)
	vaddsd	xmm0, xmm0, xmm1	;; R1 + R5 (final R1)

	vsubsd	xmm3, xmm6, xmm7	;; R3 - R7 (final R7)
	vaddsd	xmm6, xmm6, xmm7	;; R3 + R7 (final R3)

	vmovsd	YMM_TMP5, xmm2		;; Save R5
	vmovsd	YMM_TMP1, xmm0		;; Save R1

	vmovsd	YMM_TMP7, xmm3		;; Save R7
	vmovsd	YMM_TMP3, xmm6		;; Save R3

	;; Do the four complex part

	vmovapd	ymm7, mem1		;; R1
	vmovapd	ymm0, mem3		;; R2
	vsubpd	ymm5, ymm7, ymm0	;; R1 - R2 (new R2)
	vaddpd	ymm7, ymm7, ymm0	;; R1 + R2 (new R1)

	vmovapd	ymm1, mem7		;; R4
	vmovapd	ymm0, mem5		;; R3
	vsubpd	ymm3, ymm1, ymm0	;; R4 - R3 (new I4)
	vaddpd	ymm1, ymm1, ymm0	;; R4 + R3 (new R3)

	vmovapd	ymm0, mem6		;; I3
	vmovapd	ymm2, mem8		;; I4
	vsubpd	ymm4, ymm0, ymm2	;; I3 - I4 (new R4)
	vaddpd	ymm0, ymm0, ymm2	;; I3 + I4 (new I3)

	vsubpd	ymm2, ymm7, ymm1	;; R1 - R3 (final R3)
	vaddpd	ymm7, ymm7, ymm1	;; R1 + R3 (final R1)

	vsubpd	ymm1, ymm5, ymm4	;; R2 - R4 (final R4)
	vaddpd	ymm5, ymm5, ymm4	;; R2 + R4 (final R2)

	vblendpd ymm2, ymm2, YMM_TMP3, 1 ;; Blend in R3 real result
	vblendpd ymm7, ymm7, YMM_TMP1, 1 ;; Blend in R1 real result

	vblendpd ymm1, ymm1, YMM_TMP4, 1 ;; Blend in R4 real result
	vblendpd ymm5, ymm5, YMM_TMP2, 1 ;; Blend in R2 real result

	vmovapd	dst1, ymm7

	vmovapd	ymm4, mem2		;; I1
	vmovapd	ymm6, mem4		;; I2
	vsubpd	ymm7, ymm4, ymm6	;; I1 - I2 (new I2)
	vaddpd	ymm4, ymm4, ymm6	;; I1 + I2 (new I1)

	vsubpd	ymm6, ymm7, ymm3	;; I2 - I4 (final I4)
	vaddpd	ymm7, ymm7, ymm3	;; I2 + I4 (final I2)

	vsubpd	ymm3, ymm4, ymm0	;; I1 - I3 (final I3)
	vaddpd	ymm4, ymm4, ymm0	;; I1 + I3 (final I1)

	vblendpd ymm6, ymm6, YMM_TMP8, 1 ;; Blend in R8 real result
	vblendpd ymm7, ymm7, YMM_TMP6, 1 ;; Blend in R6 real result

	vblendpd ymm3, ymm3, YMM_TMP7, 1 ;; Blend in R7 real result
	vblendpd ymm4, ymm4, YMM_TMP5, 1 ;; Blend in R5 real result
	ENDM

